Project Overview
===============

Project Statistics:
Total Files: 74
Total Size: 892.73 KB

File Types:
  .kt: 34 files
  .bin: 12 files
  .lock: 7 files
  .properties: 6 files
  no extension: 3 files
  .kts: 2 files
  .fbs: 2 files
  .java: 2 files
  .probe: 1 files
  .md: 1 files
  .jar: 1 files
  .bat: 1 files
  .txt: 1 files
  .xml: 1 files

Detected Technologies:

Folder Structure (Tree)
=====================
Legend: ✓ = Included in output, ✗ = Excluded from output

├── .gitignore (514 B) ✓
├── .gradle/
│   ├── 8.14/
│   │   ├── checksums/
│   │   │   ├── checksums.lock (17 B) ✓
│   │   │   ├── md5-checksums.bin (30.76 KB) ✓
│   │   │   └── sha1-checksums.bin (80.81 KB) ✓
│   │   ├── executionHistory/
│   │   │   ├── executionHistory.bin (19.21 KB) ✓
│   │   │   └── executionHistory.lock (17 B) ✓
│   │   ├── expanded/
│   │   ├── fileChanges/
│   │   │   └── last-build.bin (1 B) ✓
│   │   ├── fileHashes/
│   │   │   ├── fileHashes.bin (18.26 KB) ✓
│   │   │   └── fileHashes.lock (17 B) ✓
│   │   ├── gc.properties (0 B) ✓
│   │   └── vcsMetadata/
│   ├── 8.7/
│   │   ├── checksums/
│   │   │   ├── checksums.lock (17 B) ✓
│   │   │   ├── md5-checksums.bin (30.95 KB) ✓
│   │   │   └── sha1-checksums.bin (86.19 KB) ✓
│   │   ├── dependencies-accessors/
│   │   │   └── gc.properties (0 B) ✓
│   │   ├── executionHistory/
│   │   │   ├── executionHistory.bin (263.17 KB) ✓
│   │   │   └── executionHistory.lock (17 B) ✓
│   │   ├── expanded/
│   │   ├── fileChanges/
│   │   │   └── last-build.bin (1 B) ✓
│   │   ├── fileHashes/
│   │   │   ├── fileHashes.bin (34.81 KB) ✓
│   │   │   ├── fileHashes.lock (17 B) ✓
│   │   │   └── resourceHashesCache.bin (22.75 KB) ✓
│   │   ├── gc.properties (0 B) ✓
│   │   └── vcsMetadata/
│   ├── buildOutputCleanup/
│   │   ├── buildOutputCleanup.lock (17 B) ✓
│   │   ├── cache.properties (50 B) ✓
│   │   └── outputFiles.bin (18.94 KB) ✓
│   ├── file-system.probe (8 B) ✓
│   └── vcs-1/
│       └── gc.properties (0 B) ✓
├── .kotlin/
│   └── sessions/
├── ARCHITECTURE.md (6.55 KB) ✓
├── Dockerfile (906 B) ✓
├── build.gradle.kts (4.58 KB) ✓
├── gradle/
│   └── wrapper/
│       ├── gradle-wrapper.jar (42.43 KB) ✓
│       └── gradle-wrapper.properties (250 B) ✓
├── gradlew (8.49 KB) ✓
├── gradlew.bat (2.85 KB) ✓
├── settings.gradle.kts (42 B) ✓
├── src/
│   ├── main/
│   │   ├── flatbuffers/
│   │   │   ├── sql_rpc.fbs (2.73 KB) ✓
│   │   │   └── sql_schema.fbs (6.28 KB) ✓
│   │   ├── kotlin/
│   │   │   ├── kotlin_09032025_1756890790.txt (90.69 KB) ✓
│   │   │   └── org/
│   │   │       └── cladbe/
│   │   │           ├── cdc/
│   │   │           │   ├── Evaluator/
│   │   │           │   │   ├── DerivedSpec.kt (1.00 KB) ✓
│   │   │           │   │   ├── DiffOps.kt (3.12 KB) ✓
│   │   │           │   │   ├── FbMappings.kt (3.00 KB) ✓
│   │   │           │   │   ├── FbPredicate.kt (7.43 KB) ✓
│   │   │           │   │   ├── FbQueryAdapter.kt (213 B) ✓
│   │   │           │   │   ├── FbQueryAdapterImpl.kt (1.04 KB) ✓
│   │   │           │   │   ├── FilterFieldExtractor.kt (950 B) ✓
│   │   │           │   │   ├── OrderSpec.kt (1.18 KB) ✓
│   │   │           │   │   ├── PageOps.kt (3.07 KB) ✓
│   │   │           │   │   ├── QueryEngine.kt (7.91 KB) ✓
│   │   │           │   │   └── SqlComparators.kt (6.07 KB) ✓
│   │   │           │   ├── engine/
│   │   │           │   │   ├── App.kt (1.66 KB) ✓
│   │   │           │   │   ├── BackfillClient.kt (4.34 KB) ✓
│   │   │           │   │   ├── BackfillReplyProcessor.kt (2.28 KB) ✓
│   │   │           │   │   ├── CdcProcessor.kt (14.39 KB) ✓
│   │   │           │   │   ├── FbRequests.kt (13.31 KB) ✓
│   │   │           │   │   ├── FbResponses.kt (636 B) ✓
│   │   │           │   │   ├── KeyDbWriter.kt (3.35 KB) ✓
│   │   │           │   │   ├── PkUtils.kt (207 B) ✓
│   │   │           │   │   ├── QueryControlProcessor.kt (1.30 KB) ✓
│   │   │           │   │   ├── SeedProcessor.kt (2.39 KB) ✓
│   │   │           │   │   ├── TablePkRegistry.kt (926 B) ✓
│   │   │           │   │   └── Topology.kt (5.05 KB) ✓
│   │   │           │   └── join/
│   │   │           └── tools/
│   │   │               └── SendQctl.kt (2.62 KB) ✓
│   │   └── resources/
│   │       └── logback.xml (274 B) ✓
│   └── test/
│       └── java/
│           └── org/
│               └── cladbe/
│                   └── cdc/
│                       ├── Evaluator/
│                       │   ├── DiffOpsTest.kt (2.01 KB) ✓
│                       │   ├── FbPredicateEndToEndTest.kt (1.78 KB) ✓
│                       │   ├── FbTestBuilders.kt (5.90 KB) ✓
│                       │   ├── LivePaginationTopology.kt (2.85 KB) ✓
│                       │   ├── PageOpsTest.kt (2.57 KB) ✓
│                       │   ├── QueryEngineSmokeTest.kt (2.00 KB) ✓
│                       │   ├── SqlComparatorAndPageOpsTest.kt (3.00 KB) ✓
│                       │   ├── SqlComparatorsTest.kt (62 B) ✓
│                       │   └── engine/
│                       │       ├── KeyDbWriterTest.kt (2.61 KB) ✓
│                       │       └── SeedProcessorTest.kt (3.45 KB) ✓
│                       ├── TestFilterProducer.java (4.45 KB) ✓
│                       └── TestFlatbuffers.java (2.12 KB) ✓
└── target/

==============

File Name: .gitignore
Size: 514 B
Code:
target/
!.mvn/wrapper/maven-wrapper.jar
!**/src/main/**/target/
!**/src/test/**/target/

### IntelliJ IDEA ###
.idea/modules.xml
.idea/jarRepositories.xml
.idea/compiler.xml
.idea/libraries/
*.iws
*.iml
*.ipr

### Eclipse ###
.apt_generated
.classpath
.factorypath
.project
.settings
.springBeans
.sts4-cache

### NetBeans ###
/nbproject/private/
/nbbuild/
/dist/
/nbdist/
/.nb-gradle/
build/
!**/src/main/**/build/
!**/src/test/**/build/

### VS Code ###
.vscode/

### Mac OS ###
.DS_Store
.idea/
.mvm
.idea/
.mvn
-------- [ Separator ] ------

File Name: .gradle/8.14/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/8.7/dependencies-accessors/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/8.7/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/buildOutputCleanup/cache.properties
Size: 50 B
Code:
#Wed Sep 03 12:07:56 CEST 2025
gradle.version=8.7

-------- [ Separator ] ------

File Name: .gradle/vcs-1/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: ARCHITECTURE.md
Size: 6.55 KB
Code:
# Kafka Streams CDC Engine — Mental Model

This document summarizes the core topology, control plane, data flow, and operational notes for the Kafka Streams CDC engine used in this project.

## A. Topology & State Stores

- **Build Entry**: `StreamsTopology.buildForTable` wires everything.
- **Sources**:
  - CDC topic (raw JSON rows)
  - Query control topic (FB `StreamingSqlDataFilter`)
  - Optional seed topic (JSON snapshot from gateway)
  - SQL-RPC replies topic (FB `ResponseEnvelope`)
  - Internal backfill topic (rows fetched from RPC)
- **Processors**:
  - `QueryControlProcessor` — registers/unregisters queries
  - `CdcProcessor` — main engine: upserts rows, diffs, backfill triggers, flush
  - `SeedProcessor` — warms caches & page from snapshot
  - `BackfillReplyProcessor` — handles DB fetch replies, warms row cache, forwards to backfill topic
- **Sink**: outbox diffs to `WS_OUTBOX_TOPIC`
- **State stores (RocksDB)**: `qmeta`, `page`, `outbox`, `rowCache`, `pending`

## B. Registering a Live Query (Control Plane)

- **Input**: `server.query.control` with key `"$table|$hashId"` and value FlatBuffer `StreamingSqlDataFilter` (order, limit K, wrapper filters, optional cursor).
- **`QueryControlProcessor`**:
  - Stores FB bytes in `qmeta[$table|$hashId]`
  - Ensures `page` and `outbox` are initialized
  - Later, `CdcProcessor` will lazily build a `QueryEngine` per `hashId`.

## C. Ingesting Rows & Producing Diffs

- `CdcProcessor.process` fires on each CDC row (and also on backfill rows):
  1. Parse JSON to `Map<String, Any?>`; extract `__lsn` if present (used for headers & cache metadata).
  2. Resolve PK column via `TablePkRegistry` (e.g., `"a_notes" → "id"`). Store raw row bytes in `rowCache[pk]` (for comparator/engine lookups).
  3. For each registered query (qmeta entries for this table):
     - Build/reuse a `QueryEngine<String>` bound to that query:
       - `FbQueryAdapterImpl.parse` converts FB → `DerivedSpec`
       - `toOrderSpec` builds order keys (+ auto-append PK if missing)
       - `whereMatches` uses `FbPredicate` to evaluate SQL-like filters (equals, <, inList, contains, regex, isNull, etc.)
       - `filterFields` lists fields referenced in WHERE (to detect if an UPDATE could change match-ness)
       - `SqlComparators.pkComparator` gives a total order (ASC/DESC + explicit NULLS FIRST/LAST, temporal normalization) using rowCache lookups.
       - `QueryEngine.upsert(row)` decides:
         - not matching? → remove if currently in page (emit removed)
         - new & better than tail? → insert and evict tail (emit added + removed)
         - updated in page?
           - if order fields changed → reposition (emit modified with from→to)
           - else payload-only → modified in-place
       - It sets two intent flags: `needsBackfill` and `probeTail`.
- **`CdcProcessor` updates**:
  - `page[$table|$hashId]` with compact CSV PK list
  - `outbox[$table|$hashId]` with a compact string batch of diffs
  - Tracks “dirty” queries so snapshots/range indexes get refreshed
  - If backfill/probe is needed → send SQL-RPC request (see D).
- **Adaptive flush (`CdcProcessor`)**:
  - On a short schedule (starts ~200ms, adapts): flushes pending outbox batches to `WS_OUTBOX_TOPIC` (key=`hashId`, value=batch bytes, header `lsn` if known).
  - Also writes to KeyDB via `KeyDbWriter`:
    - `appendDiff(hashId, lsn, base64(batch))`
    - if the query was marked dirty: `writeRangeIndex(hashId, pkOrder)` and `writeSnapshot(hashId, rows, lsn)`
  - State resets for next batch.

## D. Backfill Round-Trip (Keyset Pagination)

- When `needsBackfill` or `probeTail` is set:
  - `CdcProcessor.triggerBackfill`:
    - `engine.currentCursor()` returns the last tuple by the query’s ORDER BY (e.g., `{score: 10, id: 2}`).
    - Build a SQL-RPC `GetDataReq` using FlatBuffers builders (`FbRequests.buildGetDataFromQueryFb`):
      - Compose wrapper: `AND(userWrapper, strictlyAfter(cursorTuple))`
      - `limit = 1`, `strictlyAfter = true`
    - Generate a correlation id `corr`, remember `pending[corr] = "$table|$hashId"`, and produce to `sql.rpc.requests`.
    - Your external `postgres_rpc` worker responds on `sql.rpc.responses` with a `ResponseEnvelope` → `RowsJson`.
  - `BackfillReplyProcessor`:
    - Match `corr` → get `"$table|$hashId"`
    - Parse the first row JSON (if present)
    - Warm `rowCache` and forward the raw row JSON bytes to the internal backfill topic (`BACKFILL_ROWS_TOPIC`) where `CdcProcessor` also listens
    - Delete `pending[corr]`
    - The forwarded row is then treated as if it arrived from CDC, which lets the engine insert it and emit diffs.

## E. Seeding (Optional Happy Path)

- `SeedProcessor` consumes `server.page.seed` JSON from your gateway, e.g.:

  ```json
  {"table":"acme_a_notes","hashId":"H1","rows":[...]}
  ```

- Warms `rowCache` for each row, sets `page["acme_a_notes|H1"]` to ordered PKs, and writes a KeyDB snapshot + range index so the gateway can read a full page immediately.

## F. Ordering, Diffs, and Correctness

- **Ordering (`SqlComparators`)** implements SQL-like sorting over mixed types:
  - Numeric coercion via `BigDecimal`
  - Strings, booleans, class-safe `Comparable`
  - Null semantics: explicit FIRST/LAST or defaults (ASC → LAST, DESC → FIRST)
  - Timestamps normalized (Instant, millis, micros, nanos) with unit hints
- **PageOps + DiffOps** implement the minimal Top-K page edits:
  - Stable `insertIfBetter` (with tail eviction)
  - `repositionIfNeeded` on order change
  - Emits Added, Removed, Modified (and encodes to your wire `OutboxCodec.WireDiff`)
- **`QueryEngine`** ties it together per query: filtering, ordering, page mutations, backfill intent, cursor.

---

## What Topics Do I Need?

- **Minimum for a basic run**:
  - Input CDC: `cdc.acme_a_notes` (or your actual topic)
  - Control plane: `server.query.control` (FlatBuffer queries)
  - Output diffs: `server.cdc.filtered`
- **If you seed pages**: `server.page.seed`
- **If you test backfill**: `sql.rpc.requests`, `sql.rpc.responses`, and `server.backfill.rows` (internal)

For dev, let auto-creation happen. For prod, pre-create with your desired RF/retention and compaction where appropriate (state changelog topics are created by Streams automatically).

---

## Common Pitfalls (Quick Checklist)

- Inside Docker, always use `kafka:9092` (container listener), not `localhost:29092`.
- Ensure unique `STREAMS_APP_ID` per table/app instance.
- Mount `/tmp/kafka-streams` so RocksDB state survives container restarts.
- If you disable KeyDB, remove/blank `KEYDB_URL`.
- If you don’t run the SQL-RPC worker yet, backfill just won’t return anything—normal for dev.


-------- [ Separator ] ------

File Name: Dockerfile
Size: 906 B
Code:
# ---- build stage ----
FROM maven:3.9.6-eclipse-temurin-17 AS build
WORKDIR /app

# flatc install (keep your working snippet)
RUN apt-get update && apt-get install -y wget unzip && rm -rf /var/lib/apt/lists/*
RUN wget  https://github.com/google/flatbuffers/releases/download/v25.2.10/Linux.flatc.binary.g++-13.zip \
 && unzip  Linux.flatc.binary.g++-13.zip -d /usr/local/bin \
 && chmod +x /usr/local/bin/flatc && rm Linux.flatc.binary.g++-13.zip

# Cache layer: only pom.xml → resolves deps once and caches them
COPY pom.xml .
RUN mvn -B  -DskipTests dependency:resolve dependency:resolve-plugins

# Now bring sources and compile
COPY src ./src
RUN mvn -B  -DskipTests -T 1C package

# ---- runtime stage ----
FROM eclipse-temurin:17-jre AS runtime
WORKDIR /opt/app
RUN useradd -ms /bin/bash appuser
COPY --from=build /app/target/*.jar app.jar
USER appuser
ENTRYPOINT ["java","-jar","/opt/app/app.jar"]
-------- [ Separator ] ------

File Name: build.gradle.kts
Size: 4.58 KB
Code:
plugins {
    kotlin("jvm") version "2.2.0"
    application
    id("com.github.johnrengelman.shadow") version "8.1.1" // like Maven Shade
}

repositories {
    mavenCentral()
    maven { url = uri("https://packages.confluent.io/maven/") } // matches your <repositories>
}

val flatbuffersVersion = "25.2.10"
val kafkaStreamsVersion = "4.0.0"
val confluentVersion = "7.7.1"
val slf4jVersion = "2.0.13"
val logbackVersion = "1.5.13"
val avroVersion = "1.11.4"
val jacksonVersion = "2.17.1"
val commonsLang3Version = "3.18.0"
val jedisVersion = "5.1.3"
val kotlinVersionPin = "2.2.0"
val junitJupiterVersion = "5.10.2"

dependencies {
    // Kafka Streams
    implementation("org.apache.kafka:kafka-streams:$kafkaStreamsVersion")

    // Confluent Avro SerDes (you had them in pom)
    implementation("io.confluent:kafka-streams-avro-serde:$confluentVersion")
    implementation("io.confluent:kafka-avro-serializer:$confluentVersion")

    // Logging — same stack as pom (logback + slf4j)
    implementation("org.slf4j:slf4j-api:$slf4jVersion")
    implementation("ch.qos.logback:logback-classic:$logbackVersion")
    implementation("ch.qos.logback:logback-core:$logbackVersion")

    // Data formats / utils
    implementation("org.apache.avro:avro:$avroVersion")
    implementation("com.fasterxml.jackson.core:jackson-databind:$jacksonVersion")
    implementation("com.fasterxml.jackson.core:jackson-core:$jacksonVersion")
    implementation("com.fasterxml.jackson.core:jackson-annotations:$jacksonVersion")
    implementation("org.apache.commons:commons-lang3:$commonsLang3Version")

    // FlatBuffers runtime
    implementation("com.google.flatbuffers:flatbuffers-java:$flatbuffersVersion")

    // Redis / KeyDB
    implementation("redis.clients:jedis:$jedisVersion")

    // Kotlin stdlib
    implementation("org.jetbrains.kotlin:kotlin-stdlib-jdk8:$kotlinVersionPin")

    // Tests (match pom)
    testImplementation("org.apache.kafka:kafka-streams-test-utils:$kafkaStreamsVersion")
    testImplementation("org.jetbrains.kotlin:kotlin-test:$kotlinVersionPin")
    testImplementation("org.jetbrains.kotlin:kotlin-test-junit5:$kotlinVersionPin")
    testImplementation("io.mockk:mockk:1.13.10")
    testImplementation("io.mockk:mockk-agent-jvm:1.13.10")
    testImplementation("org.junit.jupiter:junit-jupiter-api:$junitJupiterVersion")
    testRuntimeOnly("org.junit.jupiter:junit-jupiter-engine:$junitJupiterVersion")
}

kotlin {
    jvmToolchain(17) // <source>/<target> 17 as in pom
}

application {
    mainClass.set("org.cladbe.cdc.engine.AppKt") // matches your pom <mainClass>
}

tasks.test {
    useJUnitPlatform()
}

/**
 * -------- FlatBuffers codegen (parity with exec-maven-plugin) --------
 * Generates Java sources from:
 *   src/main/flatbuffers/sql_schema.fbs
 *   src/main/flatbuffers/sql_rpc.fbs
 * into build/generated-src/flatbuffers/java
 */
val flatcOutputDir = layout.buildDirectory.dir("generated-src/flatbuffers/java")

val flatcGenerate by tasks.registering(Exec::class) {
    group = "codegen"
    description = "Run flatc to generate Java classes from .fbs schemas"
    // Ensure output dir exists
    doFirst { flatcOutputDir.get().asFile.mkdirs() }
    commandLine(
        "flatc",
        "--java",
        "-o", flatcOutputDir.get().asFile.absolutePath,
        "-I", "$projectDir/src/main/flatbuffers",
        "$projectDir/src/main/flatbuffers/sql_schema.fbs",
        "$projectDir/src/main/flatbuffers/sql_rpc.fbs"
    )
    // Rerun when schemas change
    inputs.files(
        file("$projectDir/src/main/flatbuffers/sql_schema.fbs"),
        file("$projectDir/src/main/flatbuffers/sql_rpc.fbs")
    )
    outputs.dir(flatcOutputDir)
}

// Include generated sources in main compilation (parity with build-helper-maven-plugin)
sourceSets {
    named("main") {
        java.srcDir(flatcOutputDir)
        java.srcDir("src/main/java")
        kotlin.srcDir("src/main/kotlin")
    }
    named("test") {
        java.srcDir("src/test/java")
        kotlin.srcDir("src/test/kotlin")
    }
}

// Make Kotlin & Java compilation depend on flatc codegen
tasks.withType<org.jetbrains.kotlin.gradle.tasks.KotlinCompile> {
    dependsOn(flatcGenerate)
}
tasks.withType<JavaCompile> {
    dependsOn(flatcGenerate)
}

/**
 * -------- Packaging (Shadow = Maven Shade) --------
 * Produces an uber-jar at build/libs/kafka-streams-project-all.jar
 * with Main-Class set, same as your Maven Shade config.
 */
tasks.named<com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar>("shadowJar") {
    archiveBaseName.set("kafka-streams-project")
    archiveClassifier.set("all")
    manifest {
        attributes["Main-Class"] = "org.cladbe.cdc.engine.AppKt"
    }
}
-------- [ Separator ] ------

File Name: gradle/wrapper/gradle-wrapper.properties
Size: 250 B
Code:
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.7-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists

-------- [ Separator ] ------

File Name: gradlew
Size: 8.49 KB
Code:
#!/bin/sh

#
# Copyright © 2015-2021 the original authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

##############################################################################
#
#   Gradle start up script for POSIX generated by Gradle.
#
#   Important for running:
#
#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is
#       noncompliant, but you have some other compliant shell such as ksh or
#       bash, then to run this script, type that shell name before the whole
#       command line, like:
#
#           ksh Gradle
#
#       Busybox and similar reduced shells will NOT work, because this script
#       requires all of these POSIX shell features:
#         * functions;
#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,
#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;
#         * compound commands having a testable exit status, especially «case»;
#         * various built-in commands including «command», «set», and «ulimit».
#
#   Important for patching:
#
#   (2) This script targets any POSIX shell, so it avoids extensions provided
#       by Bash, Ksh, etc; in particular arrays are avoided.
#
#       The "traditional" practice of packing multiple parameters into a
#       space-separated string is a well documented source of bugs and security
#       problems, so this is (mostly) avoided, by progressively accumulating
#       options in "$@", and eventually passing that to Java.
#
#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,
#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;
#       see the in-line comments for details.
#
#       There are tweaks for specific operating systems such as AIX, CygWin,
#       Darwin, MinGW, and NonStop.
#
#   (3) This script is generated from the Groovy template
#       https://github.com/gradle/gradle/blob/HEAD/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt
#       within the Gradle project.
#
#       You can find Gradle at https://github.com/gradle/gradle/.
#
##############################################################################

# Attempt to set APP_HOME

# Resolve links: $0 may be a link
app_path=$0

# Need this for daisy-chained symlinks.
while
    APP_HOME=${app_path%"${app_path##*/}"}  # leaves a trailing /; empty if no leading path
    [ -h "$app_path" ]
do
    ls=$( ls -ld "$app_path" )
    link=${ls#*' -> '}
    case $link in             #(
      /*)   app_path=$link ;; #(
      *)    app_path=$APP_HOME$link ;;
    esac
done

# This is normally unused
# shellcheck disable=SC2034
APP_BASE_NAME=${0##*/}
# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)
APP_HOME=$( cd "${APP_HOME:-./}" > /dev/null && pwd -P ) || exit

# Use the maximum available, or set MAX_FD != -1 to use that value.
MAX_FD=maximum

warn () {
    echo "$*"
} >&2

die () {
    echo
    echo "$*"
    echo
    exit 1
} >&2

# OS specific support (must be 'true' or 'false').
cygwin=false
msys=false
darwin=false
nonstop=false
case "$( uname )" in                #(
  CYGWIN* )         cygwin=true  ;; #(
  Darwin* )         darwin=true  ;; #(
  MSYS* | MINGW* )  msys=true    ;; #(
  NONSTOP* )        nonstop=true ;;
esac

CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar


# Determine the Java command to use to start the JVM.
if [ -n "$JAVA_HOME" ] ; then
    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
        # IBM's JDK on AIX uses strange locations for the executables
        JAVACMD=$JAVA_HOME/jre/sh/java
    else
        JAVACMD=$JAVA_HOME/bin/java
    fi
    if [ ! -x "$JAVACMD" ] ; then
        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
else
    JAVACMD=java
    if ! command -v java >/dev/null 2>&1
    then
        die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
fi

# Increase the maximum file descriptors if we can.
if ! "$cygwin" && ! "$darwin" && ! "$nonstop" ; then
    case $MAX_FD in #(
      max*)
        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.
        # shellcheck disable=SC2039,SC3045
        MAX_FD=$( ulimit -H -n ) ||
            warn "Could not query maximum file descriptor limit"
    esac
    case $MAX_FD in  #(
      '' | soft) :;; #(
      *)
        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.
        # shellcheck disable=SC2039,SC3045
        ulimit -n "$MAX_FD" ||
            warn "Could not set maximum file descriptor limit to $MAX_FD"
    esac
fi

# Collect all arguments for the java command, stacking in reverse order:
#   * args from the command line
#   * the main class name
#   * -classpath
#   * -D...appname settings
#   * --module-path (only if needed)
#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.

# For Cygwin or MSYS, switch paths to Windows format before running java
if "$cygwin" || "$msys" ; then
    APP_HOME=$( cygpath --path --mixed "$APP_HOME" )
    CLASSPATH=$( cygpath --path --mixed "$CLASSPATH" )

    JAVACMD=$( cygpath --unix "$JAVACMD" )

    # Now convert the arguments - kludge to limit ourselves to /bin/sh
    for arg do
        if
            case $arg in                                #(
              -*)   false ;;                            # don't mess with options #(
              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath
                    [ -e "$t" ] ;;                      #(
              *)    false ;;
            esac
        then
            arg=$( cygpath --path --ignore --mixed "$arg" )
        fi
        # Roll the args list around exactly as many times as the number of
        # args, so each arg winds up back in the position where it started, but
        # possibly modified.
        #
        # NB: a `for` loop captures its iteration list before it begins, so
        # changing the positional parameters here affects neither the number of
        # iterations, nor the values presented in `arg`.
        shift                   # remove old arg
        set -- "$@" "$arg"      # push replacement arg
    done
fi


# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
DEFAULT_JVM_OPTS='"-Xmx64m" "-Xms64m"'

# Collect all arguments for the java command:
#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,
#     and any embedded shellness will be escaped.
#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be
#     treated as '${Hostname}' itself on the command line.

set -- \
        "-Dorg.gradle.appname=$APP_BASE_NAME" \
        -classpath "$CLASSPATH" \
        org.gradle.wrapper.GradleWrapperMain \
        "$@"

# Stop when "xargs" is not available.
if ! command -v xargs >/dev/null 2>&1
then
    die "xargs is not available"
fi

# Use "xargs" to parse quoted args.
#
# With -n1 it outputs one arg per line, with the quotes and backslashes removed.
#
# In Bash we could simply go:
#
#   readarray ARGS < <( xargs -n1 <<<"$var" ) &&
#   set -- "${ARGS[@]}" "$@"
#
# but POSIX shell has neither arrays nor command substitution, so instead we
# post-process each arg (as a line of input to sed) to backslash-escape any
# character that might be a shell metacharacter, then use eval to reverse
# that process (while maintaining the separation between arguments), and wrap
# the whole thing up as a single "set" statement.
#
# This will of course break if any of these variables contains a newline or
# an unmatched quote.
#

eval "set -- $(
        printf '%s\n' "$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS" |
        xargs -n1 |
        sed ' s~[^-[:alnum:]+,./:=@_]~\\&~g; ' |
        tr '\n' ' '
    )" '"$@"'

exec "$JAVACMD" "$@"

-------- [ Separator ] ------

File Name: gradlew.bat
Size: 2.85 KB
Code:
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%"=="" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%"=="" set DIRNAME=.
@rem This is normally unused
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if %ERRORLEVEL% equ 0 goto execute

echo. 1>&2
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH. 1>&2
echo. 1>&2
echo Please set the JAVA_HOME variable in your environment to match the 1>&2
echo location of your Java installation. 1>&2

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo. 1>&2
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME% 1>&2
echo. 1>&2
echo Please set the JAVA_HOME variable in your environment to match the 1>&2
echo location of your Java installation. 1>&2

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar


@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if %ERRORLEVEL% equ 0 goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
set EXIT_CODE=%ERRORLEVEL%
if %EXIT_CODE% equ 0 set EXIT_CODE=1
if not ""=="%GRADLE_EXIT_CONSOLE%" exit %EXIT_CODE%
exit /b %EXIT_CODE%

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega

-------- [ Separator ] ------

File Name: settings.gradle.kts
Size: 42 B
Code:
rootProject.name = "kafka-streams-project"
-------- [ Separator ] ------

File Name: src/main/flatbuffers/sql_rpc.fbs
Size: 2.73 KB
Code:
include "sql_schema.fbs";

namespace SqlRpc;

// ---------- RPC Method ----------
enum RpcMethod : ubyte {
  GET_DATA         = 0,
  GET_SINGLE       = 1,
  ADD_SINGLE       = 2,
  UPDATE_SINGLE    = 3,
  DELETE_ROW       = 4,
  CREATE_TABLE     = 5,
  TABLE_EXISTS     = 6,
  RUN_AGGREGATION  = 7
}

// ---------- Requests ----------
table GetDataReq {
  company_id: string;
  table_name: string;
  wrapper: SqlSchema.BasicSqlDataFilterWrapper; // optional
  limit: uint32 = 0;                             // 0 → no limit
  offset: uint32 = 0;                            // legacy; prefer cursor
  order: [SqlSchema.OrderKeySpec];               // optional
  cursor: [SqlSchema.CursorEntry];               // keyset cursor
  strict_after: bool = true;                     // default true
}

table GetSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table AddSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  row_json: string; // JSON row payload
}

table UpdateSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
  updates_json: string; // JSON partial
}

table DeleteRowReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table CreateTableReq {
  company_id: string;
  definition: SqlSchema.TableDefinition;
}

table TableExistsReq {
  company_id: string;
  table_name: string;
}

table RunAggregationReq {
  company_id: string;
  table_name: string;
  count_enabled: bool = false;
  sum_fields: [string];
  average_fields: [string];
  minimum_fields: [string];
  maximum_fields: [string];
  wrapper: SqlSchema.BasicSqlDataFilterWrapper;
}

// ---------- Responses ----------
table RowsJson { rows: [string]; }           // unchanged
table RowJson  { row: string; }
table BoolRes  { value: bool; }
table AggRes   { agg: SqlSchema.DataHelperAggregation; }

// NEW: rows + cursor (e.g. WAL LSN) in one message
table RowsWithCursor {
  rows: [string];
  cursor: [SqlSchema.CursorEntry];
}

enum ErrorCode : ubyte { NONE = 0, BAD_REQUEST = 1, INTERNAL = 2 }

// Single union definition including the new type
union RpcResponse {
  RowsJson,
  RowJson,
  BoolRes,
  AggRes,
  RowsWithCursor
}

table ResponseEnvelope {
  correlation_id: string;
  ok: bool = true;
  error_code: ErrorCode = NONE;
  error_message: string;
  data: RpcResponse;
}

// ---------- Envelope ----------
union RpcPayload {
  GetDataReq,
  GetSingleReq,
  AddSingleReq,
  UpdateSingleReq,
  DeleteRowReq,
  CreateTableReq,
  TableExistsReq,
  RunAggregationReq
}

table RequestEnvelope {
  correlation_id: string;
  reply_topic: string;
  method: RpcMethod;
  payload: RpcPayload;
}

root_type RequestEnvelope;
-------- [ Separator ] ------

File Name: src/main/flatbuffers/sql_schema.fbs
Size: 6.28 KB
Code:
namespace SqlSchema;

// ---------------- Enums with explicit numbers ----------------

// Stable: text/numeric types
enum SQLDataType : ubyte {
  text             = 0,
  varchar          = 1,
  char_            = 2,
  varcharArray     = 3,
  textArray        = 4,
  charArray        = 5,
  integer          = 6,
  bigInt           = 7,
  smallInt         = 8,
  decimal          = 9,
  numeric          = 10,
  real             = 11,
  doublePrecision  = 12,
  serial           = 13,
  bigSerial        = 14,
  smallSerial      = 15,
  money            = 16,
  date             = 17,
  time             = 18,
  timestamp        = 19,
  timestamptz      = 20,
  interval         = 21,
  timetz           = 22,
  boolean_         = 23,
  bytea            = 24,
  json             = 25,
  jsonb            = 26,
  jsonArray        = 27,
  jsonbArray       = 28,
  uuid             = 29,
  xml              = 30,
  array            = 31,
  custom           = 32
}

enum ColumnConstraint : ubyte {
  primaryKey        = 0,
  unique            = 1,
  notNull           = 2,
  check             = 3,
  default_          = 4,
  indexed           = 5,
  exclusion         = 6,
  generated         = 7,
  identity          = 8,
  references        = 9,
  noInherit         = 10,
  nullsNotDistinct  = 11
}

enum SQLFilterWrapperType : ubyte {
  or   = 0,
  and  = 1
}

enum NullsSortOrder : ubyte {
  first   = 0,
  last    = 1,
  default_= 2
}

enum TimeUnit : ubyte {
  SECONDS = 0,
  MILLIS  = 1,
  MICROS  = 2,
  NANOS   = 3
}

enum BasicSqlDataFilterType : ubyte {
  equals              = 0,
  notEquals           = 1,
  lessThan            = 2,
  lessThanOrEquals    = 3,
  greaterThan         = 4,
  greaterThanOrEquals = 5,

  isNull              = 6,
  isNotNull           = 7,

  regex               = 8,
  notRegex            = 9,
  startsWith          = 10,
  endsWith            = 11,
  contains            = 12,
  notContains         = 13,

  arrayContains       = 14,
  arrayContainedBy    = 15,
  arrayOverlaps       = 16,
  arrayEquals         = 17,
  arrayNotEquals      = 18,
  arrayEmpty          = 19,
  arrayNotEmpty       = 20,
  arrayLength         = 21,

  jsonContains        = 22,
  jsonContainedBy     = 23,
  jsonHasKey          = 24,
  jsonHasAnyKey       = 25,
  jsonHasAllKeys      = 26,
  jsonGetField        = 27,
  jsonGetFieldAsText  = 28,

  between             = 29,
  notBetween          = 30,
  rangeContains       = 31,
  rangeContainedBy    = 32,

  inList              = 33,
  notInList           = 34
}

enum OrderSort : ubyte {
  ASC_DEFAULT      = 0,
  ASC_NULLS_FIRST  = 1,
  ASC_NULLS_LAST   = 2,
  DESC_DEFAULT     = 3,
  DESC_NULLS_FIRST = 4,
  DESC_NULLS_LAST  = 5
}

// ---------------- Helper tables (unchanged) ----------------
table KeyValuePair { key: string; value: string; }
table CustomOptions { options: [KeyValuePair]; }
table TableOptions  { options: [KeyValuePair]; }

table DataHelperAggregation {
  count: uint32;
  sum_values: [KeyValuePair];
  avg_values: [KeyValuePair];
  minimum_values: [KeyValuePair];
  maximum_values: [KeyValuePair];
}

table DataSort { field: string; ascending: bool; }

// ---------------- Value tables ----------------
table StringValue   { value: string; }
table NumberValue   { value: double; }
table Int64Value    { value: long; }
table BoolValue     { value: bool; }
table NullValue     { }

table TimestampValue {
  epoch: long;
  unit:  TimeUnit = MICROS;
}

table StringList  { values:[string] (required); }
table Int64List   { values:[long]   (required); }
table Float64List { values:[double] (required); }
table BoolList    { values:[bool]   (required); }

// ---------------- Unions ----------------
union FilterValue {
  StringValue,
  NumberValue,
  BoolValue,
  NullValue,
  Int64Value,
  TimestampValue,
  StringList,
  Int64List,
  Float64List,
  BoolList
}

table RangeValue {
  low:  FilterValue;
  high: FilterValue;
  include_low:  bool = true;
  include_high: bool = true;
}

table CursorEntry { field:string (required); value:FilterValue; }

// ---------------- Filters ----------------
table SqlFilterModifier {
  distinct: bool;
  case_insensitive: bool;
  nulls_order: NullsSortOrder;
}

table BasicSqlDataFilter {
  field_name: string;
  value: FilterValue;
  filter_type: BasicSqlDataFilterType;
  modifier: SqlFilterModifier;
}

union BasicSqlDataFilterUnion { BasicSqlDataFilterWrapper, BasicSqlDataFilter }

table BasicSqlDataFilterWrapper {
  filter_wrapper_type: SQLFilterWrapperType;
  filters: [BasicSqlDataFilterUnion];
}

// ---------------- Query ----------------
table OrderKeySpec {
  field:string (required);
  sort:OrderSort = DESC_DEFAULT;
  is_pk:bool = false;
}

table StreamingSqlDataFilter {
  hash: string;
  wrapper: BasicSqlDataFilterWrapper;
  limit: uint32 = 50;
  order: [OrderKeySpec];
  cursor: [CursorEntry];
  schema_version: ushort = 1;
}

// ---------------- Table metadata ----------------
table TableColumn {
  name: string;
  data_type: SQLDataType;
  is_nullable: bool;
  constraints: [ColumnConstraint];
  custom_options: CustomOptions;
}

table TableDefinition {
  name: string;
  columns: [TableColumn];
  comment: string;
  table_options: TableOptions;
}

root_type StreamingSqlDataFilter;


// ---------------- Query spec for non-streaming RPC ----------------
// Same modeling as StreamingSqlDataFilter but:
// - no 'hash' (RPC doesn't need it)
// - adds 'strict_after' to mirror your keyset semantics
// - no schema_version (rpc payload versioning usually handled at the envelope)
table SqlQuerySpec {
  // WHERE / boolean-expression tree
  wrapper: BasicSqlDataFilterWrapper;

  // LIMIT
  limit: uint32 = 50;

  // ORDER BY (multi key). Use is_pk=true on keys that are part of the PK if needed.
  order: [OrderKeySpec];

  // KEYSET pagination: the last-seen tuple of ordered columns
  cursor: [CursorEntry];

  // Keyset semantics: when true, results must be strictly AFTER cursor
  // (equivalent to your "strictAfter" flag in manager)
  strict_after: bool = true;
}

// ---------------- Aggregation spec for RPC ----------------
table AggregationSpec {
  // SELECT COUNT(*)
  count_enabled: bool = false;

  // Aggregation over fields (server-side schema validates numeric types)
  sum_fields:      [string];
  average_fields:  [string];
  minimum_fields:  [string];
  maximum_fields:  [string];

  // Optional WHERE (reuse your wrapper)
  wrapper: BasicSqlDataFilterWrapper;
}
-------- [ Separator ] ------

File Name: src/main/kotlin/kotlin_09032025_1756890790.txt
Size: 90.69 KB
Code:
Project Overview
===============

Project Statistics:
Total Files: 24
Total Size: 86.47 KB

File Types:
  .kt: 24 files

Detected Technologies:

Folder Structure (Tree)
=====================
Legend: ✓ = Included in output, ✗ = Excluded from output

└── org/
    └── cladbe/
        ├── cdc/
        │   ├── Evaluator/
        │   │   ├── DerivedSpec.kt (1.00 KB) ✓
        │   │   ├── DiffOps.kt (3.12 KB) ✓
        │   │   ├── FbMappings.kt (3.22 KB) ✓
        │   │   ├── FbPredicate.kt (7.43 KB) ✓
        │   │   ├── FbQueryAdapter.kt (213 B) ✓
        │   │   ├── FbQueryAdapterImpl.kt (1.04 KB) ✓
        │   │   ├── FilterFieldExtractor.kt (950 B) ✓
        │   │   ├── OrderSpec.kt (1.18 KB) ✓
        │   │   ├── PageOps.kt (3.07 KB) ✓
        │   │   ├── QueryEngine.kt (7.69 KB) ✓
        │   │   └── SqlComparators.kt (6.07 KB) ✓
        │   ├── engine/
        │   │   ├── App.kt (1.66 KB) ✓
        │   │   ├── BackfillClient.kt (4.34 KB) ✓
        │   │   ├── BackfillReplyProcessor.kt (2.92 KB) ✓
        │   │   ├── CdcProcessor.kt (11.47 KB) ✓
        │   │   ├── FbRequests.kt (13.73 KB) ✓
        │   │   ├── FbResponses.kt (636 B) ✓
        │   │   ├── KeyDbWriter.kt (3.35 KB) ✓
        │   │   ├── PkUtils.kt (207 B) ✓
        │   │   ├── QueryControlProcessor.kt (1.30 KB) ✓
        │   │   ├── SeedProcessor.kt (2.39 KB) ✓
        │   │   ├── TablePkRegistry.kt (926 B) ✓
        │   │   └── Topology.kt (5.91 KB) ✓
        │   └── join/
        └── tools/
            └── SendQctl.kt (2.71 KB) ✓

==============

File Name: org/cladbe/cdc/Evaluator/DerivedSpec.kt
Size: 1.00 KB
Code:
package org.cladbe.cdc.Evaluator

/**
 * Fully-derived spec the QueryEngine needs at runtime.
 *
 * - `order` MUST end with the PK key (isPk=true), giving a total order.
 * - `whereMatches` implements the user predicate with exact SQL semantics.
 * - `filterFields` includes every field referenced by WHERE; the engine uses it
 *   to detect when an UPDATE changes “match-ness” and whether to probe backfill.
 * - `k` is the page capacity.
 */
data class DerivedSpec(
    val table: String,
    val k: Int,
    val order: List<OrderKeySpec>,
    val whereMatches: (Map<String, Any?>) -> Boolean,
    val filterFields: Set<String>
) {
    init {
        require(order.isNotEmpty()) { "order must not be empty" }
        require(order.last().isPk) { "last order key must be the PK (isPk=true)" }
        require(order.count { it.isPk } == 1) { "exactly one order key must be marked isPk=true" }
    }

    /** Convenience: the PK field name from the trailing order key. */
    val pkField: String get() = order.last().field
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/DiffOps.kt
Size: 3.12 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

/** Minimal change model for your outbox. */
sealed interface Diff<PK> {
    val pk: PK
    data class Added<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    data class Removed<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    /**
     * Modified: row stayed in the page but content and/or order position changed.
     * - If `from != null && from != pos`, the row moved (promotion/demotion).
     * - If `from == pos`, treat as in-place payload change.
     */
    data class Modified<PK>(override val pk: PK, val pos: Int, val from: Int? = null) : Diff<PK>
}

object DiffOps {

    /**
     * Insert a PK into a sorted Top-K page and emit diffs:
     * - ADDED(pos) when inserted.
     * - REMOVED(tailPos) if the tail was evicted.
     * - [] if nothing changed.
     */
    @JvmStatic
    fun <PK> insertAndDiff(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): List<Diff<PK>> {
        val diffs = mutableListOf<Diff<PK>>()
        val prevSize = page.size
        val res = PageOps.insertIfBetter(page, pk, k, cmp)

        if (res.inserted) {
            diffs += Diff.Added(pk, res.pos)
        }
        if (res.evicted != null) {
            // evicted was the previous tail
            val evictedPos = (prevSize - 1).coerceAtLeast(0)
            diffs += Diff.Removed(res.evicted, evictedPos)
        }
        return diffs
    }

    /**
     * Reposition a PK already in the page when its order fields changed.
     * Emits MODIFIED(newPos, from=oldPos) if moved, or MODIFIED(pos, from=pos) if payload-only change.
     * If the PK is not present, returns [] (call insertAndDiff instead).
     */
    @JvmStatic
    fun <PK> repositionAndDiff(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>,
        payloadChanged: Boolean = true
    ): List<Diff<PK>> {
        val res = PageOps.repositionIfNeeded(page, pk, cmp)
        return if (res.from >= 0) {
            val from = res.from
            val to = res.to
            // Always emit Modified if caller says payload changed, or if moved
            if (payloadChanged || res.moved) listOf(Diff.Modified(pk, to, from)) else emptyList()
        } else {
            emptyList()
        }
    }

    /**
     * Remove a PK if present and emit REMOVED(pos).
     */
    @JvmStatic
    fun <PK> removeAndDiff(
        page: MutableList<PK>,
        pk: PK
    ): List<Diff<PK>> {
        val idx = page.indexOf(pk)
        if (idx >= 0) {
            page.removeAt(idx)
            return listOf(Diff.Removed(pk, idx))
        }
        return emptyList()
    }
}
object OutboxCodec {
    data class WireDiff<PK>(val type: String, val pk: PK, val pos: Int? = null, val from: Int? = null)

    @JvmStatic
    fun <PK> toWire(diffs: List<Diff<PK>>): List<WireDiff<PK>> =
        diffs.map {
            when (it) {
                is Diff.Added   -> WireDiff("added",   it.pk, pos = it.pos)
                is Diff.Removed -> WireDiff("removed", it.pk, pos = it.pos)
                is Diff.Modified-> WireDiff("modified",it.pk, pos = it.pos, from = it.from)
            }
        }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/FbMappings.kt
Size: 3.22 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import SqlSchema.OrderKeySpec as FbOrderKeySpec
import SqlSchema.OrderSort as FbOrderSort
import SqlSchema.FilterValue
import SqlSchema.TimestampValue
import SqlSchema.TimeUnit
import SqlSchema.StringValue
import SqlSchema.NumberValue
import SqlSchema.Int64Value
import SqlSchema.BoolValue

object FbMappings {

    /** Map FB OrderSort (provided as Int by generated getter) -> (Direction, Nulls). */
    @JvmStatic
    fun mapOrderSort(code: Int): Pair<Direction, Nulls> = when (code) {
        FbOrderSort.ASC_DEFAULT.toInt()      -> Direction.ASC  to Nulls.LAST
        FbOrderSort.ASC_NULLS_FIRST.toInt()  -> Direction.ASC  to Nulls.FIRST
        FbOrderSort.ASC_NULLS_LAST.toInt()   -> Direction.ASC  to Nulls.LAST
        FbOrderSort.DESC_DEFAULT.toInt()     -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_FIRST.toInt() -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_LAST.toInt()  -> Direction.DESC to Nulls.LAST
        else                                 -> Direction.DESC to Nulls.FIRST
    }

    /** Build runtime OrderKeySpec list; auto-append PK if missing. */
    @JvmStatic
    fun toOrderSpec(q: StreamingSqlDataFilter, pkField: String, pkDir: Direction = Direction.DESC): List<OrderKeySpec> {
        val list = buildList {
            for (i in 0 until q.orderLength()) {
                val ok: FbOrderKeySpec = q.order(i)!!
                val (dir, nulls) = mapOrderSort(ok.sort().toInt()) // ok.sort() is Int in Kotlin
                add(OrderKeySpec.of(ok.field(), dir, nulls))
            }
        }
        return if (list.isNotEmpty() && list.last().isPk) list else list + OrderKeySpec.pk(pkField, pkDir)
    }

    /** Keyset cursor tuple (field -> JVM value). Lists aren’t used for cursor. */
    @JvmStatic
    fun toCursorTuple(q: StreamingSqlDataFilter): Map<String, Any?>? {
        if (q.cursorLength() == 0) return null
        val m = LinkedHashMap<String, Any?>(q.cursorLength())
        for (i in 0 until q.cursorLength()) {
            val ce = q.cursor(i)!!
            m[ce.field()] = readValue(ce.valueType().toInt(), ce.value(null))
        }
        return m
    }

    /** Read a FilterValue union into a JVM value. Accept Int to match FB getters. */
    @JvmStatic
    fun readValue(t: Int, table: com.google.flatbuffers.Table?): Any? = when (t) {
        FilterValue.StringValue.toInt()    -> (table as StringValue).value()
        FilterValue.NumberValue.toInt()    -> (table as NumberValue).value()      // Double
        FilterValue.Int64Value.toInt()     -> (table as Int64Value).value()       // Long
        FilterValue.BoolValue.toInt()      -> (table as BoolValue).value()
        FilterValue.NullValue.toInt()      -> null
        FilterValue.TimestampValue.toInt() -> {
            val tv = table as TimestampValue
            val base = tv.epoch()
            when (tv.unit().toInt()) {
                TimeUnit.SECONDS.toInt() -> base * 1_000_000L
                TimeUnit.MILLIS.toInt()  -> base * 1_000L
                TimeUnit.MICROS.toInt()  -> base
                TimeUnit.NANOS.toInt()   -> base / 1_000L   // keep micros internally
                else -> base
            }
        }
        else -> null
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/FbPredicate.kt
Size: 7.43 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FbPredicate {

    @JvmStatic
    fun evalWrapper(w: BasicSqlDataFilterWrapper?, row: Map<String, Any?>): Boolean {
        if (w == null) return true
        val isAnd = w.filterWrapperType() == SQLFilterWrapperType.and
        var acc = if (isAnd) true else false
        for (i in 0 until w.filtersLength()) {
            val t = w.filtersType(i).toInt() // ← normalize to Int
            val ok = when (t) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() ->
                    evalBasic(w.filters(null, i) as BasicSqlDataFilter, row)
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() ->
                    evalWrapper(w.filters(null, i) as BasicSqlDataFilterWrapper, row)
                else -> false
            }
            acc = if (isAnd) (acc && ok) else (acc || ok)
            if (isAnd && !acc) return false
            if (!isAnd && acc) return true
        }
        return acc
    }

    @JvmStatic
    fun evalBasic(f: BasicSqlDataFilter, row: Map<String, Any?>): Boolean {
        val field = f.fieldName()
        val got = row[field]
        val ci = f.modifier()?.caseInsensitive() == true

        return when (f.filterType().toInt()) { // ← normalize to Int
            BasicSqlDataFilterType.equals.toInt()              -> eq(got, f, ci)
            BasicSqlDataFilterType.notEquals.toInt()           -> !eq(got, f, ci)

            BasicSqlDataFilterType.lessThan.toInt()            -> cmp(got, f) { a, b -> a <  b }
            BasicSqlDataFilterType.lessThanOrEquals.toInt()    -> cmp(got, f) { a, b -> a <= b }
            BasicSqlDataFilterType.greaterThan.toInt()         -> cmp(got, f) { a, b -> a >  b }
            BasicSqlDataFilterType.greaterThanOrEquals.toInt() -> cmp(got, f) { a, b -> a >= b }

            BasicSqlDataFilterType.inList.toInt()              -> inList(got, f, ci)
            BasicSqlDataFilterType.notInList.toInt()           -> !inList(got, f, ci)

            BasicSqlDataFilterType.isNull.toInt()              -> got == null
            BasicSqlDataFilterType.isNotNull.toInt()           -> got != null

            BasicSqlDataFilterType.startsWith.toInt()          -> str(got)?.let { a -> startsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.endsWith.toInt()            -> str(got)?.let { a -> endsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.contains.toInt()            -> str(got)?.let { a -> contains(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.notContains.toInt()         -> str(got)?.let { a -> !contains(a, strVal(f), ci) } == true

            BasicSqlDataFilterType.regex.toInt()               -> regexMatch(str(got) ?: "", strVal(f), ci)
            BasicSqlDataFilterType.notRegex.toInt()            -> !regexMatch(str(got) ?: "", strVal(f), ci)

            else -> false
        }
    }

    private fun strVal(f: BasicSqlDataFilter): String =
        (f.value(StringValue()) as? StringValue)?.value() ?: ""

    private fun eq(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean = when (f.valueType().toInt()) { // ← normalize
        FilterValue.StringValue.toInt()    -> str(got)?.let { a -> eqStr(a, (f.value(StringValue()) as StringValue).value(), ci) } ?: false
        FilterValue.NumberValue.toInt()    -> num(got)?.let  { a -> a == (f.value(NumberValue()) as NumberValue).value() } ?: false
        FilterValue.Int64Value.toInt()     -> long(got)?.let { a -> a == (f.value(Int64Value()) as Int64Value).value() } ?: false
        FilterValue.BoolValue.toInt()      -> (got as? Boolean) == (f.value(BoolValue()) as BoolValue).value()
        FilterValue.TimestampValue.toInt() -> micros(got)?.let { a -> a == tvMicros(f.value(TimestampValue()) as TimestampValue) } ?: false
        FilterValue.NullValue.toInt()      -> got == null
        else -> false
    }

    private inline fun cmp(got: Any?, f: BasicSqlDataFilter, crossinline op: (Double, Double) -> Boolean): Boolean {
        val g = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> micros(got)?.toDouble()
            FilterValue.Int64Value.toInt()     -> long(got)?.toDouble()
            else                               -> num(got)
        } ?: return false

        val e = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> tvMicros(f.value(TimestampValue()) as TimestampValue).toDouble()
            FilterValue.Int64Value.toInt()     -> (f.value(Int64Value()) as Int64Value).value().toDouble()
            FilterValue.NumberValue.toInt()    -> (f.value(NumberValue()) as NumberValue).value()
            else -> return false
        }
        return op(g, e)
    }

    private fun inList(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean {
        if (got == null) return false
        return when (f.valueType().toInt()) { // ← normalize
            FilterValue.StringList.toInt() -> {
                val L = f.value(StringList()) as StringList
                val probe = str(got) ?: return false
                (0 until L.valuesLength()).any { i -> eqStr(probe, L.values(i) ?: return@any false, ci) }
            }
            FilterValue.Int64List.toInt() -> {
                val L = f.value(Int64List()) as Int64List
                val g = long(got) ?: return false
                (0 until L.valuesLength()).any { i -> g == L.values(i) }
            }
            FilterValue.Float64List.toInt() -> {
                val L = f.value(Float64List()) as Float64List
                val g = num(got) ?: return false
                (0 until L.valuesLength()).any { i -> java.lang.Double.compare(g, L.values(i)) == 0 }
            }
            FilterValue.BoolList.toInt() -> {
                val L = f.value(BoolList()) as BoolList
                val g = got as? Boolean ?: return false
                (0 until L.valuesLength()).any { i -> g == (L.values(i) == true) }
            }
            else -> false
        }
    }

    // small utils
    private fun str(v: Any?): String? = when (v) { null -> null; is CharSequence -> v.toString(); else -> v.toString() }
    private fun eqStr(a: String, b: String, ci: Boolean) = if (ci) a.equals(b, true) else a == b
    private fun startsWith(a: String, b: String, ci: Boolean) = if (ci) a.lowercase().startsWith(b.lowercase()) else a.startsWith(b)
    private fun endsWith(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().endsWith(b.lowercase()) else a.endsWith(b)
    private fun contains(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().contains(b.lowercase()) else a.contains(b)
    private fun regexMatch(text: String, pattern: String, ci: Boolean) =
        if (ci) Regex(pattern, setOf(RegexOption.IGNORE_CASE)).containsMatchIn(text) else Regex(pattern).containsMatchIn(text)

    private fun num(v: Any?): Double? = when (v) { is Number -> v.toDouble(); is String -> v.toDoubleOrNull(); else -> null }
    private fun long(v: Any?): Long?  = when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }
    private fun micros(v: Any?): Long?= when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }

    private fun tvMicros(tv: TimestampValue): Long = when (tv.unit().toInt()) {
        TimeUnit.SECONDS.toInt() -> tv.epoch() * 1_000_000L
        TimeUnit.MILLIS.toInt()  -> tv.epoch() * 1_000L
        TimeUnit.MICROS.toInt()  -> tv.epoch()
        TimeUnit.NANOS.toInt()   -> tv.epoch() / 1_000L
        else -> tv.epoch()
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/FbQueryAdapter.kt
Size: 213 B
Code:
// Evaluator/FbQueryAdapter.kt
package org.cladbe.cdc.Evaluator

/** Adapts a FlatBuffer StreamingSqlDataFilter into a runtime DerivedSpec. */
interface FbQueryAdapter {
    fun parse(fb: ByteArray): DerivedSpec
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/FbQueryAdapterImpl.kt
Size: 1.04 KB
Code:
// Evaluator/FbQueryAdapterImpl.kt
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import java.nio.ByteBuffer

class FbQueryAdapterImpl(
    private val tableName: String,
    private val pkField: String,
    private val pkDir: Direction = Direction.DESC
) : FbQueryAdapter {

    override fun parse(fb: ByteArray): DerivedSpec {
        val q = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(ByteBuffer.wrap(fb))
        val k = q.limit().toInt()
        val order = FbMappings.toOrderSpec(q, pkField, pkDir)
        val where: (Map<String, Any?>) -> Boolean = { row -> FbPredicate.evalWrapper(q.wrapper(), row) }

        // TODO: if you add a real extractor, plug it here.
        // For now, a safe fallback is to scan leaf filters and collect field names.
        val filterFields = FilterFieldExtractor.fromWrapper(q.wrapper())

        return DerivedSpec(
            table = tableName,
            k = k,
            order = order,
            whereMatches = where,
            filterFields = filterFields
        )
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/FilterFieldExtractor.kt
Size: 950 B
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FilterFieldExtractor {
    @JvmStatic
    fun fromWrapper(w: BasicSqlDataFilterWrapper?): Set<String> {
        if (w == null) return emptySet()
        val out = LinkedHashSet<String>()
        walk(w, out)
        return out
    }

    private fun walk(w: BasicSqlDataFilterWrapper, out: MutableSet<String>) {
        for (i in 0 until w.filtersLength()) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() -> {
                    val f = w.filters(BasicSqlDataFilter(), i) as BasicSqlDataFilter
                    out += f.fieldName()
                }
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val child = w.filters(BasicSqlDataFilterWrapper(), i) as BasicSqlDataFilterWrapper
                    walk(child, out)
                }
            }
        }
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/OrderSpec.kt
Size: 1.18 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.*

enum class TimestampUnit { SECONDS, MILLIS, MICROS, NANOS }
enum class Direction { ASC, DESC }
enum class Nulls { FIRST, LAST }

/**
 * One ORDER BY item.
 * - `isPk` must be true for exactly one trailing key (the primary-key tiebreaker).
 * - `nulls` is explicit (no null); default follows SQL: ASC→LAST, DESC→FIRST.
 */
data class OrderKeySpec(
    val field: String,
    val dir: Direction,
    val nulls: Nulls = nullsDefaultFor(dir),
    val isPk: Boolean = false
) {
    companion object {
        @JvmStatic fun of(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, false)

        @JvmStatic fun pk(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, true)

        @JvmStatic fun nullsDefaultFor(d: Direction): Nulls =
            if (d == Direction.ASC) Nulls.LAST else Nulls.FIRST
    }
}

/** Row field accessor (SAM for Java interop). */
fun interface RowAccessor<R> { fun get(row: R, field: String): Any? }

/** Lookup a row by PK (SAM for Java interop). */
fun interface PkLookup<PK, R> { fun get(pk: PK): R? }
-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/PageOps.kt
Size: 3.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

object PageOps {

    data class InsertResult<PK>(
        val inserted: Boolean,
        val pos: Int,            // where we inserted; -1 if not inserted
        val evicted: PK? = null  // tail PK evicted when window was full
    )

    data class RepositionResult<PK>(
        val moved: Boolean,
        val from: Int,
        val to: Int
    )

    /**
     * Lower-bound binary search: first index where [key] could be inserted
     * without violating sorted order (stable insert).
     */
    @JvmStatic
    fun <T> insertionIndex(list: List<T>, key: T, cmp: Comparator<T>): Int {
        var lo = 0
        var hi = list.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val c = cmp.compare(list[mid], key)
            if (c < 0) lo = mid + 1 else hi = mid
        }
        return lo
    }

    /**
     * Insert a PK into a sorted Top-K page if it belongs.
     * - If page.size < K → insert at correct position.
     * - If full and new PK is “better” than current tail → evict tail, insert.
     * - Else → ignore.
     */
    @JvmStatic
    fun <PK> insertIfBetter(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): InsertResult<PK> {
        // If already present, do nothing (call repositionIfNeeded after you’ve merged rowCache)
        val existing = page.indexOf(pk)
        if (existing >= 0) return InsertResult(inserted = false, pos = existing, evicted = null)

        if (page.size < k) {
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = null)
        }

        // Page full: compare against tail
        val tail = page.last()
        if (cmp.compare(pk, tail) < 0) {
            // New item outranks the tail → evict tail and insert
            page.removeAt(page.size - 1)
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = tail)
        }
        return InsertResult(inserted = false, pos = -1, evicted = null)
    }

    /**
     * If a PK is already in the page and its order fields changed,
     * remove it and reinsert at its correct position.
     */
    @JvmStatic
    fun <PK> repositionIfNeeded(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>
    ): RepositionResult<PK> {
        val from = page.indexOf(pk)
        if (from < 0) return RepositionResult(moved = false, from = -1, to = -1)

        // Remove first so the insertion index is computed against the remaining items
        page.removeAt(from)
        val to = insertionIndex(page, pk, cmp)
        page.add(to, pk)
        return RepositionResult(moved = (to != from), from = from, to = to)
    }

    /** Remove PK if present. Returns the removed index or -1. */
    @JvmStatic
    fun <PK> removeIfPresent(page: MutableList<PK>, pk: PK): Int {
        val idx = page.indexOf(pk)
        if (idx >= 0) page.removeAt(idx)
        return idx
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/QueryEngine.kt
Size: 7.69 KB
Code:
package org.cladbe.cdc.engine

import kotlin.math.max

import org.cladbe.cdc.Evaluator.DerivedSpec
import org.cladbe.cdc.Evaluator.Direction
import org.cladbe.cdc.Evaluator.Nulls
import org.cladbe.cdc.Evaluator.OutboxCodec

/** Diff item your outbox uses (same shape as before). */
sealed class WireDiff<PK> {
    data class Added<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    data class Removed<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    data class Modified<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    /** Optional: when an item jumps position due to order-field change */
    data class Promoted<PK>(val pk: PK, val from: Int, val to: Int) : WireDiff<PK>()
}

/**
 * New result shape for upsert. CdcProcessor can use these flags to decide
 * whether to call the RPC backfill (tail probe or fill-a-hole).
 */
data class UpsertResult<PK>(
    val changes: List<OutboxCodec.WireDiff<PK>>,
    val needsBackfill: Boolean,
    val probeTail: Boolean
)

/**
 * QueryEngine maintains the in-memory K page (PK list) and gives deterministic
 * updates in response to row upserts.
 *
 * @param rowProvider  supply previous (cached) row if available (e.g., from rowCache KV store).
 *                     It can return null if unknown; we’ll infer change-sets best-effort.
 */
class QueryEngine<PK>(
    private val spec: DerivedSpec,
    private val pkField: String,
    private val getPk: (Map<String, Any?>) -> PK,
    private val rowProvider: (PK) -> Map<String, Any?>? = { null }
) {
    val page: MutableList<PK> = mutableListOf()
    fun pageCapacity(): Int = spec.k

    fun upsert(row: Map<String, Any?>): UpsertResult<PK> {
        val pk = getPk(row)
        val prev = rowProvider(pk)
        val inPageBefore = page.contains(pk)

        val matchesAfter = spec.whereMatches(row)
        val changed = diffFields(prev, row)

        val orderFields = spec.order.map { it.field }.toSet()
        val orderChanged = changed.any { it in orderFields }
        val filterChanged = changed.any { it in spec.filterFields }

        if (!matchesAfter) {
            if (!inPageBefore) return UpsertResult(emptyList(), false, false)
            val idx = page.indexOf(pk)
            if (idx >= 0) {
                page.removeAt(idx)
                return UpsertResult(
                    changes = listOf(OutboxCodec.WireDiff("removed", pk, pos = idx)),
                    needsBackfill = true,
                    probeTail = false
                )
            }
            return UpsertResult(emptyList(), false, false)
        }

        if (!inPageBefore) {
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            } else {
                val tailPk = page.last()
                val tailRow = rowProvider(tailPk) ?: return UpsertResult(emptyList(), false, false)
                if (compareRows(row, tailRow) < 0) {
                    val evicted = page.removeLast()
                    val pos = findInsertPosition(row)
                    page.add(pos, pk)
                    return UpsertResult(
                        changes = listOf(
                            OutboxCodec.WireDiff("removed", evicted, pos = max(pos, page.size) - 1),
                            OutboxCodec.WireDiff("added", pk, pos = pos)
                        ),
                        needsBackfill = false,
                        probeTail = false
                    )
                }
                return UpsertResult(emptyList(), false, false)
            }
        }

        val oldIdx = page.indexOf(pk)
        if (oldIdx < 0) {
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            } else {
                val tailPk = page.last()
                val tailRow = rowProvider(tailPk)
                return if (tailRow != null && compareRows(row, tailRow) < 0) {
                    val evicted = page.removeLast()
                    val pos = findInsertPosition(row)
                    page.add(pos, pk)
                    UpsertResult(
                        listOf(
                            OutboxCodec.WireDiff("removed", evicted, pos = page.size),
                            OutboxCodec.WireDiff("added", pk, pos = pos)
                        ),
                        false, false
                    )
                } else UpsertResult(emptyList(), false, false)
            }
        }

        if (orderChanged) {
            page.removeAt(oldIdx)
            val newPos = findInsertPosition(row)
            page.add(newPos, pk)
            val diff = OutboxCodec.WireDiff("modified", pk, pos = newPos, from = oldIdx)
            val probe = (filterChanged || orderChanged) && page.size >= spec.k
            return UpsertResult(listOf(diff), needsBackfill = false, probeTail = probe)
        }

        val diffs =
            if (changed.isNotEmpty()) listOf(OutboxCodec.WireDiff("modified", pk, pos = oldIdx, from = oldIdx))
            else emptyList()

        val probe = (filterChanged || orderChanged) && page.size >= spec.k
        return UpsertResult(diffs, needsBackfill = false, probeTail = probe)
    }

    fun currentCursor(): Map<String, Any?>? {
        if (page.isEmpty()) return null
        val tailPk = page.last()
        val tail = rowProvider(tailPk) ?: return null
        val m = LinkedHashMap<String, Any?>(spec.order.size)
        for (ok in spec.order) m[ok.field] = tail[ok.field]
        return m
    }

    // ---- helpers ----
    private fun findInsertPosition(row: Map<String, Any?>): Int {
        var lo = 0
        var hi = page.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val midRow = rowProvider(page[mid])
            val cmp = if (midRow == null) 1 else compareRows(row, midRow)
            if (cmp < 0) hi = mid else lo = mid + 1
        }
        return lo
    }

    private fun compareRows(a: Map<String, Any?>, b: Map<String, Any?>): Int {
        for ((i, ok) in spec.order.withIndex()) {
            val av = a[ok.field]
            val bv = b[ok.field]
            val asc = ok.dir == Direction.ASC
            val nullsFirst = ok.nulls == Nulls.FIRST
            val c = compareField(av, bv, asc, nullsFirst)
            if (c != 0) return c
            if (i == spec.order.lastIndex) return 0
        }
        return 0
    }

    private fun compareField(a: Any?, b: Any?, asc: Boolean, nullsFirst: Boolean): Int {
        if (a == null && b == null) return 0
        if (a == null) return if (nullsFirst) -1 else 1
        if (b == null) return if (nullsFirst)  1 else -1
        val base = when {
            a is Number && b is Number -> a.toDouble().compareTo(b.toDouble())
            a is String && b is String -> a.compareTo(b)
            a is Boolean && b is Boolean -> a.compareTo(b)
            a::class == b::class && a is Comparable<*> -> (a as Comparable<Any>).compareTo(b)
            else -> a.toString().compareTo(b.toString())
        }
        return if (asc) base else -base
    }

    private fun diffFields(prev: Map<String, Any?>?, next: Map<String, Any?>): Set<String> {
        if (prev == null) return next.keys
        val keys = HashSet<String>(next.keys).apply { addAll(prev.keys) }
        val changed = mutableSetOf<String>()
        for (k in keys) if (!eq(prev[k], next[k])) changed += k
        return changed
    }
    private fun eq(a: Any?, b: Any?): Boolean = when {
        a === b -> true
        a == null || b == null -> false
        a is Number && b is Number -> a.toDouble() == b.toDouble()
        else -> a == b
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/Evaluator/SqlComparators.kt
Size: 6.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.math.BigDecimal
import java.time.*
import java.util.Comparator

object SqlComparators {

    /** Comparator for rows, SQL-like semantics with NULLS rule and PK tiebreaker. */
    @JvmStatic
    fun <R> rowComparator(
        spec: List<OrderKeySpec>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<R> {
        validateSpec(spec)
        val s = applyNullDefaults(spec)
        val hints = tsHints

        return Comparator { a, b ->
            for (k in s) {
                var va = acc.get(a, k.field)
                var vb = acc.get(b, k.field)

                // normalize temporals to epoch-nanos (Long)
                val hint = hints[k.field]
                va = normalizeTemporal(va, hint)
                vb = normalizeTemporal(vb, hint)

                val c = compareField(va, vb, k)
                if (c != 0) return@Comparator c
            }
            0 // equal across all fields (including PK)
        }
    }

    /** Comparator for PKs that fetches rows from a cache on demand. */
    @JvmStatic
    fun <PK, R> pkComparator(
        spec: List<OrderKeySpec>,
        lookup: PkLookup<PK, R>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<PK> {
        val rowCmp = rowComparator(spec, acc, tsHints)
        return Comparator { pk1, pk2 ->
            if (pk1 == pk2) return@Comparator 0
            val r1 = lookup.get(pk1)
            val r2 = lookup.get(pk2)
            if (r1 == null && r2 == null) return@Comparator 0
            if (r1 == null) return@Comparator 1
            if (r2 == null) return@Comparator -1
            val c = rowCmp.compare(r1, r2)
            if (c != 0) c else safeCompare(pk1, pk2)
        }
    }

    // ---------- internals ----------

    private fun validateSpec(spec: List<OrderKeySpec>) {
        require(spec.isNotEmpty()) { "orderSpec must contain at least one field (include PK as last)" }
        require(spec.last().isPk) { "orderSpec must end with the PK field as final tiebreaker" }
    }

    private fun applyNullDefaults(inSpec: List<OrderKeySpec>): List<OrderKeySpec> =
        inSpec.map { k ->
            val n = k.nulls ?: OrderKeySpec.nullsDefaultFor(k.dir)
            k.copy(nulls = n)
        }

    private fun compareField(a: Any?, b: Any?, k: OrderKeySpec): Int {
        // 1) NULL rules
        if (a == null || b == null) {
            if (a == null && b == null) return 0
            val firstIsNull = a == null
            return if (k.nulls == Nulls.FIRST)
                if (firstIsNull) -1 else 1
            else
                if (firstIsNull) 1 else -1
        }

        // 2) Non-null comparison
        val base = compareNonNull(a, b)
        if (base == 0) return 0

        // 3) Direction
        return if (k.dir == Direction.ASC) base else -base
    }

    @Suppress("UNCHECKED_CAST")
    private fun compareNonNull(a: Any, b: Any): Int {
        // Numbers → BigDecimal
        if (a is Number && b is Number) {
            val da = toBigDecimal(a)
            val db = toBigDecimal(b)
            return da.compareTo(db)
        }

        // CharSequence → String (safe fallback for mixed types later)
        if (a is CharSequence && b is CharSequence) {
            return a.toString().compareTo(b.toString())
        }

        // Boolean: false < true
        if (a is Boolean && b is Boolean) {
            return java.lang.Boolean.compare(a, b)
        }

        // Same class & Comparable
        if (a::class.java == b::class.java && a is Comparable<*>) {
            return (a as Comparable<Any>).compareTo(b)
        }

        // Fallback: String compare to guarantee total order
        return a.toString().compareTo(b.toString())
    }

    private fun toBigDecimal(n: Number): BigDecimal =
        when (n) {
            is Long, is Int, is Short, is Byte -> BigDecimal.valueOf(n.toLong())
            is BigDecimal -> n
            is Float, is Double -> BigDecimal.valueOf(n.toDouble())
            else -> BigDecimal(n.toString())
        }

    private fun normalizeTemporal(v: Any?, hint: TimestampUnit?): Any? {
        if (v == null) return null

        // java.time first
        when (v) {
            is Instant -> return toEpochNanos(v)
            is OffsetDateTime -> return toEpochNanos(v.toInstant())
            is ZonedDateTime -> return toEpochNanos(v.toInstant())
            is LocalDateTime -> return toEpochNanos(v.toInstant(ZoneOffset.UTC))      // assume UTC
            is LocalDate -> return toEpochNanos(v.atStartOfDay(ZoneOffset.UTC).toInstant())
        }

        // legacy dates
        if (v is java.util.Date) return toEpochNanos(v.toInstant())
        if (v is java.sql.Timestamp) return toEpochNanos(v.toInstant())

        // numeric epochs
        if (v is Number) {
            val raw = v.toLong()
            val unit = hint ?: guessUnit(raw)
            return when (unit) {
                TimestampUnit.SECONDS -> raw * 1_000_000_000L
                TimestampUnit.MILLIS  -> raw * 1_000_000L
                TimestampUnit.MICROS  -> raw * 1_000L
                TimestampUnit.NANOS   -> raw
            }
        }

        // Not temporal → return as-is
        return v
    }

    private fun toEpochNanos(i: Instant): Long =
        i.epochSecond * 1_000_000_000L + i.nano

    private fun guessUnit(v: Long): TimestampUnit {
        val av = kotlin.math.abs(v)
        return when {
            av < 100_000_000_000L -> TimestampUnit.SECONDS      // < 1e11
            av < 100_000_000_000_000L -> TimestampUnit.MILLIS   // < 1e14
            av < 100_000_000_000_000_000L -> TimestampUnit.MICROS // < 1e17
            else -> TimestampUnit.NANOS
        }
    }

    private fun <T> safeCompare(a: T?, b: T?): Int {
        if (a == null && b == null) return 0
        if (a == null) return 1
        if (b == null) return -1
        return if (a::class.java == b::class.java && a is Comparable<*>) {
            @Suppress("UNCHECKED_CAST")
            (a as Comparable<T>).compareTo(b)
        } else {
            a.toString().compareTo(b.toString())
        }
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/App.kt
Size: 1.66 KB
Code:
// src/main/java/org/cladbe/cdc/engine/App.kt
package org.cladbe.cdc.engine

import org.apache.kafka.streams.KafkaStreams
import org.apache.kafka.streams.StreamsConfig
import java.util.Properties
import java.util.regex.Pattern

fun main() {
    val brokers  = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"
    val appId    = System.getenv("STREAMS_APP_ID") ?: "cladbe-kstreams-shared"

    val topics = StreamsTopology.Topics(
        cdcPattern        = Pattern.compile(System.getenv("CDC_TOPIC_PATTERN") ?: "^server\\.cdc\\..*$"),
        queryControlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control",
        seedTopic         = System.getenv("PAGE_SEED_TOPIC") ?: "server.page.seed",
        rpcReplyTopic     = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
        outboxTopic       = System.getenv("WS_OUTBOX_TOPIC") ?: "server.page.diffs",
        backfillTopic     = System.getenv("BACKFILL_ROWS_TOPIC") ?: "server.backfill.rows"
    )

    val topology = StreamsTopology.build(topics)

    val props = Properties().apply {
        put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
        put(StreamsConfig.APPLICATION_ID_CONFIG, appId)
        put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.AT_LEAST_ONCE)
        put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
        put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
    }

    val streams = KafkaStreams(topology, props)
    Runtime.getRuntime().addShutdownHook(Thread { streams.close() })
    streams.start()
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/BackfillClient.kt
Size: 4.34 KB
Code:
// src/main/java/org/cladbe/cdc/engine/BackfillClient.kt
package org.cladbe.cdc.engine

import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.ByteArrayDeserializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.kafka.common.serialization.StringSerializer
import java.time.Duration
import java.util.*
import java.util.concurrent.ThreadLocalRandom

/**
 * Minimal Kafka RPC client for the postgres_rpc worker.
 *
 * NOTE:
 * - We send a RequestEnvelope(GetData) that the TS worker understands.
 * - The worker now supports keyset: orderKeys + cursor + strictAfter on its side.
 * - Response payload "RowsJson.rows[]" are JSON strings (one per row).
 */
class BackfillClient(
    private val requestTopic: String = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests",
    private val responseTopic: String = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
    bootstrapServers: String = System.getenv("KAFKA_BROKERS") ?: "localhost:9092",
    private val groupId: String = System.getenv("SQL_RPC_GROUP_ID") ?: "cladbe-kstreams-backfill"
) : AutoCloseable {

    private val producer = KafkaProducer<String, ByteArray>(
        Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
    )

    private val consumer = KafkaConsumer<String, ByteArray>(
        Properties().apply {
            put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ConsumerConfig.GROUP_ID_CONFIG, "$groupId-${ThreadLocalRandom.current().nextInt(1_000_000)}")
            put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
            put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer::class.java)
            put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest")
            put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true")
        }
    ).also { it.subscribe(listOf(responseTopic)) }

    override fun close() {
        try { consumer.close() } catch (_: Exception) {}
        try { producer.close() } catch (_: Exception) {}
    }

    // ---------- FB builders (RequestEnvelope<GetData>) ----------
    // We reuse your FlatBuffers generated Java from sql_rpc.fbs & sql_schema.fbs
    // Package name below assumes flatc generated into SqlRpc / SqlSchema namespaces.

    fun requestOne(
        companyId: String,
        table: String,
        order: List<OrderKey>,               // simple POJO below
        filtersFbBytes: ByteArray?,          // optional FB BasicSqlDataFilterWrapper (or null)
        cursorTuple: Map<String, Any?>?,     // last-tuple of order keys
        strictAfter: Boolean = true,
        timeoutMs: Long = 1_000
    ): String? /* row JSON */ {
        val corr = UUID.randomUUID().toString()

        val reqBytes = FbRequests.buildGetDataEnvelope(
            correlationId = corr,
            replyTopic = responseTopic,
            companyId = companyId,
            tableName = table,
            order = order,
            wrapperFbBytes = filtersFbBytes,
            limit = 1,
            offset = null,
            cursorTuple = cursorTuple,
            strictAfter = strictAfter
        )

        // send
        producer.send(ProducerRecord(requestTopic, corr, reqBytes))

        // wait single reply for corr id
        val deadline = System.currentTimeMillis() + timeoutMs
        while (System.currentTimeMillis() < deadline) {
            val polled = consumer.poll(Duration.ofMillis(50))
            for (rec in polled) {
                if (rec.key() != corr) continue
                val rowJson = FbResponses.parseRowsJsonFirst(rec.value()) // null if none
                return rowJson
            }
        }
        return null
    }
}

/** Minimal order POJO for the FB builder */
data class OrderKey(val field: String, val sortOrdinal: Int)

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/BackfillReplyProcessor.kt
Size: 2.92 KB
Code:
// src/main/java/org/cladbe/cdc/engine/BackfillReplyProcessor.kt
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore
import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper

/**
 * Consumes SQL-RPC replies (key=corrId, value=FB ResponseEnvelope).
 * If it contains RowsJson with a row, we:
 *   - resolve corrId -> "$table|$hashId" from `pending`
 *   - warm rowCache with the row JSON
 *   - forward the row bytes to the internal backfill topic (via child sink)
 *   - delete the pending entry
 *
 * Value we forward is the raw row JSON bytes; key is unused by CdcProcessor.
 */
class BackfillReplyProcessor(
    private val table: String,
    private val rowCacheName: String,
    private val pendingName: String,
    /** Name of the child sink node wired to the internal backfill topic. */
    private val backfillTopicName: String
) : Processor<String, ByteArray, ByteArray, ByteArray> {

    private lateinit var ctx: ProcessorContext<ByteArray, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var pending : KeyValueStore<String, String>
    private val mapper = ObjectMapper()

    override fun init(context: ProcessorContext<ByteArray, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        pending  = context.getStateStore(pendingName)
    }

    override fun process(record: Record<String, ByteArray>) {
        val corr = record.key() ?: return
        val fb = record.value() ?: return

        val hashKey = pending.get(corr) ?: return // unknown corr → ignore
        val hashId = hashKey.substringAfter('|')

        // Parse first row JSON (if any) from FB ResponseEnvelope
        val rowJson: String = FbResponses.parseRowsJsonFirst(fb) ?: return

        // Decode to Map (for PK extraction) and re-encode to bytes for stores/forward
        val rowMap: Map<String, Any?> = try {
            mapper.readValue(rowJson, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(rowMap, pkCol) ?: return
        val rowBytes = rowJson.toByteArray()

        // Warm row cache (so comparator & engine have the row available)
        rowCache.put(pk, rowBytes)

        // Forward to internal backfill topic (same format as CDC path expects)
        // CdcProcessor ignores key; we keep it simple and use pk bytes.
        ctx.forward(
            Record(pk.toByteArray(), rowBytes, record.timestamp()),
            backfillTopicName // child node name (the sink we attached in Topology)
        )

        // Clear pending
        pending.delete(corr)
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/CdcProcessor.kt
Size: 11.47 KB
Code:
// src/main/java/org/cladbe/cdc/engine/Processors.kt  (CdcProcessor part)
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.header.internals.RecordHeaders
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.streams.processor.Cancellable
import org.apache.kafka.streams.processor.PunctuationType
import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore
import org.cladbe.cdc.Evaluator.FbQueryAdapterImpl
import org.cladbe.cdc.Evaluator.OutboxCodec
import java.time.Duration
import java.util.*
import kotlin.math.max

class CdcProcessor(
    private val rowCacheName: String,
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String,
    private val pendingName: String? = null
) : Processor<ByteArray, ByteArray, String, ByteArray> {

    private lateinit var ctx: ProcessorContext<String, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var qmeta   : KeyValueStore<String, ByteArray>
    private lateinit var page    : KeyValueStore<String, ByteArray>
    private lateinit var outbox  : KeyValueStore<String, ByteArray>
    private var pending          : KeyValueStore<String, String>? = null

    private val engines = mutableMapOf<String, QueryEngine<String>>() // key: "$table|$hashId"
    private val mapper = ObjectMapper()

    // adaptive flush
    private var flushTask: Cancellable? = null
    private var flushIntervalMs = 200L
    private val minFlushMs = 50L
    private val maxFlushMs = 5_000L
    private val batchChangeThreshold = 64
    private val hardMaxLatencyMs = 750L

    private var pendingChangeCount = 0
    private var lastFlushMs = 0L
    private var lastActivityMs = 0L
    private var lastSeenLsn: Long = 0L
    private val dirtyHashes = mutableSetOf<String>()
    private var keydb: KeyDbWriter? = null

    // backfill
    private lateinit var rpcProducer: KafkaProducer<ByteArray, ByteArray>
    private val requestTopic = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests"
    private val replyTopic   = System.getenv("SQL_RPC_REPLY_TOPIC")
        ?: System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses"
    private val rpcBrokers   = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"

    override fun init(context: ProcessorContext<String, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        qmeta    = context.getStateStore(qmetaName)
        page     = context.getStateStore(pageName)
        outbox   = context.getStateStore(outboxName)
        if (pendingName != null) pending = context.getStateStore(pendingName)

        val now = System.currentTimeMillis()
        lastFlushMs = now
        lastActivityMs = now

        scheduleFlush(flushIntervalMs)
        rpcProducer = newProducer()
        runCatching { keydb = KeyDbWriter() }.onFailure { e ->
            println("[streams] KeyDB disabled: ${e.message}")
        }
    }

    override fun process(record: Record<ByteArray, ByteArray>) {
        // --- derive table from topic name ---
        val topic = ctx.recordMetadata().orElse(null)?.topic() ?: return
        // expect topics like "server.cdc.tenant_table"
        val table = topic.substringAfter("server.cdc.", topic.substringAfterLast('.'))

        val row: Map<String, Any?> = decodeRow(record.value()) ?: return
        extractLsn(record.value())?.let { lsn -> if (lsn > lastSeenLsn) lastSeenLsn = lsn }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(row, pkCol) ?: return

        rowCache.put(pk, record.value())
        keydb?.writeHotRow(table = table, pk = pk, row = row, lsn = lastSeenLsn)

        var producedChanges = 0

        // Iterate only queries for this table: keys starting with "$table|"
        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val entry = iter.next()
                val key = entry.key
                if (!key.startsWith("$table|")) continue
                val hashKey = key // "$table|$hashId"

                val engine = engines.getOrPut(hashKey) {
                    val spec = FbQueryAdapterImpl(tableName = table, pkField = pkCol).parse(entry.value)
                    QueryEngine(
                        spec = spec,
                        pkField = pkCol,
                        getPk = { r -> extractPkOrNull(r, pkCol)!! },
                        rowProvider = { p -> rowFromCache(p) }
                    )
                }

                val result = engine.upsert(row)

                if (result.changes.isNotEmpty()) {
                    producedChanges += result.changes.size
                    page.put(hashKey, encodePkList(engine.page))
                    outbox.put(hashKey, appendOutbox(outbox.get(hashKey), result.changes))
                    dirtyHashes += hashKey
                }

                when {
                    result.needsBackfill -> triggerBackfill(hashKey, engine, table)
                    result.probeTail     -> triggerBackfill(hashKey, engine, table)
                }
            }
        }

        if (producedChanges > 0) {
            lastActivityMs = System.currentTimeMillis()
            pendingChangeCount += producedChanges
            val age = lastActivityMs - lastFlushMs
            val hitBatch = pendingChangeCount >= batchChangeThreshold
            val hitAge   = age >= hardMaxLatencyMs
            if (hitBatch || hitAge) {
                flushAll()
                if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            } else if (flushIntervalMs > 2 * minFlushMs) {
                rescheduleFlush(flushIntervalMs / 2)
            }
        }
    }

    private fun onFlushTick() {
        if (pendingChangeCount > 0) {
            flushAll()
            if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            return
        }
        val newInterval = (flushIntervalMs * 2).coerceAtMost(maxFlushMs)
        if (newInterval != flushIntervalMs) rescheduleFlush(newInterval)
    }

    private fun flushAll() {
        var flushedAny = false
        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val e = iter.next()
                val hashKey = e.key              // "$table|$hashId"
                val hashId  = hashKey.substringAfter('|')
                val ob = outbox.get(hashKey) ?: continue
                if (ob.isEmpty()) continue

                val headers = RecordHeaders().apply {
                    if (lastSeenLsn > 0L) add("lsn", longToBe(lastSeenLsn))
                }
                ctx.forward(Record(hashId, ob, System.currentTimeMillis(), headers))

                runCatching {
                    val b64 = java.util.Base64.getEncoder().encodeToString(ob)
                    keydb?.appendDiff(hashId, lastSeenLsn, b64)
                }

                outbox.put(hashKey, ByteArray(0))
                flushedAny = true

                if (hashKey in dirtyHashes) {
                    val engine = engines[hashKey]
                    if (engine != null) {
                        runCatching {
                            keydb?.writeRangeIndex(hashId, engine.page.toList())
                            keydb?.writeReverseIndex(hashId, engine.page.toList())
                        }
                        val rows = buildSnapshotRows(hashKey)
                        runCatching { keydb?.writeSnapshot(hashId, rows, lastSeenLsn) }
                    }
                    dirtyHashes.remove(hashKey)
                }
            }
        }
        if (flushedAny) {
            pendingChangeCount = 0
            lastFlushMs = System.currentTimeMillis()
        }
    }

    override fun close() {
        flushTask?.cancel()
        runCatching { rpcProducer.close() }
    }

    private fun triggerBackfill(hashKey: String, engine: QueryEngine<String>, table: String) {
        val tailCursor = engine.currentCursor() ?: return
        val qfb = qmeta.get(hashKey) ?: return

        val corr = UUID.randomUUID().toString()
        pending?.put(corr, hashKey)

        val reqBytes = FbRequests.buildGetDataFromQueryFb(
            correlationId = corr,
            replyTopic = replyTopic,
            queryFb = qfb,
            cursorTuple = tailCursor,
            limit = 1,
            strictlyAfter = true,
            fullTableName = table
        )
        rpcProducer.send(ProducerRecord(requestTopic, corr.toByteArray(), reqBytes))
    }

    // ---- timers & helpers ----
    private fun scheduleFlush(intervalMs: Long) {
        flushTask?.cancel()
        flushIntervalMs = intervalMs
        flushTask = ctx.schedule(Duration.ofMillis(intervalMs), PunctuationType.WALL_CLOCK_TIME) { onFlushTick() }
    }
    private fun rescheduleFlush(newIntervalMs: Long) {
        if (newIntervalMs == flushIntervalMs) return
        scheduleFlush(newIntervalMs)
    }

    private fun rowFromCache(pk: String): Map<String, Any?>? {
        val bytes = rowCache.get(pk) ?: return null
        return decodeRow(bytes)
    }
    @Suppress("UNCHECKED_CAST")
    private fun decodeRow(bytes: ByteArray): Map<String, Any?>? =
        try { mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {}) } catch (_: Exception) { null }

    private fun encodePkList(list: List<String>): ByteArray = list.joinToString(",").toByteArray()
    private fun appendOutbox(prev: ByteArray?, changes: List<OutboxCodec.WireDiff<String>>): ByteArray {
        val add = changes.joinToString("|") { "${it.type}:${it.pk}@${it.pos ?: -1},${it.from ?: -1}" }
        val s = (prev?.toString(Charsets.UTF_8)?.takeIf { it.isNotEmpty() }?.plus("||") ?: "") + add
        return s.toByteArray()
    }
    private fun buildSnapshotRows(hashKey: String): List<Map<String, Any?>> {
        val engine = engines[hashKey] ?: return emptyList()
        val rows = ArrayList<Map<String, Any?>>(engine.page.size)
        for (pk in engine.page) rowFromCache(pk)?.let { rows += it }
        return rows
    }
    private fun newProducer(): KafkaProducer<ByteArray, ByteArray> {
        val props = java.util.Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, rpcBrokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
            put(ProducerConfig.LINGER_MS_CONFIG, "5")
        }
        return KafkaProducer(props)
    }
    private fun extractLsn(bytes: ByteArray): Long? = try {
        val m: Map<String, Any?> = mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {})
        when (val v = m["__lsn"]) {
            is Number -> v.toLong()
            is String -> v.toLongOrNull()
            is Map<*, *> -> (v["long"] as? Number)?.toLong() ?: (v["long"] as? String)?.toLongOrNull()
            else -> null
        }
    } catch (_: Exception) { null }

    private fun longToBe(v: Long): ByteArray {
        val b = ByteArray(8)
        b[0] = (v ushr 56).toByte(); b[1] = (v ushr 48).toByte()
        b[2] = (v ushr 40).toByte(); b[3] = (v ushr 32).toByte()
        b[4] = (v ushr 24).toByte(); b[5] = (v ushr 16).toByte()
        b[6] = (v ushr 8).toByte();  b[7] = (v).toByte()
        return b
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/FbRequests.kt
Size: 13.73 KB
Code:
// src/main/java/org/cladbe/cdc/engine/FbRequests.kt
package org.cladbe.cdc.engine

import com.google.flatbuffers.FlatBufferBuilder
import SqlRpc.*
import SqlSchema.*
import java.nio.ByteBuffer

object FbRequests {

    fun buildGetDataEnvelope(
        correlationId: String,
        replyTopic: String,
        companyId: String,
        tableName: String,
        order: List<OrderKey>,
        wrapperFbBytes: ByteArray?,
        limit: Int,
        offset: Int?,
        cursorTuple: Map<String, Any?>?,
        strictAfter: Boolean
    ): ByteArray {

        val b = FlatBufferBuilder(1024)

        // ---- Order vec ----
        val orderOffsets = order.map { ok ->
            val fieldOff = b.createString(ok.field)
            SqlSchema.OrderKeySpec.createOrderKeySpec(
                b,
                fieldOff,
                ok.sortOrdinal,   // must match rpc schema enum ordinals
                false
            )
        }.toIntArray()
        val orderVec = GetDataReq.createOrderVector(b, orderOffsets)

        // ---- Wrapper: pass through existing FB bytes (optional) ----
        val wrapperOff = wrapperFbBytes?.let {
            val bb = ByteBuffer.wrap(it)
            val w = BasicSqlDataFilterWrapper.getRootAsBasicSqlDataFilterWrapper(bb)
            // Repack wrapper into this builder:
            // … easiest: copy the bytes into a string field or rebuild; for simplicity we rebuild shallowly:
            // We’ll assume caller passes null or *already* built from THIS schema/builder in production.
            // For now, omit when null:
            // (If you already store qmeta as FB bytes from same schema version, you can rebuild fully here.)
            null
        } ?: 0

        // ---- Cursor entries (keyset) ----
        val cursorOffsets = cursorTuple?.entries?.map { (field, value) ->
            val fieldOff = b.createString(field)
            val (valType, valOff) = encodeFilterValue(b, value)
            CursorEntry.createCursorEntry(b, fieldOff, valType, valOff)
        }?.toIntArray()

        val cursorVec = if (cursorOffsets != null && cursorOffsets.isNotEmpty())
            GetDataReq.createCursorVector(b, cursorOffsets)
        else 0

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        val companyOff = b.createString(companyId)
        val tableOff = b.createString(tableName)

        val reqOff = GetDataReq.createGetDataReq(
            b,
            companyOff,
            tableOff,
            wrapperOff,
            limit.toLong(),
            (offset ?: 0).toLong(),
            orderVec,
            cursorVec,
            strictAfter
        )

        val envOff = RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(envOff)
        return b.sizedByteArray()
    }

    /**
     * Build RequestEnvelope(GetData) from a stored StreamingSqlDataFilter (queryFb),
     * adding a keyset "strictly after cursorTuple" constraint.
     *
     * @param fullTableName e.g. "acme_orders" so we can split company/table parts.
     */
    fun buildGetDataFromQueryFb(
        correlationId: String,
        replyTopic: String,
        queryFb: ByteArray,
        cursorTuple: Map<String, Any?>,
        limit: Int,
        strictlyAfter: Boolean,
        fullTableName: String
    ): ByteArray {
        val bb = ByteBuffer.wrap(queryFb)
        val src = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(bb)

        val companyId = fullTableName.substringBefore('_', "")
        val baseTable = fullTableName.substringAfter('_', fullTableName)

        val b = FlatBufferBuilder(1024)

        // Copy ORDER keys
        val orderOffsets = IntArray(src.orderLength())
        for (i in 0 until src.orderLength()) {
            val ok = src.order(i)!!
            val fieldOff = b.createString(ok.field())
            val okOff = SqlSchema.OrderKeySpec.createOrderKeySpec(
                b, fieldOff, ok.sort(), ok.isPk()
            )
            orderOffsets[i] = okOff
        }
        val orderVec = SqlRpc.GetDataReq.createOrderVector(b, orderOffsets)

        // Compose wrapper = AND(userWrapper, strictlyAfter(cursor))
        val userWrapper = src.wrapper()
        val afterOff = buildStrictlyAfterWrapper(b, cursorTuple, src)
        val wrapperOff = if (userWrapper == null) {
            afterOff
        } else {
            val children = intArrayOf(copyWrapper(b, userWrapper), afterOff)
            val unionTypes = byteArrayOf(
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte(),
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
            )
            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(
                b,
                BasicSqlDataFilterWrapper.createFiltersVector(b, children)
            )
            BasicSqlDataFilterWrapper.addFiltersType(
                b,
                BasicSqlDataFilterWrapper.createFiltersTypeVector(b, unionTypes)
            )
            BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
        }

        val cidOff  = b.createString(companyId)
        val tblOff  = b.createString(baseTable)
        val reqOff = SqlRpc.GetDataReq.createGetDataReq(
            b,
            cidOff,
            tblOff,
            wrapperOff,
            limit.toLong(),
            0L,
            orderVec,
            0,
            strictlyAfter
        )

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        val env = SqlRpc.RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(env)
        return b.sizedByteArray()
    }

    // ---------------- helpers moved here from CdcProcessor ----------------

    private fun buildStrictlyAfterWrapper(
        b: FlatBufferBuilder,
        cursorTuple: Map<String, Any?>,
        src: StreamingSqlDataFilter
    ): Int {
        val disjChildren = mutableListOf<Int>()
        val disjTypes = mutableListOf<Byte>()

        val n = src.orderLength()
        for (k in 0 until n) {
            val conj = mutableListOf<Int>()
            val conjTypes = mutableListOf<Byte>()

            for (i in 0 until k) {
                val ok = src.order(i)!!
                val fName = ok.field()
                val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])
                conj += buildBinaryFilter(b, fName, t, vOff, BasicSqlDataFilterType.equals)
                conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
            }

            val ok = src.order(k)!!
            val fName = ok.field()
            val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])

            val type = when (ok.sort()) {
                OrderSort.ASC_DEFAULT, OrderSort.ASC_NULLS_FIRST, OrderSort.ASC_NULLS_LAST ->
                    BasicSqlDataFilterType.greaterThan
                else -> BasicSqlDataFilterType.lessThan
            }
            conj += buildBinaryFilter(b, fName, t, vOff, type)
            conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()

            val conjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, conj.toIntArray())
            val conjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, conjTypes.toByteArray())

            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(b, conjVec)
            BasicSqlDataFilterWrapper.addFiltersType(b, conjTypeVec)
            val conjOff = BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)

            disjChildren += conjOff
            disjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
        }

        val disjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, disjChildren.toIntArray())
        val disjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, disjTypes.toByteArray())
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.or)
        BasicSqlDataFilterWrapper.addFilters(b, disjVec)
        BasicSqlDataFilterWrapper.addFiltersType(b, disjTypeVec)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyWrapper(b: FlatBufferBuilder, w: BasicSqlDataFilterWrapper?): Int {
        if (w == null) return 0
        val count = w.filtersLength()
        val children = IntArray(count)
        val types = ByteArray(count)

        for (i in 0 until count) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val childW = BasicSqlDataFilterWrapper()
                    w.filters(childW, i)
                    children[i] = copyWrapper(b, childW)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
                }
                else -> {
                    val childL = BasicSqlDataFilter()
                    w.filters(childL, i)
                    children[i] = copyLeaf(b, childL)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
                }
            }
        }

        val fv = BasicSqlDataFilterWrapper.createFiltersVector(b, children)
        val tv = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, types)
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, w.filterWrapperType())
        BasicSqlDataFilterWrapper.addFilters(b, fv)
        BasicSqlDataFilterWrapper.addFiltersType(b, tv)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyLeaf(b: FlatBufferBuilder, lf: BasicSqlDataFilter): Int {
        val fieldOff = b.createString(lf.fieldName())

        val (vt: Byte, vOff: Int) = when (lf.valueType().toInt()) {
            FilterValue.StringValue.toInt() -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
            FilterValue.NumberValue.toInt() -> {
                val tmp = NumberValue(); lf.value(tmp)
                FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, tmp.value())
            }
            FilterValue.Int64Value.toInt() -> {
                val tmp = Int64Value(); lf.value(tmp)
                FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, tmp.value())
            }
            FilterValue.BoolValue.toInt() -> {
                val tmp = BoolValue(); lf.value(tmp)
                FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, tmp.value())
            }
            FilterValue.NullValue.toInt() -> {
                NullValue.startNullValue(b)
                val nv = NullValue.endNullValue(b)
                FilterValue.NullValue.toByte() to nv
            }
            else -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
        }

        val mod = lf.modifier()
        val modOff = if (mod == null) 0 else run {
            SqlFilterModifier.startSqlFilterModifier(b)
            SqlFilterModifier.addDistinct(b, mod.distinct())
            SqlFilterModifier.addCaseInsensitive(b, mod.caseInsensitive())
            SqlFilterModifier.addNullsOrder(b, mod.nullsOrder())
            SqlFilterModifier.endSqlFilterModifier(b)
        }

        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, lf.filterType())
        if (modOff != 0) BasicSqlDataFilter.addModifier(b, modOff)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun buildBinaryFilter(
        b: FlatBufferBuilder,
        field: String,
        vt: Byte,
        vOff: Int,
        type: Int
    ): Int {
        val fieldOff = b.createString(field)
        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, type)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun encodeFilterValue(b: FlatBufferBuilder, v: Any?): Pair<Byte, Int> {
        if (v == null) {
            NullValue.startNullValue(b)
            val off = NullValue.endNullValue(b)
            return FilterValue.NullValue.toByte() to off
        }
        return when (v) {
            is String  -> FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(v))
            is Boolean -> FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, v)
            is Int     -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v.toLong())
            is Long    -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v)
            is Number  -> FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, v.toDouble())
            else       -> {
                val s = v.toString()
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(s))
            }
        }
    }


}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/FbResponses.kt
Size: 636 B
Code:
// src/main/java/org/cladbe/cdc/engine/FbResponses.kt
package org.cladbe.cdc.engine

import SqlRpc.*
import java.nio.ByteBuffer

object FbResponses {
    /** Returns first row JSON (string) from RowsJson, or null if none/other response */
    fun parseRowsJsonFirst(bytes: ByteArray): String? {
        val bb = ByteBuffer.wrap(bytes)
        val env = ResponseEnvelope.getRootAsResponseEnvelope(bb)
        if (!env.ok()) return null
        if (env.dataType() != RpcResponse.RowsJson) return null
        val rows = RowsJson()
        env.data(rows)
        if (rows.rowsLength() <= 0) return null
        return rows.rows(0)
    }
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/KeyDbWriter.kt
Size: 3.35 KB
Code:
// src/main/java/org/cladbe/cdc/engine/KeyDbWriter.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.databind.ObjectMapper
import redis.clients.jedis.JedisPooled
import java.net.URI

class KeyDbWriter(
    url: String = System.getenv("KEYDB_URL") ?: "redis://127.0.0.1:6379",
    private val prefix: String = (System.getenv("KEYDB_PREFIX") ?: "hcache:").replace("\\s".toRegex(), ""),
    private val maxDiffs: Int = (System.getenv("HOTCACHE_MAX_DIFFS") ?: "5000").toInt(),
    private val retentionMs: Long = (System.getenv("HOTCACHE_RETENTION_MS") ?: "${10 * 60_000}").toLong()
) : AutoCloseable {

    private val jedis = JedisPooled(URI(url))
    private val mapper = ObjectMapper()

    private fun kSnap(hashId: String)  = "${prefix}snap:$hashId"
    private fun kDiff(hashId: String)  = "${prefix}diff:$hashId"
    private fun kRange(hashId: String) = "${prefix}range:$hashId"
    private fun kRow(table: String, pk: String) = "${prefix}row:$table:$pk"

    /** snap:{hashId} → {"rows":[...], "cursor":{"lsn": "..."}, "ts": epochMillis} */
    fun writeSnapshot(hashId: String, rows: List<Map<String, Any?>>, lsn: Long, ts: Long = System.currentTimeMillis()) {
        val payload = mapOf("rows" to rows, "cursor" to mapOf("lsn" to lsn.toString()), "ts" to ts)
        val json = mapper.writeValueAsString(payload)
        if (retentionMs > 0) jedis.psetex(kSnap(hashId), retentionMs, json) else jedis.set(kSnap(hashId), json)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs) // keep list alive in step with snap
    }

    /** diff:{hashId} → RPUSH {"lsn":"...", "b64":"..."}; LTRIM; PEXPIRE */
    fun appendDiff(hashId: String, lsn: Long, b64: String) {
        val item = mapper.writeValueAsString(mapOf("lsn" to lsn.toString(), "b64" to b64))
        jedis.rpush(kDiff(hashId), item)
        jedis.ltrim(kDiff(hashId), -maxDiffs.toLong(), -1)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs)
    }

    /** range:{hashId} → list of PKs (index = position) */
    fun writeRangeIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRange(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) jedis.rpush(key, *pksInOrder.toTypedArray())
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    /** row:{table}:{pk} → {"row":{...},"lsn":"..."} (hot row cache for ws_gateway) */
    fun writeHotRow(table: String, pk: String, row: Map<String, Any?>, lsn: Long?) {
        val payload = if (lsn != null && lsn > 0) mapOf("row" to row, "lsn" to lsn.toString()) else mapOf("row" to row)
        val json = mapper.writeValueAsString(payload)
        val key = kRow(table, pk)
        if (retentionMs > 0) jedis.psetex(key, retentionMs, json) else jedis.set(key, json)
    }

    private fun kRev(hashId: String) = "${prefix}ridx:$hashId"

    /** ridx:{hashId} → Hash pk -> pos */
    fun writeReverseIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRev(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) {
            val kv = mutableListOf<String>()
            pksInOrder.forEachIndexed { i, pk ->
                kv += pk; kv += i.toString()
            }
            jedis.hset(key, kv.chunked(2).associate { it[0] to it[1] })
        }
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    override fun close() {
        runCatching { jedis.close() }
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/PkUtils.kt
Size: 207 B
Code:
package org.cladbe.cdc.engine

/** Extract the PK value from a decoded row using the configured PK column. */
fun extractPkOrNull(row: Map<String, Any?>, pkCol: String): String? =
    row[pkCol]?.toString()

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/QueryControlProcessor.kt
Size: 1.30 KB
Code:
// src/main/java/org/cladbe/cdc/engine/Processors.kt  (QueryControlProcessor part)
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore

class QueryControlProcessor(
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var qmeta: KeyValueStore<String, ByteArray>
    private lateinit var page : KeyValueStore<String, ByteArray>
    private lateinit var out  : KeyValueStore<String, ByteArray>

    override fun init(context: ProcessorContext<Void, Void>) {
        qmeta = context.getStateStore(qmetaName)
        page  = context.getStateStore(pageName)
        out   = context.getStateStore(outboxName)
    }

    override fun process(record: Record<String, ByteArray>) {
        // Expect key = "$table|$hashId"
        val key = record.key() ?: return
        val fb = record.value()

        if (fb == null) {
            page.delete(key)
            out.delete(key)
            qmeta.delete(key)
            return
        }

        qmeta.put(key, fb)
        if (page.get(key) == null) page.put(key, ByteArray(0))
        if (out.get(key)  == null) out.put(key, ByteArray(0))
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/SeedProcessor.kt
Size: 2.39 KB
Code:
// src/main/java/org/cladbe/cdc/engine/SeedProcessor.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore

class SeedProcessor(
    private val rowCacheName: String,
    private val pageName: String,
    // allow tests to inject a fake/mocked KeyDbWriter
    private val keyDbFactory: () -> KeyDbWriter = { KeyDbWriter() }
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var ctx: ProcessorContext<Void, Void>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var page: KeyValueStore<String, ByteArray>
    private val mapper = ObjectMapper()
    private var keydb: KeyDbWriter? = null

    override fun init(context: ProcessorContext<Void, Void>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        page = context.getStateStore(pageName)
        runCatching { keydb = keyDbFactory() }.onFailure { e ->
            println("[streams] KeyDB disabled in SeedProcessor: ${e.message}")
        }
    }

    override fun process(record: Record<String, ByteArray>) {
        val payload = record.value() ?: return
        val m: Map<String, Any?> = try {
            mapper.readValue(payload, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val table = (m["table"] as? String) ?: return
        val hashId = (m["hashId"] as? String) ?: return
        @Suppress("UNCHECKED_CAST")
        val rows = (m["rows"] as? List<Map<String, Any?>>) ?: return

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"

        val pks = ArrayList<String>(rows.size)
        for (r in rows) {
            val pk = extractPkOrNull(r, pkCol) ?: continue
            rowCache.put(pk, mapper.writeValueAsBytes(r))
            keydb?.writeHotRow(table = table, pk = pk, row = r, lsn = 0L)
            pks += pk
        }

        page.put("$table|$hashId", pks.joinToString(",").toByteArray())

        runCatching {
            keydb?.writeRangeIndex(hashId, pks)
            keydb?.writeSnapshot(hashId, rows, lsn = 0L)
        }
    }

    override fun close() {
        runCatching { keydb?.close() }
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/TablePkRegistry.kt
Size: 926 B
Code:
package org.cladbe.cdc.engine

/**
 * Immutable registry of primary-key *column names* per base table.
 * If your Kafka topic/table name is "tenant_table", we'll also try the base part after '_' so
 * the same map works for all tenants that share schema.
 */
object TablePkRegistry {
    // Map base table -> PK column name (edit as needed)
    private val pkByBase: Map<String, String> = mapOf(
        "a_notes"            to "id",
        "whatsapp_messages"  to "message_uuid",
        "a_timeline"         to "messageId"
        // add the rest of your tables here…
    )

    /** For names like "tenant_table", return "table"; if no '_', return input. */
    private fun baseName(table: String): String =
        table.substringAfter('_', table)

    /** Return PK column name for this table (or null if unknown). */
    fun pkColumnFor(table: String): String? =
        pkByBase[table] ?: pkByBase[baseName(table)]
}

-------- [ Separator ] ------

File Name: org/cladbe/cdc/engine/Topology.kt
Size: 5.91 KB
Code:
// src/main/java/org/cladbe/cdc/engine/Topology.kt
package org.cladbe.cdc.engine

import org.apache.kafka.common.serialization.*
import org.apache.kafka.streams.Topology
import org.apache.kafka.streams.state.Stores
import java.util.regex.Pattern

object StreamsTopology {

    data class Topics(
        val cdcPattern: Pattern,        // <-- regex for ALL CDC topics, e.g. ^server\.cdc\..*$
        val queryControlTopic: String,  // qmeta publish/clear; key MUST be "$table|$hashId"
        val seedTopic: String?,         // optional: seeds; payload must include "table" and "hashId"
        val rpcReplyTopic: String,      // SQL-RPC replies
        val outboxTopic: String,        // diffs to ws_gateway (key = hashId, value = batch bytes)
        val backfillTopic: String       // internal backfill rows (value=row bytes)
    )

    data class StateStoreNames(        // shared stores (no -$table suffix)
        val qmeta: String   = "qmeta",
        val page: String    = "page",
        val outbox: String  = "outbox",
        val rowCache: String= "rowCache",
        val pending: String = "pending"
    )

    /** Single topology that handles *all* tables; table is derived per-record from the topic name. */
    fun build(topics: Topics, stores: StateStoreNames = StateStoreNames()): Topology {
        val top = Topology()

        // ----- State stores (persistent, compacted)
        val qmetaStore = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.qmeta),
            Serdes.String(), Serdes.ByteArray())
        val pageStore  = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.page),
            Serdes.String(), Serdes.ByteArray())
        val outboxStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.outbox),
            Serdes.String(), Serdes.ByteArray())
        val rowCacheStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.rowCache),
            Serdes.String(), Serdes.ByteArray())
        val pendingStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.pending),
            Serdes.String(), Serdes.String())

        // ----- Sources (shared)
        val cdcSource = "src-cdc"
        top.addSource(
            cdcSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.cdcPattern                                         // <-- regex subscription
        )

        val qctlSource = "src-qctl"
        top.addSource(
            qctlSource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.queryControlTopic
        )

        val backfillSource = "src-backfill"
        top.addSource(
            backfillSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.backfillTopic
        )

        val rpcReplySource = "src-rpc-replies"
        top.addSource(
            rpcReplySource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.rpcReplyTopic
        )

        val seedSource: String? = topics.seedTopic?.let {
            "src-seed".also { name ->
                top.addSource(
                    name,
                    StringDeserializer(), ByteArrayDeserializer(),
                    it
                )
            }
        }

        // ----- Processors

        // Query control (key must be "$table|$hashId")
        val qctlProc = "proc-qctl"
        top.addProcessor(qctlProc, {
            QueryControlProcessor(
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox
            )
        }, qctlSource)
        top.addStateStore(qmetaStore, qctlProc)
        top.addStateStore(pageStore, qctlProc)
        top.addStateStore(outboxStore, qctlProc)

        // CDC + Backfill rows → single processor handling all tables
        val cdcProc = "proc-cdc"
        top.addProcessor(cdcProc, {
            CdcProcessor(
                rowCacheName = stores.rowCache,
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox,
                pendingName = stores.pending
            )
        }, cdcSource, backfillSource)
        top.addStateStore(rowCacheStore, cdcProc)
        top.addStateStore(qmetaStore, cdcProc)
        top.addStateStore(pageStore, cdcProc)
        top.addStateStore(outboxStore, cdcProc)
        top.addStateStore(pendingStore, cdcProc)

        // Seeds (optional) → warm rowCache + page
        if (seedSource != null) {
            val seedProc = "proc-seed"
            top.addProcessor(seedProc, {
                SeedProcessor(
                    rowCacheName = stores.rowCache,
                    pageName = stores.page
                )
            }, seedSource)
            top.addStateStore(rowCacheStore, seedProc)
            top.addStateStore(pageStore, seedProc)
        }

        // SQL-RPC replies → emit to internal backfill topic, warm rowCache
        val backfillProc = "proc-backfill-replies"
        val backfillSink = "sink-backfill"
        top.addProcessor(backfillProc, {
            BackfillReplyProcessor(
                table = "<dynamic>",                 // not used for keying; safe to leave placeholder
                rowCacheName = stores.rowCache,
                pendingName = stores.pending,
                backfillTopicName = backfillSink
            )
        }, rpcReplySource)
        top.addStateStore(rowCacheStore, backfillProc)
        top.addStateStore(pendingStore, backfillProc)
        top.addSink(
            backfillSink,
            topics.backfillTopic,
            ByteArraySerializer(), ByteArraySerializer(),
            backfillProc
        )

        // Outbox sink to ws_gateway consumer topic (key = hashId only)
        val outboxSink = "sink-outbox"
        top.addSink(
            outboxSink,
            topics.outboxTopic,
            StringSerializer(), ByteArraySerializer(),
            cdcProc
        )

        return top
    }
}
-------- [ Separator ] ------

File Name: org/cladbe/tools/SendQctl.kt
Size: 2.71 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.StringSerializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import java.util.Properties
import java.nio.ByteBuffer
import com.google.flatbuffers.FlatBufferBuilder
import SqlSchema.*
import SqlSchema.StreamingSqlDataFilter as Q

object Tools {
    @JvmStatic
    fun main(args: Array<String>) {
        val brokers = System.getenv("KAFKA_BROKERS") ?: "localhost:29092"
        val qctlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control"

        if (args.isEmpty()) {
            println("Usage: Tools install-notes-query");
            return
        }
        when (args[0]) {
            "install-notes-query" -> {
                val key = "a_notes|demo" // table|hashId
                val fb = buildNotesQueryFb() // ORDER BY __lsn DESC, id DESC (id is PK) LIMIT 10
                send(kTopic = qctlTopic, key = key, value = fb, brokers = brokers)
                println("Installed query meta for key=$key to $qctlTopic")
            }
            else -> println("Unknown cmd: ${args[0]}")
        }
    }

    private fun send(kTopic: String, key: String, value: ByteArray, brokers: String) {
        val props = Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
        KafkaProducer<String, ByteArray>(props).use { p ->
            p.send(ProducerRecord(kTopic, key, value)).get()
        }
    }

    /**
     * StreamingSqlDataFilter:
     *   table: a_notes (implicit in key)
     *   limit: 10
     *   order: [ __lsn DESC, id DESC (isPk=true) ]
     *   where: (null)
     */
    private fun buildNotesQueryFb(): ByteArray {
        val b = FlatBufferBuilder(256)

        val order = intArrayOf(
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("__lsn"),
                OrderSort.DESC_DEFAULT, false
            ),
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("id"),
                OrderSort.DESC_DEFAULT, true  // PK tiebreaker
            )
        )
        val orderVec = Q.createOrderVector(b, order)
        Q.startStreamingSqlDataFilter(b)
        Q.addLimit(b, 10)
        Q.addOrder(b, orderVec)
        val off = Q.endStreamingSqlDataFilter(b)
        b.finish(off)
        return b.sizedByteArray()
    }
}
-------- [ Separator ] ------

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/DerivedSpec.kt
Size: 1.00 KB
Code:
package org.cladbe.cdc.Evaluator

/**
 * Fully-derived spec the QueryEngine needs at runtime.
 *
 * - `order` MUST end with the PK key (isPk=true), giving a total order.
 * - `whereMatches` implements the user predicate with exact SQL semantics.
 * - `filterFields` includes every field referenced by WHERE; the engine uses it
 *   to detect when an UPDATE changes “match-ness” and whether to probe backfill.
 * - `k` is the page capacity.
 */
data class DerivedSpec(
    val table: String,
    val k: Int,
    val order: List<OrderKeySpec>,
    val whereMatches: (Map<String, Any?>) -> Boolean,
    val filterFields: Set<String>
) {
    init {
        require(order.isNotEmpty()) { "order must not be empty" }
        require(order.last().isPk) { "last order key must be the PK (isPk=true)" }
        require(order.count { it.isPk } == 1) { "exactly one order key must be marked isPk=true" }
    }

    /** Convenience: the PK field name from the trailing order key. */
    val pkField: String get() = order.last().field
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/DiffOps.kt
Size: 3.12 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

/** Minimal change model for your outbox. */
sealed interface Diff<PK> {
    val pk: PK
    data class Added<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    data class Removed<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    /**
     * Modified: row stayed in the page but content and/or order position changed.
     * - If `from != null && from != pos`, the row moved (promotion/demotion).
     * - If `from == pos`, treat as in-place payload change.
     */
    data class Modified<PK>(override val pk: PK, val pos: Int, val from: Int? = null) : Diff<PK>
}

object DiffOps {

    /**
     * Insert a PK into a sorted Top-K page and emit diffs:
     * - ADDED(pos) when inserted.
     * - REMOVED(tailPos) if the tail was evicted.
     * - [] if nothing changed.
     */
    @JvmStatic
    fun <PK> insertAndDiff(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): List<Diff<PK>> {
        val diffs = mutableListOf<Diff<PK>>()
        val prevSize = page.size
        val res = PageOps.insertIfBetter(page, pk, k, cmp)

        if (res.inserted) {
            diffs += Diff.Added(pk, res.pos)
        }
        if (res.evicted != null) {
            // evicted was the previous tail
            val evictedPos = (prevSize - 1).coerceAtLeast(0)
            diffs += Diff.Removed(res.evicted, evictedPos)
        }
        return diffs
    }

    /**
     * Reposition a PK already in the page when its order fields changed.
     * Emits MODIFIED(newPos, from=oldPos) if moved, or MODIFIED(pos, from=pos) if payload-only change.
     * If the PK is not present, returns [] (call insertAndDiff instead).
     */
    @JvmStatic
    fun <PK> repositionAndDiff(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>,
        payloadChanged: Boolean = true
    ): List<Diff<PK>> {
        val res = PageOps.repositionIfNeeded(page, pk, cmp)
        return if (res.from >= 0) {
            val from = res.from
            val to = res.to
            // Always emit Modified if caller says payload changed, or if moved
            if (payloadChanged || res.moved) listOf(Diff.Modified(pk, to, from)) else emptyList()
        } else {
            emptyList()
        }
    }

    /**
     * Remove a PK if present and emit REMOVED(pos).
     */
    @JvmStatic
    fun <PK> removeAndDiff(
        page: MutableList<PK>,
        pk: PK
    ): List<Diff<PK>> {
        val idx = page.indexOf(pk)
        if (idx >= 0) {
            page.removeAt(idx)
            return listOf(Diff.Removed(pk, idx))
        }
        return emptyList()
    }
}
object OutboxCodec {
    data class WireDiff<PK>(val type: String, val pk: PK, val pos: Int? = null, val from: Int? = null)

    @JvmStatic
    fun <PK> toWire(diffs: List<Diff<PK>>): List<WireDiff<PK>> =
        diffs.map {
            when (it) {
                is Diff.Added   -> WireDiff("added",   it.pk, pos = it.pos)
                is Diff.Removed -> WireDiff("removed", it.pk, pos = it.pos)
                is Diff.Modified-> WireDiff("modified",it.pk, pos = it.pos, from = it.from)
            }
        }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbMappings.kt
Size: 3.00 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import SqlSchema.OrderKeySpec as FbOrderKeySpec
import SqlSchema.OrderSort as FbOrderSort
import SqlSchema.*

object FbMappings {

    @JvmStatic
    fun mapOrderSort(code: Int): Pair<Direction, Nulls> = when (code) {
        FbOrderSort.ASC_DEFAULT.toInt()      -> Direction.ASC  to Nulls.LAST
        FbOrderSort.ASC_NULLS_FIRST.toInt()  -> Direction.ASC  to Nulls.FIRST
        FbOrderSort.ASC_NULLS_LAST.toInt()   -> Direction.ASC  to Nulls.LAST
        FbOrderSort.DESC_DEFAULT.toInt()     -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_FIRST.toInt() -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_LAST.toInt()  -> Direction.DESC to Nulls.LAST
        else                                 -> Direction.DESC to Nulls.FIRST
    }

    @JvmStatic
    fun toOrderSpec(q: StreamingSqlDataFilter, pkField: String, pkDir: Direction = Direction.DESC): List<OrderKeySpec> {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val list = buildList {
            for (i in 0 until q.orderLength()) {
                val ok: FbOrderKeySpec = q.order(i)!!
                val (dir, nulls) = mapOrderSort(ok.sort().toInt())
                add(if (ok.isPk()) OrderKeySpec.pk(ok.field(), dir, nulls) else OrderKeySpec.of(ok.field(), dir, nulls))
            }
        }

        val hasPk = list.any { it.isPk }
        val res =
            if (hasPk && list.last().isPk) list
            else if (hasPk) {
                val pk = list.first { it.isPk }
                list.filterNot { it.isPk } + pk
            } else list + OrderKeySpec.pk(pkField, pkDir)

        if (debug) println("[FbMappings] orderSpec=$res")
        return res
    }

    @JvmStatic
    fun toCursorTuple(q: StreamingSqlDataFilter): Map<String, Any?>? {
        if (q.cursorLength() == 0) return null
        val m = LinkedHashMap<String, Any?>(q.cursorLength())
        for (i in 0 until q.cursorLength()) {
            val ce = q.cursor(i)!!
            m[ce.field()] = readValue(ce.valueType().toInt(), ce.value(null))
        }
        return m
    }

    @JvmStatic
    fun readValue(t: Int, table: com.google.flatbuffers.Table?): Any? = when (t) {
        FilterValue.StringValue.toInt()    -> (table as StringValue).value()
        FilterValue.NumberValue.toInt()    -> (table as NumberValue).value()
        FilterValue.Int64Value.toInt()     -> (table as Int64Value).value()
        FilterValue.BoolValue.toInt()      -> (table as BoolValue).value()
        FilterValue.NullValue.toInt()      -> null
        FilterValue.TimestampValue.toInt() -> {
            val tv = table as TimestampValue
            val base = tv.epoch()
            when (tv.unit().toInt()) {
                TimeUnit.SECONDS.toInt() -> base * 1_000_000L
                TimeUnit.MILLIS.toInt()  -> base * 1_000L
                TimeUnit.MICROS.toInt()  -> base
                TimeUnit.NANOS.toInt()   -> base / 1_000L
                else -> base
            }
        }
        else -> null
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbPredicate.kt
Size: 7.43 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FbPredicate {

    @JvmStatic
    fun evalWrapper(w: BasicSqlDataFilterWrapper?, row: Map<String, Any?>): Boolean {
        if (w == null) return true
        val isAnd = w.filterWrapperType() == SQLFilterWrapperType.and
        var acc = if (isAnd) true else false
        for (i in 0 until w.filtersLength()) {
            val t = w.filtersType(i).toInt() // ← normalize to Int
            val ok = when (t) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() ->
                    evalBasic(w.filters(null, i) as BasicSqlDataFilter, row)
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() ->
                    evalWrapper(w.filters(null, i) as BasicSqlDataFilterWrapper, row)
                else -> false
            }
            acc = if (isAnd) (acc && ok) else (acc || ok)
            if (isAnd && !acc) return false
            if (!isAnd && acc) return true
        }
        return acc
    }

    @JvmStatic
    fun evalBasic(f: BasicSqlDataFilter, row: Map<String, Any?>): Boolean {
        val field = f.fieldName()
        val got = row[field]
        val ci = f.modifier()?.caseInsensitive() == true

        return when (f.filterType().toInt()) { // ← normalize to Int
            BasicSqlDataFilterType.equals.toInt()              -> eq(got, f, ci)
            BasicSqlDataFilterType.notEquals.toInt()           -> !eq(got, f, ci)

            BasicSqlDataFilterType.lessThan.toInt()            -> cmp(got, f) { a, b -> a <  b }
            BasicSqlDataFilterType.lessThanOrEquals.toInt()    -> cmp(got, f) { a, b -> a <= b }
            BasicSqlDataFilterType.greaterThan.toInt()         -> cmp(got, f) { a, b -> a >  b }
            BasicSqlDataFilterType.greaterThanOrEquals.toInt() -> cmp(got, f) { a, b -> a >= b }

            BasicSqlDataFilterType.inList.toInt()              -> inList(got, f, ci)
            BasicSqlDataFilterType.notInList.toInt()           -> !inList(got, f, ci)

            BasicSqlDataFilterType.isNull.toInt()              -> got == null
            BasicSqlDataFilterType.isNotNull.toInt()           -> got != null

            BasicSqlDataFilterType.startsWith.toInt()          -> str(got)?.let { a -> startsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.endsWith.toInt()            -> str(got)?.let { a -> endsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.contains.toInt()            -> str(got)?.let { a -> contains(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.notContains.toInt()         -> str(got)?.let { a -> !contains(a, strVal(f), ci) } == true

            BasicSqlDataFilterType.regex.toInt()               -> regexMatch(str(got) ?: "", strVal(f), ci)
            BasicSqlDataFilterType.notRegex.toInt()            -> !regexMatch(str(got) ?: "", strVal(f), ci)

            else -> false
        }
    }

    private fun strVal(f: BasicSqlDataFilter): String =
        (f.value(StringValue()) as? StringValue)?.value() ?: ""

    private fun eq(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean = when (f.valueType().toInt()) { // ← normalize
        FilterValue.StringValue.toInt()    -> str(got)?.let { a -> eqStr(a, (f.value(StringValue()) as StringValue).value(), ci) } ?: false
        FilterValue.NumberValue.toInt()    -> num(got)?.let  { a -> a == (f.value(NumberValue()) as NumberValue).value() } ?: false
        FilterValue.Int64Value.toInt()     -> long(got)?.let { a -> a == (f.value(Int64Value()) as Int64Value).value() } ?: false
        FilterValue.BoolValue.toInt()      -> (got as? Boolean) == (f.value(BoolValue()) as BoolValue).value()
        FilterValue.TimestampValue.toInt() -> micros(got)?.let { a -> a == tvMicros(f.value(TimestampValue()) as TimestampValue) } ?: false
        FilterValue.NullValue.toInt()      -> got == null
        else -> false
    }

    private inline fun cmp(got: Any?, f: BasicSqlDataFilter, crossinline op: (Double, Double) -> Boolean): Boolean {
        val g = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> micros(got)?.toDouble()
            FilterValue.Int64Value.toInt()     -> long(got)?.toDouble()
            else                               -> num(got)
        } ?: return false

        val e = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> tvMicros(f.value(TimestampValue()) as TimestampValue).toDouble()
            FilterValue.Int64Value.toInt()     -> (f.value(Int64Value()) as Int64Value).value().toDouble()
            FilterValue.NumberValue.toInt()    -> (f.value(NumberValue()) as NumberValue).value()
            else -> return false
        }
        return op(g, e)
    }

    private fun inList(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean {
        if (got == null) return false
        return when (f.valueType().toInt()) { // ← normalize
            FilterValue.StringList.toInt() -> {
                val L = f.value(StringList()) as StringList
                val probe = str(got) ?: return false
                (0 until L.valuesLength()).any { i -> eqStr(probe, L.values(i) ?: return@any false, ci) }
            }
            FilterValue.Int64List.toInt() -> {
                val L = f.value(Int64List()) as Int64List
                val g = long(got) ?: return false
                (0 until L.valuesLength()).any { i -> g == L.values(i) }
            }
            FilterValue.Float64List.toInt() -> {
                val L = f.value(Float64List()) as Float64List
                val g = num(got) ?: return false
                (0 until L.valuesLength()).any { i -> java.lang.Double.compare(g, L.values(i)) == 0 }
            }
            FilterValue.BoolList.toInt() -> {
                val L = f.value(BoolList()) as BoolList
                val g = got as? Boolean ?: return false
                (0 until L.valuesLength()).any { i -> g == (L.values(i) == true) }
            }
            else -> false
        }
    }

    // small utils
    private fun str(v: Any?): String? = when (v) { null -> null; is CharSequence -> v.toString(); else -> v.toString() }
    private fun eqStr(a: String, b: String, ci: Boolean) = if (ci) a.equals(b, true) else a == b
    private fun startsWith(a: String, b: String, ci: Boolean) = if (ci) a.lowercase().startsWith(b.lowercase()) else a.startsWith(b)
    private fun endsWith(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().endsWith(b.lowercase()) else a.endsWith(b)
    private fun contains(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().contains(b.lowercase()) else a.contains(b)
    private fun regexMatch(text: String, pattern: String, ci: Boolean) =
        if (ci) Regex(pattern, setOf(RegexOption.IGNORE_CASE)).containsMatchIn(text) else Regex(pattern).containsMatchIn(text)

    private fun num(v: Any?): Double? = when (v) { is Number -> v.toDouble(); is String -> v.toDoubleOrNull(); else -> null }
    private fun long(v: Any?): Long?  = when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }
    private fun micros(v: Any?): Long?= when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }

    private fun tvMicros(tv: TimestampValue): Long = when (tv.unit().toInt()) {
        TimeUnit.SECONDS.toInt() -> tv.epoch() * 1_000_000L
        TimeUnit.MILLIS.toInt()  -> tv.epoch() * 1_000L
        TimeUnit.MICROS.toInt()  -> tv.epoch()
        TimeUnit.NANOS.toInt()   -> tv.epoch() / 1_000L
        else -> tv.epoch()
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbQueryAdapter.kt
Size: 213 B
Code:
// Evaluator/FbQueryAdapter.kt
package org.cladbe.cdc.Evaluator

/** Adapts a FlatBuffer StreamingSqlDataFilter into a runtime DerivedSpec. */
interface FbQueryAdapter {
    fun parse(fb: ByteArray): DerivedSpec
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbQueryAdapterImpl.kt
Size: 1.04 KB
Code:
// Evaluator/FbQueryAdapterImpl.kt
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import java.nio.ByteBuffer

class FbQueryAdapterImpl(
    private val tableName: String,
    private val pkField: String,
    private val pkDir: Direction = Direction.DESC
) : FbQueryAdapter {

    override fun parse(fb: ByteArray): DerivedSpec {
        val q = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(ByteBuffer.wrap(fb))
        val k = q.limit().toInt()
        val order = FbMappings.toOrderSpec(q, pkField, pkDir)
        val where: (Map<String, Any?>) -> Boolean = { row -> FbPredicate.evalWrapper(q.wrapper(), row) }

        // TODO: if you add a real extractor, plug it here.
        // For now, a safe fallback is to scan leaf filters and collect field names.
        val filterFields = FilterFieldExtractor.fromWrapper(q.wrapper())

        return DerivedSpec(
            table = tableName,
            k = k,
            order = order,
            whereMatches = where,
            filterFields = filterFields
        )
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FilterFieldExtractor.kt
Size: 950 B
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FilterFieldExtractor {
    @JvmStatic
    fun fromWrapper(w: BasicSqlDataFilterWrapper?): Set<String> {
        if (w == null) return emptySet()
        val out = LinkedHashSet<String>()
        walk(w, out)
        return out
    }

    private fun walk(w: BasicSqlDataFilterWrapper, out: MutableSet<String>) {
        for (i in 0 until w.filtersLength()) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() -> {
                    val f = w.filters(BasicSqlDataFilter(), i) as BasicSqlDataFilter
                    out += f.fieldName()
                }
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val child = w.filters(BasicSqlDataFilterWrapper(), i) as BasicSqlDataFilterWrapper
                    walk(child, out)
                }
            }
        }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/OrderSpec.kt
Size: 1.18 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.*

enum class TimestampUnit { SECONDS, MILLIS, MICROS, NANOS }
enum class Direction { ASC, DESC }
enum class Nulls { FIRST, LAST }

/**
 * One ORDER BY item.
 * - `isPk` must be true for exactly one trailing key (the primary-key tiebreaker).
 * - `nulls` is explicit (no null); default follows SQL: ASC→LAST, DESC→FIRST.
 */
data class OrderKeySpec(
    val field: String,
    val dir: Direction,
    val nulls: Nulls = nullsDefaultFor(dir),
    val isPk: Boolean = false
) {
    companion object {
        @JvmStatic fun of(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, false)

        @JvmStatic fun pk(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, true)

        @JvmStatic fun nullsDefaultFor(d: Direction): Nulls =
            if (d == Direction.ASC) Nulls.LAST else Nulls.FIRST
    }
}

/** Row field accessor (SAM for Java interop). */
fun interface RowAccessor<R> { fun get(row: R, field: String): Any? }

/** Lookup a row by PK (SAM for Java interop). */
fun interface PkLookup<PK, R> { fun get(pk: PK): R? }
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/PageOps.kt
Size: 3.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

object PageOps {

    data class InsertResult<PK>(
        val inserted: Boolean,
        val pos: Int,            // where we inserted; -1 if not inserted
        val evicted: PK? = null  // tail PK evicted when window was full
    )

    data class RepositionResult<PK>(
        val moved: Boolean,
        val from: Int,
        val to: Int
    )

    /**
     * Lower-bound binary search: first index where [key] could be inserted
     * without violating sorted order (stable insert).
     */
    @JvmStatic
    fun <T> insertionIndex(list: List<T>, key: T, cmp: Comparator<T>): Int {
        var lo = 0
        var hi = list.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val c = cmp.compare(list[mid], key)
            if (c < 0) lo = mid + 1 else hi = mid
        }
        return lo
    }

    /**
     * Insert a PK into a sorted Top-K page if it belongs.
     * - If page.size < K → insert at correct position.
     * - If full and new PK is “better” than current tail → evict tail, insert.
     * - Else → ignore.
     */
    @JvmStatic
    fun <PK> insertIfBetter(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): InsertResult<PK> {
        // If already present, do nothing (call repositionIfNeeded after you’ve merged rowCache)
        val existing = page.indexOf(pk)
        if (existing >= 0) return InsertResult(inserted = false, pos = existing, evicted = null)

        if (page.size < k) {
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = null)
        }

        // Page full: compare against tail
        val tail = page.last()
        if (cmp.compare(pk, tail) < 0) {
            // New item outranks the tail → evict tail and insert
            page.removeAt(page.size - 1)
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = tail)
        }
        return InsertResult(inserted = false, pos = -1, evicted = null)
    }

    /**
     * If a PK is already in the page and its order fields changed,
     * remove it and reinsert at its correct position.
     */
    @JvmStatic
    fun <PK> repositionIfNeeded(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>
    ): RepositionResult<PK> {
        val from = page.indexOf(pk)
        if (from < 0) return RepositionResult(moved = false, from = -1, to = -1)

        // Remove first so the insertion index is computed against the remaining items
        page.removeAt(from)
        val to = insertionIndex(page, pk, cmp)
        page.add(to, pk)
        return RepositionResult(moved = (to != from), from = from, to = to)
    }

    /** Remove PK if present. Returns the removed index or -1. */
    @JvmStatic
    fun <PK> removeIfPresent(page: MutableList<PK>, pk: PK): Int {
        val idx = page.indexOf(pk)
        if (idx >= 0) page.removeAt(idx)
        return idx
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/QueryEngine.kt
Size: 7.91 KB
Code:
package org.cladbe.cdc.engine

import kotlin.math.max
import org.cladbe.cdc.Evaluator.DerivedSpec
import org.cladbe.cdc.Evaluator.Direction
import org.cladbe.cdc.Evaluator.Nulls
import org.cladbe.cdc.Evaluator.OutboxCodec

data class UpsertResult<PK>(
    val changes: List<OutboxCodec.WireDiff<PK>>,
    val needsBackfill: Boolean,
    val probeTail: Boolean
)

class QueryEngine<PK>(
    private val spec: DerivedSpec,
    private val pkField: String,
    private val getPk: (Map<String, Any?>) -> PK,
    private val rowProvider: (PK) -> Map<String, Any?>? = { null }
) {
    private val debug = System.getenv("CDC_DEBUG") == "1"
    val page: MutableList<PK> = mutableListOf()
    fun pageCapacity(): Int = spec.k

    fun upsert(row: Map<String, Any?>): UpsertResult<PK> {
        val pk = getPk(row)
        val prev = rowProvider(pk)
        val inPageBefore = page.contains(pk)

        val matchesAfter = spec.whereMatches(row)
        val changed = diffFields(prev, row)

        val orderFields = spec.order.map { it.field }.toSet()
        val orderChanged = changed.any { it in orderFields }
        val filterChanged = changed.any { it in spec.filterFields }

        if (debug) println("[QueryEngine] upsert pk=$pk inPage=$inPageBefore matches=$matchesAfter changed=$changed orderChanged=$orderChanged filterChanged=$filterChanged pageSize=${page.size}")

        if (!matchesAfter) {
            if (!inPageBefore) return UpsertResult(emptyList(), false, false)
            val idx = page.indexOf(pk)
            if (idx >= 0) {
                page.removeAt(idx)
                if (debug) println("[QueryEngine] remove pk=$pk at pos=$idx")
                return UpsertResult(
                    changes = listOf(OutboxCodec.WireDiff("removed", pk, pos = idx)),
                    needsBackfill = true,
                    probeTail = false
                )
            }
            return UpsertResult(emptyList(), false, false)
        }

        if (!inPageBefore) {
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                if (debug) println("[QueryEngine] add pk=$pk pos=$pos (page had room)")
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            } else {
                val tailPk = page.last()
                val tailRow = rowProvider(tailPk) ?: return UpsertResult(emptyList(), false, false)
                if (compareRows(row, tailRow) < 0) {
                    val tailIdx = page.size - 1
                    val evicted = page.removeAt(tailIdx)
                    val pos = findInsertPosition(row)
                    page.add(pos, pk)
                    if (debug) println("[QueryEngine] evict tail pk=$evicted at pos=$tailIdx; add pk=$pk pos=$pos")
                    return UpsertResult(
                        changes = listOf(
                            OutboxCodec.WireDiff("removed", evicted, pos = tailIdx),
                            OutboxCodec.WireDiff("added", pk, pos = pos)
                        ),
                        needsBackfill = false,
                        probeTail = false
                    )
                }
                return UpsertResult(emptyList(), false, false)
            }
        }

        val oldIdx = page.indexOf(pk)
        if (oldIdx < 0) {
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                if (debug) println("[QueryEngine] add pk=$pk pos=$pos (reappeared; page had room)")
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            } else {
                val tailPk = page.last()
                val tailRow = rowProvider(tailPk)
                return if (tailRow != null && compareRows(row, tailRow) < 0) {
                    val tailIdx = page.size - 1
                    val evicted = page.removeAt(tailIdx)
                    val pos = findInsertPosition(row)
                    page.add(pos, pk)
                    if (debug) println("[QueryEngine] evict tail pk=$evicted at pos=$tailIdx; add pk=$pk pos=$pos (reappeared)")
                    UpsertResult(
                        listOf(
                            OutboxCodec.WireDiff("removed", evicted, pos = tailIdx),
                            OutboxCodec.WireDiff("added", pk, pos = pos)
                        ),
                        false, false
                    )
                } else UpsertResult(emptyList(), false, false)
            }
        }

        if (orderChanged) {
            page.removeAt(oldIdx)
            val newPos = findInsertPosition(row)
            page.add(newPos, pk)
            if (debug) println("[QueryEngine] move pk=$pk from=$oldIdx to=$newPos (order change)")
            val diff = OutboxCodec.WireDiff("modified", pk, pos = newPos, from = oldIdx)
            val probe = (filterChanged || orderChanged) && page.size >= spec.k
            return UpsertResult(listOf(diff), needsBackfill = false, probeTail = probe)
        }

        val diffs =
            if (changed.isNotEmpty()) listOf(OutboxCodec.WireDiff("modified", pk, pos = oldIdx, from = oldIdx))
            else emptyList()

        val probe = (filterChanged || orderChanged) && page.size >= spec.k
        if (debug && diffs.isNotEmpty()) println("[QueryEngine] in-place modify pk=$pk at pos=$oldIdx")
        return UpsertResult(diffs, needsBackfill = false, probeTail = probe)
    }

    fun currentCursor(): Map<String, Any?>? {
        if (page.isEmpty()) return null
        val tailPk = page.last()
        val tail = rowProvider(tailPk) ?: return null
        val m = LinkedHashMap<String, Any?>(spec.order.size)
        for (ok in spec.order) m[ok.field] = tail[ok.field]
        if (debug) println("[QueryEngine] currentCursor tailPk=$tailPk cursor=$m")
        return m
    }

    // ---- helpers ----
    private fun findInsertPosition(row: Map<String, Any?>): Int {
        var lo = 0
        var hi = page.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val midRow = rowProvider(page[mid])
            val cmp = if (midRow == null) 1 else compareRows(row, midRow)
            if (cmp < 0) hi = mid else lo = mid + 1
        }
        return lo
    }

    private fun compareRows(a: Map<String, Any?>, b: Map<String, Any?>): Int {
        for ((i, ok) in spec.order.withIndex()) {
            val av = a[ok.field]
            val bv = b[ok.field]
            val asc = ok.dir == Direction.ASC
            val nullsFirst = ok.nulls == Nulls.FIRST
            val c = compareField(av, bv, asc, nullsFirst)
            if (c != 0) return c
            if (i == spec.order.lastIndex) return 0
        }
        return 0
    }

    private fun compareField(a: Any?, b: Any?, asc: Boolean, nullsFirst: Boolean): Int {
        if (a == null && b == null) return 0
        if (a == null) return if (nullsFirst) -1 else 1
        if (b == null) return if (nullsFirst)  1 else -1
        val base = when {
            a is Number && b is Number -> a.toDouble().compareTo(b.toDouble())
            a is String && b is String -> a.compareTo(b)
            a is Boolean && b is Boolean -> java.lang.Boolean.compare(a, b)
            a::class == b::class && a is Comparable<*> -> (a as Comparable<Any>).compareTo(b)
            else -> a.toString().compareTo(b.toString())
        }
        return if (asc) base else -base
    }

    private fun diffFields(prev: Map<String, Any?>?, next: Map<String, Any?>): Set<String> {
        if (prev == null) return next.keys
        val keys = HashSet<String>(next.keys).apply { addAll(prev.keys) }
        val changed = mutableSetOf<String>()
        for (k in keys) if (!eq(prev[k], next[k])) changed += k
        return changed
    }
    private fun eq(a: Any?, b: Any?): Boolean = when {
        a === b -> true
        a == null || b == null -> false
        a is Number && b is Number -> a.toDouble() == b.toDouble()
        else -> a == b
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/SqlComparators.kt
Size: 6.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.math.BigDecimal
import java.time.*
import java.util.Comparator

object SqlComparators {

    /** Comparator for rows, SQL-like semantics with NULLS rule and PK tiebreaker. */
    @JvmStatic
    fun <R> rowComparator(
        spec: List<OrderKeySpec>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<R> {
        validateSpec(spec)
        val s = applyNullDefaults(spec)
        val hints = tsHints

        return Comparator { a, b ->
            for (k in s) {
                var va = acc.get(a, k.field)
                var vb = acc.get(b, k.field)

                // normalize temporals to epoch-nanos (Long)
                val hint = hints[k.field]
                va = normalizeTemporal(va, hint)
                vb = normalizeTemporal(vb, hint)

                val c = compareField(va, vb, k)
                if (c != 0) return@Comparator c
            }
            0 // equal across all fields (including PK)
        }
    }

    /** Comparator for PKs that fetches rows from a cache on demand. */
    @JvmStatic
    fun <PK, R> pkComparator(
        spec: List<OrderKeySpec>,
        lookup: PkLookup<PK, R>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<PK> {
        val rowCmp = rowComparator(spec, acc, tsHints)
        return Comparator { pk1, pk2 ->
            if (pk1 == pk2) return@Comparator 0
            val r1 = lookup.get(pk1)
            val r2 = lookup.get(pk2)
            if (r1 == null && r2 == null) return@Comparator 0
            if (r1 == null) return@Comparator 1
            if (r2 == null) return@Comparator -1
            val c = rowCmp.compare(r1, r2)
            if (c != 0) c else safeCompare(pk1, pk2)
        }
    }

    // ---------- internals ----------

    private fun validateSpec(spec: List<OrderKeySpec>) {
        require(spec.isNotEmpty()) { "orderSpec must contain at least one field (include PK as last)" }
        require(spec.last().isPk) { "orderSpec must end with the PK field as final tiebreaker" }
    }

    private fun applyNullDefaults(inSpec: List<OrderKeySpec>): List<OrderKeySpec> =
        inSpec.map { k ->
            val n = k.nulls ?: OrderKeySpec.nullsDefaultFor(k.dir)
            k.copy(nulls = n)
        }

    private fun compareField(a: Any?, b: Any?, k: OrderKeySpec): Int {
        // 1) NULL rules
        if (a == null || b == null) {
            if (a == null && b == null) return 0
            val firstIsNull = a == null
            return if (k.nulls == Nulls.FIRST)
                if (firstIsNull) -1 else 1
            else
                if (firstIsNull) 1 else -1
        }

        // 2) Non-null comparison
        val base = compareNonNull(a, b)
        if (base == 0) return 0

        // 3) Direction
        return if (k.dir == Direction.ASC) base else -base
    }

    @Suppress("UNCHECKED_CAST")
    private fun compareNonNull(a: Any, b: Any): Int {
        // Numbers → BigDecimal
        if (a is Number && b is Number) {
            val da = toBigDecimal(a)
            val db = toBigDecimal(b)
            return da.compareTo(db)
        }

        // CharSequence → String (safe fallback for mixed types later)
        if (a is CharSequence && b is CharSequence) {
            return a.toString().compareTo(b.toString())
        }

        // Boolean: false < true
        if (a is Boolean && b is Boolean) {
            return java.lang.Boolean.compare(a, b)
        }

        // Same class & Comparable
        if (a::class.java == b::class.java && a is Comparable<*>) {
            return (a as Comparable<Any>).compareTo(b)
        }

        // Fallback: String compare to guarantee total order
        return a.toString().compareTo(b.toString())
    }

    private fun toBigDecimal(n: Number): BigDecimal =
        when (n) {
            is Long, is Int, is Short, is Byte -> BigDecimal.valueOf(n.toLong())
            is BigDecimal -> n
            is Float, is Double -> BigDecimal.valueOf(n.toDouble())
            else -> BigDecimal(n.toString())
        }

    private fun normalizeTemporal(v: Any?, hint: TimestampUnit?): Any? {
        if (v == null) return null

        // java.time first
        when (v) {
            is Instant -> return toEpochNanos(v)
            is OffsetDateTime -> return toEpochNanos(v.toInstant())
            is ZonedDateTime -> return toEpochNanos(v.toInstant())
            is LocalDateTime -> return toEpochNanos(v.toInstant(ZoneOffset.UTC))      // assume UTC
            is LocalDate -> return toEpochNanos(v.atStartOfDay(ZoneOffset.UTC).toInstant())
        }

        // legacy dates
        if (v is java.util.Date) return toEpochNanos(v.toInstant())
        if (v is java.sql.Timestamp) return toEpochNanos(v.toInstant())

        // numeric epochs
        if (v is Number) {
            val raw = v.toLong()
            val unit = hint ?: guessUnit(raw)
            return when (unit) {
                TimestampUnit.SECONDS -> raw * 1_000_000_000L
                TimestampUnit.MILLIS  -> raw * 1_000_000L
                TimestampUnit.MICROS  -> raw * 1_000L
                TimestampUnit.NANOS   -> raw
            }
        }

        // Not temporal → return as-is
        return v
    }

    private fun toEpochNanos(i: Instant): Long =
        i.epochSecond * 1_000_000_000L + i.nano

    private fun guessUnit(v: Long): TimestampUnit {
        val av = kotlin.math.abs(v)
        return when {
            av < 100_000_000_000L -> TimestampUnit.SECONDS      // < 1e11
            av < 100_000_000_000_000L -> TimestampUnit.MILLIS   // < 1e14
            av < 100_000_000_000_000_000L -> TimestampUnit.MICROS // < 1e17
            else -> TimestampUnit.NANOS
        }
    }

    private fun <T> safeCompare(a: T?, b: T?): Int {
        if (a == null && b == null) return 0
        if (a == null) return 1
        if (b == null) return -1
        return if (a::class.java == b::class.java && a is Comparable<*>) {
            @Suppress("UNCHECKED_CAST")
            (a as Comparable<T>).compareTo(b)
        } else {
            a.toString().compareTo(b.toString())
        }
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/App.kt
Size: 1.66 KB
Code:
// src/main/java/org/cladbe/cdc/engine/App.kt
package org.cladbe.cdc.engine

import org.apache.kafka.streams.KafkaStreams
import org.apache.kafka.streams.StreamsConfig
import java.util.Properties
import java.util.regex.Pattern

fun main() {
    val brokers  = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"
    val appId    = System.getenv("STREAMS_APP_ID") ?: "cladbe-kstreams-shared"

    val topics = StreamsTopology.Topics(
        cdcPattern        = Pattern.compile(System.getenv("CDC_TOPIC_PATTERN") ?: "^server\\.cdc\\..*$"),
        queryControlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control",
        seedTopic         = System.getenv("PAGE_SEED_TOPIC") ?: "server.page.seed",
        rpcReplyTopic     = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
        outboxTopic       = System.getenv("WS_OUTBOX_TOPIC") ?: "server.page.diffs",
        backfillTopic     = System.getenv("BACKFILL_ROWS_TOPIC") ?: "server.backfill.rows"
    )

    val topology = StreamsTopology.build(topics)

    val props = Properties().apply {
        put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
        put(StreamsConfig.APPLICATION_ID_CONFIG, appId)
        put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.AT_LEAST_ONCE)
        put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
        put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
    }

    val streams = KafkaStreams(topology, props)
    Runtime.getRuntime().addShutdownHook(Thread { streams.close() })
    streams.start()
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/BackfillClient.kt
Size: 4.34 KB
Code:
// src/main/java/org/cladbe/cdc/engine/BackfillClient.kt
package org.cladbe.cdc.engine

import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.ByteArrayDeserializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.kafka.common.serialization.StringSerializer
import java.time.Duration
import java.util.*
import java.util.concurrent.ThreadLocalRandom

/**
 * Minimal Kafka RPC client for the postgres_rpc worker.
 *
 * NOTE:
 * - We send a RequestEnvelope(GetData) that the TS worker understands.
 * - The worker now supports keyset: orderKeys + cursor + strictAfter on its side.
 * - Response payload "RowsJson.rows[]" are JSON strings (one per row).
 */
class BackfillClient(
    private val requestTopic: String = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests",
    private val responseTopic: String = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
    bootstrapServers: String = System.getenv("KAFKA_BROKERS") ?: "localhost:9092",
    private val groupId: String = System.getenv("SQL_RPC_GROUP_ID") ?: "cladbe-kstreams-backfill"
) : AutoCloseable {

    private val producer = KafkaProducer<String, ByteArray>(
        Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
    )

    private val consumer = KafkaConsumer<String, ByteArray>(
        Properties().apply {
            put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ConsumerConfig.GROUP_ID_CONFIG, "$groupId-${ThreadLocalRandom.current().nextInt(1_000_000)}")
            put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
            put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer::class.java)
            put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest")
            put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true")
        }
    ).also { it.subscribe(listOf(responseTopic)) }

    override fun close() {
        try { consumer.close() } catch (_: Exception) {}
        try { producer.close() } catch (_: Exception) {}
    }

    // ---------- FB builders (RequestEnvelope<GetData>) ----------
    // We reuse your FlatBuffers generated Java from sql_rpc.fbs & sql_schema.fbs
    // Package name below assumes flatc generated into SqlRpc / SqlSchema namespaces.

    fun requestOne(
        companyId: String,
        table: String,
        order: List<OrderKey>,               // simple POJO below
        filtersFbBytes: ByteArray?,          // optional FB BasicSqlDataFilterWrapper (or null)
        cursorTuple: Map<String, Any?>?,     // last-tuple of order keys
        strictAfter: Boolean = true,
        timeoutMs: Long = 1_000
    ): String? /* row JSON */ {
        val corr = UUID.randomUUID().toString()

        val reqBytes = FbRequests.buildGetDataEnvelope(
            correlationId = corr,
            replyTopic = responseTopic,
            companyId = companyId,
            tableName = table,
            order = order,
            wrapperFbBytes = filtersFbBytes,
            limit = 1,
            offset = null,
            cursorTuple = cursorTuple,
            strictAfter = strictAfter
        )

        // send
        producer.send(ProducerRecord(requestTopic, corr, reqBytes))

        // wait single reply for corr id
        val deadline = System.currentTimeMillis() + timeoutMs
        while (System.currentTimeMillis() < deadline) {
            val polled = consumer.poll(Duration.ofMillis(50))
            for (rec in polled) {
                if (rec.key() != corr) continue
                val rowJson = FbResponses.parseRowsJsonFirst(rec.value()) // null if none
                return rowJson
            }
        }
        return null
    }
}

/** Minimal order POJO for the FB builder */
data class OrderKey(val field: String, val sortOrdinal: Int)

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/BackfillReplyProcessor.kt
Size: 2.28 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore
import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper

class BackfillReplyProcessor(
    private val rowCacheName: String,
    private val pendingName: String,
    /** Name of the child sink node wired to the internal backfill topic. */
    private val backfillTopicName: String
) : Processor<String, ByteArray, ByteArray, ByteArray> {

    private lateinit var ctx: ProcessorContext<ByteArray, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var pending : KeyValueStore<String, String>
    private val mapper = ObjectMapper()
    private val debug = System.getenv("CDC_DEBUG") == "1"

    override fun init(context: ProcessorContext<ByteArray, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        pending  = context.getStateStore(pendingName)
    }

    override fun process(record: Record<String, ByteArray>) {
        val corr = record.key() ?: return
        val fb = record.value() ?: return

        val hashKey = pending.get(corr) ?: return // "$table|$hashId"
        val table = hashKey.substringBefore('|')
        val hashId = hashKey.substringAfter('|')

        val rowJson: String = FbResponses.parseRowsJsonFirst(fb) ?: return

        val rowMap: Map<String, Any?> = try {
            mapper.readValue(rowJson, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(rowMap, pkCol) ?: return
        val rowBytes = rowJson.toByteArray()

        if (debug) println("[BackfillReply] corr=$corr table=$table hashId=$hashId pkCol=$pkCol pk=$pk forwarding to backfill topic")

        // Warm row cache
        rowCache.put(pk, rowBytes)

        // Forward to internal backfill topic
        ctx.forward(
            Record(pk.toByteArray(), rowBytes, record.timestamp()),
            backfillTopicName
        )

        pending.delete(corr)
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/CdcProcessor.kt
Size: 14.39 KB
Code:
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.header.internals.RecordHeaders
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.streams.processor.Cancellable
import org.apache.kafka.streams.processor.PunctuationType
import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore
import org.cladbe.cdc.Evaluator.FbQueryAdapterImpl
import org.cladbe.cdc.Evaluator.OutboxCodec
import java.time.Duration
import java.util.*

class CdcProcessor(
    private val rowCacheName: String,
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String,
    private val pendingName: String? = null
) : Processor<ByteArray, ByteArray, String, ByteArray> {

    private lateinit var ctx: ProcessorContext<String, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var qmeta   : KeyValueStore<String, ByteArray>
    private lateinit var page    : KeyValueStore<String, ByteArray>
    private lateinit var outbox  : KeyValueStore<String, ByteArray>
    private var pending          : KeyValueStore<String, String>? = null

    private val engines = mutableMapOf<String, QueryEngine<String>>() // key: "$table|$hashId"
    private val mapper = ObjectMapper()
    private val debug = System.getenv("CDC_DEBUG") == "1"

    // adaptive flush
    private var flushTask: Cancellable? = null
    private var flushIntervalMs = 200L
    private val minFlushMs = 50L
    private val maxFlushMs = 5_000L
    private val batchChangeThreshold = 64
    private val hardMaxLatencyMs = 750L

    private var pendingChangeCount = 0
    private var lastFlushMs = 0L
    private var lastActivityMs = 0L
    private var lastSeenLsn: Long = 0L
    private val dirtyHashes = mutableSetOf<String>()
    private var keydb: KeyDbWriter? = null

    // backfill
    private lateinit var rpcProducer: KafkaProducer<ByteArray, ByteArray>
    private val requestTopic = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests"
    private val replyTopic   = System.getenv("SQL_RPC_REPLY_TOPIC")
        ?: System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses"
    private val rpcBrokers   = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"

    override fun init(context: ProcessorContext<String, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        qmeta    = context.getStateStore(qmetaName)
        page     = context.getStateStore(pageName)
        outbox   = context.getStateStore(outboxName)
        if (pendingName != null) pending = context.getStateStore(pendingName)

        val now = System.currentTimeMillis()
        lastFlushMs = now
        lastActivityMs = now

        scheduleFlush(flushIntervalMs)
        rpcProducer = newProducer()
        runCatching { keydb = KeyDbWriter() }.onFailure { e ->
            println("[streams] KeyDB disabled: ${e.message}")
        }
        if (debug) {
            println("[CdcProcessor] init: requestTopic=$requestTopic replyTopic=$replyTopic brokers=$rpcBrokers")
            println("[CdcProcessor] init: flushIntervalMs=$flushIntervalMs min=$minFlushMs max=$maxFlushMs")
        }
    }

    override fun process(record: Record<ByteArray, ByteArray>) {
        // --- derive table from topic name ---
        val topic = ctx.recordMetadata().orElse(null)?.topic() ?: return
        // expect topics like "server.cdc.tenant_table"
        val table = topic.substringAfter("server.cdc.", topic.substringAfterLast('.'))

        val row: Map<String, Any?> = decodeRow(record.value()) ?: return
        extractLsn(record.value())?.let { lsn -> if (lsn > lastSeenLsn) lastSeenLsn = lsn }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(row, pkCol) ?: return

        rowCache.put(pk, record.value())
        keydb?.writeHotRow(table = table, pk = pk, row = row, lsn = lastSeenLsn)

        if (debug) {
            println("[CdcProcessor] process: topic=$topic table=$table pkCol=$pkCol pk=$pk lsn=$lastSeenLsn")
        }

        var producedChanges = 0

        // Iterate only queries for this table: keys starting with "$table|"
        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val entry = iter.next()
                val key = entry.key
                if (!key.startsWith("$table|")) continue
                val hashKey = key // "$table|$hashId"
                val hashId = hashKey.substringAfter('|')

                val engine = engines.getOrPut(hashKey) {
                    val spec = FbQueryAdapterImpl(tableName = table, pkField = pkCol).parse(entry.value)
                    if (debug) println("[CdcProcessor] engine:create hashId=$hashId specK=${spec.k} order=${spec.order} filterFields=${spec.filterFields}")
                    QueryEngine(
                        spec = spec,
                        pkField = pkCol,
                        getPk = { r -> extractPkOrNull(r, pkCol)!! },
                        rowProvider = { p -> rowFromCache(p) }
                    )
                }

                val result = engine.upsert(row)

                if (result.changes.isNotEmpty()) {
                    producedChanges += result.changes.size
                    page.put(hashKey, encodePkList(engine.page))
                    val prev = outbox.get(hashKey)
                    val appended = appendOutbox(prev, result.changes)
                    outbox.put(hashKey, appended)

                    if (debug) {
                        val added = result.changes.count { it.type == "added" }
                        val removed = result.changes.count { it.type == "removed" }
                        val modified = result.changes.count { it.type == "modified" }
                        println("[CdcProcessor] outbox:+ hashId=$hashId Δ=${result.changes.size} (add=$added rm=$removed mod=$modified) pageSize=${engine.page.size} bytes=${appended.size}")
                    }

                    dirtyHashes += hashKey
                }

                when {
                    result.needsBackfill -> {
                        if (debug) println("[CdcProcessor] needsBackfill: hashId=$hashId")
                        triggerBackfill(hashKey, engine, table)
                    }
                    result.probeTail     -> {
                        if (debug) println("[CdcProcessor] probeTail: hashId=$hashId")
                        triggerBackfill(hashKey, engine, table)
                    }
                }
            }
        }

        if (producedChanges > 0) {
            lastActivityMs = System.currentTimeMillis()
            pendingChangeCount += producedChanges
            val age = lastActivityMs - lastFlushMs
            val hitBatch = pendingChangeCount >= batchChangeThreshold
            val hitAge   = age >= hardMaxLatencyMs
            if (debug) {
                println("[CdcProcessor] batch: produced=$producedChanges pending=$pendingChangeCount ageMs=$age hitBatch=$hitBatch hitAge=$hitAge flushIntervalMs=$flushIntervalMs")
            }
            if (hitBatch || hitAge) {
                flushAll()
                if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            } else if (flushIntervalMs > 2 * minFlushMs) {
                rescheduleFlush(flushIntervalMs / 2)
            }
        }
    }

    private fun onFlushTick() {
        if (pendingChangeCount > 0) {
            if (debug) println("[CdcProcessor] flushTick: pending=$pendingChangeCount → flushAll() now")
            flushAll()
            if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            return
        }
        val newInterval = (flushIntervalMs * 2).coerceAtMost(maxFlushMs)
        if (newInterval != flushIntervalMs) rescheduleFlush(newInterval)
        if (debug) println("[CdcProcessor] flushTick: idle; nextInMs=$flushIntervalMs")
    }

    private fun flushAll() {
        var flushedAny = false
        var batches = 0
        var totalBytes = 0

        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val e = iter.next()
                val hashKey = e.key              // "$table|$hashId"
                val hashId  = hashKey.substringAfter('|')
                val ob = outbox.get(hashKey) ?: continue
                if (ob.isEmpty()) continue

                val headers = RecordHeaders().apply {
                    if (lastSeenLsn > 0L) add("lsn", longToBe(lastSeenLsn))
                }
                ctx.forward(Record(hashId, ob, System.currentTimeMillis(), headers))
                if (debug) println("[CdcProcessor] flush: hashId=$hashId bytes=${ob.size} lsn=$lastSeenLsn")

                runCatching {
                    val b64 = java.util.Base64.getEncoder().encodeToString(ob)
                    keydb?.appendDiff(hashId, lastSeenLsn, b64)
                }

                outbox.put(hashKey, ByteArray(0))
                flushedAny = true
                batches += 1
                totalBytes += ob.size

                if (hashKey in dirtyHashes) {
                    val engine = engines[hashKey]
                    if (engine != null) {
                        runCatching {
                            keydb?.writeRangeIndex(hashId, engine.page.toList())
                            keydb?.writeReverseIndex(hashId, engine.page.toList())
                        }
                        val rows = buildSnapshotRows(hashKey)
                        runCatching { keydb?.writeSnapshot(hashId, rows, lastSeenLsn) }
                        if (debug) println("[CdcProcessor] snapshot: hashId=$hashId rows=${engine.page.size}")
                    }
                    dirtyHashes.remove(hashKey)
                }
            }
        }
        if (flushedAny) {
            pendingChangeCount = 0
            lastFlushMs = System.currentTimeMillis()
            if (debug) println("[CdcProcessor] flush: complete batches=$batches bytes=$totalBytes nextFlushMs=$flushIntervalMs")
        } else if (debug) {
            println("[CdcProcessor] flush: nothing-to-do")
        }
    }

    override fun close() {
        flushTask?.cancel()
        runCatching { rpcProducer.close() }
        if (debug) println("[CdcProcessor] close()")
    }

    private fun triggerBackfill(hashKey: String, engine: QueryEngine<String>, table: String) {
        val tailCursor = engine.currentCursor() ?: run {
            if (debug) println("[CdcProcessor] backfill: skip (no cursor) hashKey=$hashKey")
            return
        }
        val qfb = qmeta.get(hashKey) ?: run {
            if (debug) println("[CdcProcessor] backfill: skip (no qmeta) hashKey=$hashKey")
            return
        }

        val corr = UUID.randomUUID().toString()
        pending?.put(corr, hashKey)

        val reqBytes = FbRequests.buildGetDataFromQueryFb(
            correlationId = corr,
            replyTopic = replyTopic,
            queryFb = qfb,
            cursorTuple = tailCursor,
            limit = 1,
            strictlyAfter = true,
            fullTableName = table
        )
        rpcProducer.send(ProducerRecord(requestTopic, corr.toByteArray(), reqBytes))
        if (debug) println("[CdcProcessor] backfill: sent corr=$corr hashKey=$hashKey cursor=$tailCursor -> requestTopic=$requestTopic")
    }

    // ---- timers & helpers ----
    private fun scheduleFlush(intervalMs: Long) {
        flushTask?.cancel()
        flushIntervalMs = intervalMs
        flushTask = ctx.schedule(Duration.ofMillis(intervalMs), PunctuationType.WALL_CLOCK_TIME) { onFlushTick() }
        if (debug) println("[CdcProcessor] scheduleFlush: every ${intervalMs}ms")
    }
    private fun rescheduleFlush(newIntervalMs: Long) {
        if (newIntervalMs == flushIntervalMs) return
        scheduleFlush(newIntervalMs)
        if (debug) println("[CdcProcessor] rescheduleFlush: → ${newIntervalMs}ms")
    }

    private fun rowFromCache(pk: String): Map<String, Any?>? {
        val bytes = rowCache.get(pk) ?: return null
        return decodeRow(bytes)
    }
    @Suppress("UNCHECKED_CAST")
    private fun decodeRow(bytes: ByteArray): Map<String, Any?>? =
        try { mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {}) } catch (_: Exception) { null }

    private fun encodePkList(list: List<String>): ByteArray = list.joinToString(",").toByteArray()
    private fun appendOutbox(prev: ByteArray?, changes: List<OutboxCodec.WireDiff<String>>): ByteArray {
        val add = changes.joinToString("|") { "${it.type}:${it.pk}@${it.pos ?: -1},${it.from ?: -1}" }
        val s = (prev?.toString(Charsets.UTF_8)?.takeIf { it.isNotEmpty() }?.plus("||") ?: "") + add
        return s.toByteArray()
    }
    private fun buildSnapshotRows(hashKey: String): List<Map<String, Any?>> {
        val engine = engines[hashKey] ?: return emptyList()
        val rows = ArrayList<Map<String, Any?>>(engine.page.size)
        for (pk in engine.page) rowFromCache(pk)?.let { rows += it }
        return rows
    }
    private fun newProducer(): KafkaProducer<ByteArray, ByteArray> {
        val props = java.util.Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, rpcBrokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
            put(ProducerConfig.LINGER_MS_CONFIG, "5")
        }
        return KafkaProducer(props)
    }
    private fun extractLsn(bytes: ByteArray): Long? = try {
        val m: Map<String, Any?> = mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {})
        when (val v = m["__lsn"]) {
            is Number -> v.toLong()
            is String -> v.toLongOrNull()
            is Map<*, *> -> (v["long"] as? Number)?.toLong() ?: (v["long"] as? String)?.toLongOrNull()
            else -> null
        }
    } catch (_: Exception) { null }

    private fun longToBe(v: Long): ByteArray {
        val b = ByteArray(8)
        b[0] = (v ushr 56).toByte(); b[1] = (v ushr 48).toByte()
        b[2] = (v ushr 40).toByte(); b[3] = (v ushr 32).toByte()
        b[4] = (v ushr 24).toByte(); b[5] = (v ushr 16).toByte()
        b[6] = (v ushr 8).toByte();  b[7] = (v).toByte()
        return b
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/FbRequests.kt
Size: 13.31 KB
Code:
// src/main/java/org/cladbe/cdc/engine/FbRequests.kt
package org.cladbe.cdc.engine

import com.google.flatbuffers.FlatBufferBuilder
import SqlRpc.*
import SqlSchema.*
import java.nio.ByteBuffer


object FbRequests {

    fun buildGetDataEnvelope(
        correlationId: String,
        replyTopic: String,
        companyId: String,
        tableName: String,
        order: List<OrderKey>,
        wrapperFbBytes: ByteArray?,
        limit: Int,
        offset: Int?,
        cursorTuple: Map<String, Any?>?,
        strictAfter: Boolean
    ): ByteArray {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val b = FlatBufferBuilder(1024)

        val orderOffsets = order.map { ok ->
            val fieldOff = b.createString(ok.field)
            SqlSchema.OrderKeySpec.createOrderKeySpec(
                b,
                fieldOff,
                ok.sortOrdinal,
                false
            )
        }.toIntArray()
        val orderVec = GetDataReq.createOrderVector(b, orderOffsets)

        // INCLUDE the wrapper if provided
        val wrapperOff = wrapperFbBytes?.let {
            val bb = ByteBuffer.wrap(it)
            val w = BasicSqlDataFilterWrapper.getRootAsBasicSqlDataFilterWrapper(bb)
            copyWrapper(b, w)
        } ?: 0

        val cursorOffsets = cursorTuple?.entries?.map { (field, value) ->
            val fieldOff = b.createString(field)
            val (valType, valOff) = encodeFilterValue(b, value)
            CursorEntry.createCursorEntry(b, fieldOff, valType, valOff)
        }?.toIntArray()

        val cursorVec = if (cursorOffsets != null && cursorOffsets.isNotEmpty())
            GetDataReq.createCursorVector(b, cursorOffsets)
        else 0

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        val companyOff = b.createString(companyId)
        val tableOff = b.createString(tableName)

        val reqOff = GetDataReq.createGetDataReq(
            b,
            companyOff,
            tableOff,
            wrapperOff,
            limit.toLong(),
            (offset ?: 0).toLong(),
            orderVec,
            cursorVec,
            strictAfter
        )

        if (debug) println("[FbRequests] buildGetDataEnvelope corr=$correlationId company=$companyId table=$tableName limit=$limit strictAfter=$strictAfter hasWrapper=${wrapperOff!=0} hasCursor=${cursorVec!=0}")

        val envOff = RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(envOff)
        return b.sizedByteArray()
    }

    fun buildGetDataFromQueryFb(
        correlationId: String,
        replyTopic: String,
        queryFb: ByteArray,
        cursorTuple: Map<String, Any?>,
        limit: Int,
        strictlyAfter: Boolean,
        fullTableName: String
    ): ByteArray {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val bb = ByteBuffer.wrap(queryFb)
        val src = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(bb)

        val companyId = fullTableName.substringBefore('_', "")
        val baseTable = fullTableName.substringAfter('_', fullTableName)

        val b = FlatBufferBuilder(1024)

        val orderOffsets = IntArray(src.orderLength())
        for (i in 0 until src.orderLength()) {
            val ok = src.order(i)!!
            val fieldOff = b.createString(ok.field())
            val okOff = SqlSchema.OrderKeySpec.createOrderKeySpec(
                b, fieldOff, ok.sort(), ok.isPk()
            )
            orderOffsets[i] = okOff
        }
        val orderVec = SqlRpc.GetDataReq.createOrderVector(b, orderOffsets)

        val userWrapper = src.wrapper()
        val afterOff = buildStrictlyAfterWrapper(b, cursorTuple, src)
        val wrapperOff = if (userWrapper == null) {
            afterOff
        } else {
            val children = intArrayOf(copyWrapper(b, userWrapper), afterOff)
            val unionTypes = byteArrayOf(
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte(),
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
            )
            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(
                b,
                BasicSqlDataFilterWrapper.createFiltersVector(b, children)
            )
            BasicSqlDataFilterWrapper.addFiltersType(
                b,
                BasicSqlDataFilterWrapper.createFiltersTypeVector(b, unionTypes)
            )
            BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
        }

        val cidOff  = b.createString(companyId)
        val tblOff  = b.createString(baseTable)
        val reqOff = SqlRpc.GetDataReq.createGetDataReq(
            b,
            cidOff,
            tblOff,
            wrapperOff,
            limit.toLong(),
            0L,
            orderVec,
            0,
            strictlyAfter
        )

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        if (debug) println("[FbRequests] buildGetDataFromQueryFb corr=$correlationId table=$fullTableName limit=$limit strictlyAfter=$strictlyAfter")
        val env = SqlRpc.RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(env)
        return b.sizedByteArray()
    }

    // ---------------- helpers moved here from CdcProcessor ----------------

    private fun buildStrictlyAfterWrapper(
        b: FlatBufferBuilder,
        cursorTuple: Map<String, Any?>,
        src: StreamingSqlDataFilter
    ): Int {
        val disjChildren = mutableListOf<Int>()
        val disjTypes = mutableListOf<Byte>()

        val n = src.orderLength()
        for (k in 0 until n) {
            val conj = mutableListOf<Int>()
            val conjTypes = mutableListOf<Byte>()

            for (i in 0 until k) {
                val ok = src.order(i)!!
                val fName = ok.field()
                val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])
                conj += buildBinaryFilter(b, fName, t, vOff, BasicSqlDataFilterType.equals)
                conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
            }

            val ok = src.order(k)!!
            val fName = ok.field()
            val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])

            val type = when (ok.sort()) {
                OrderSort.ASC_DEFAULT, OrderSort.ASC_NULLS_FIRST, OrderSort.ASC_NULLS_LAST ->
                    BasicSqlDataFilterType.greaterThan
                else -> BasicSqlDataFilterType.lessThan
            }
            conj += buildBinaryFilter(b, fName, t, vOff, type)
            conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()

            val conjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, conj.toIntArray())
            val conjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, conjTypes.toByteArray())

            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(b, conjVec)
            BasicSqlDataFilterWrapper.addFiltersType(b, conjTypeVec)
            val conjOff = BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)

            disjChildren += conjOff
            disjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
        }

        val disjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, disjChildren.toIntArray())
        val disjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, disjTypes.toByteArray())
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.or)
        BasicSqlDataFilterWrapper.addFilters(b, disjVec)
        BasicSqlDataFilterWrapper.addFiltersType(b, disjTypeVec)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyWrapper(b: FlatBufferBuilder, w: BasicSqlDataFilterWrapper?): Int {
        if (w == null) return 0
        val count = w.filtersLength()
        val children = IntArray(count)
        val types = ByteArray(count)

        for (i in 0 until count) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val childW = BasicSqlDataFilterWrapper()
                    w.filters(childW, i)
                    children[i] = copyWrapper(b, childW)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
                }
                else -> {
                    val childL = BasicSqlDataFilter()
                    w.filters(childL, i)
                    children[i] = copyLeaf(b, childL)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
                }
            }
        }

        val fv = BasicSqlDataFilterWrapper.createFiltersVector(b, children)
        val tv = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, types)
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, w.filterWrapperType())
        BasicSqlDataFilterWrapper.addFilters(b, fv)
        BasicSqlDataFilterWrapper.addFiltersType(b, tv)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyLeaf(b: FlatBufferBuilder, lf: BasicSqlDataFilter): Int {
        val fieldOff = b.createString(lf.fieldName())

        val (vt: Byte, vOff: Int) = when (lf.valueType().toInt()) {
            FilterValue.StringValue.toInt() -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
            FilterValue.NumberValue.toInt() -> {
                val tmp = NumberValue(); lf.value(tmp)
                FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, tmp.value())
            }
            FilterValue.Int64Value.toInt() -> {
                val tmp = Int64Value(); lf.value(tmp)
                FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, tmp.value())
            }
            FilterValue.BoolValue.toInt() -> {
                val tmp = BoolValue(); lf.value(tmp)
                FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, tmp.value())
            }
            FilterValue.NullValue.toInt() -> {
                NullValue.startNullValue(b)
                val nv = NullValue.endNullValue(b)
                FilterValue.NullValue.toByte() to nv
            }
            else -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
        }

        val mod = lf.modifier()
        val modOff = if (mod == null) 0 else run {
            SqlFilterModifier.startSqlFilterModifier(b)
            SqlFilterModifier.addDistinct(b, mod.distinct())
            SqlFilterModifier.addCaseInsensitive(b, mod.caseInsensitive())
            SqlFilterModifier.addNullsOrder(b, mod.nullsOrder())
            SqlFilterModifier.endSqlFilterModifier(b)
        }

        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, lf.filterType())
        if (modOff != 0) BasicSqlDataFilter.addModifier(b, modOff)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun buildBinaryFilter(
        b: FlatBufferBuilder,
        field: String,
        vt: Byte,
        vOff: Int,
        type: Int
    ): Int {
        val fieldOff = b.createString(field)
        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, type)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun encodeFilterValue(b: FlatBufferBuilder, v: Any?): Pair<Byte, Int> {
        if (v == null) {
            NullValue.startNullValue(b)
            val off = NullValue.endNullValue(b)
            return FilterValue.NullValue.toByte() to off
        }
        return when (v) {
            is String  -> FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(v))
            is Boolean -> FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, v)
            is Int     -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v.toLong())
            is Long    -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v)
            is Number  -> FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, v.toDouble())
            else       -> {
                val s = v.toString()
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(s))
            }
        }
    }


}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/FbResponses.kt
Size: 636 B
Code:
// src/main/java/org/cladbe/cdc/engine/FbResponses.kt
package org.cladbe.cdc.engine

import SqlRpc.*
import java.nio.ByteBuffer

object FbResponses {
    /** Returns first row JSON (string) from RowsJson, or null if none/other response */
    fun parseRowsJsonFirst(bytes: ByteArray): String? {
        val bb = ByteBuffer.wrap(bytes)
        val env = ResponseEnvelope.getRootAsResponseEnvelope(bb)
        if (!env.ok()) return null
        if (env.dataType() != RpcResponse.RowsJson) return null
        val rows = RowsJson()
        env.data(rows)
        if (rows.rowsLength() <= 0) return null
        return rows.rows(0)
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/KeyDbWriter.kt
Size: 3.35 KB
Code:
// src/main/java/org/cladbe/cdc/engine/KeyDbWriter.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.databind.ObjectMapper
import redis.clients.jedis.JedisPooled
import java.net.URI

class KeyDbWriter(
    url: String = System.getenv("KEYDB_URL") ?: "redis://127.0.0.1:6379",
    private val prefix: String = (System.getenv("KEYDB_PREFIX") ?: "hcache:").replace("\\s".toRegex(), ""),
    private val maxDiffs: Int = (System.getenv("HOTCACHE_MAX_DIFFS") ?: "5000").toInt(),
    private val retentionMs: Long = (System.getenv("HOTCACHE_RETENTION_MS") ?: "${10 * 60_000}").toLong()
) : AutoCloseable {

    private val jedis = JedisPooled(URI(url))
    private val mapper = ObjectMapper()

    private fun kSnap(hashId: String)  = "${prefix}snap:$hashId"
    private fun kDiff(hashId: String)  = "${prefix}diff:$hashId"
    private fun kRange(hashId: String) = "${prefix}range:$hashId"
    private fun kRow(table: String, pk: String) = "${prefix}row:$table:$pk"

    /** snap:{hashId} → {"rows":[...], "cursor":{"lsn": "..."}, "ts": epochMillis} */
    fun writeSnapshot(hashId: String, rows: List<Map<String, Any?>>, lsn: Long, ts: Long = System.currentTimeMillis()) {
        val payload = mapOf("rows" to rows, "cursor" to mapOf("lsn" to lsn.toString()), "ts" to ts)
        val json = mapper.writeValueAsString(payload)
        if (retentionMs > 0) jedis.psetex(kSnap(hashId), retentionMs, json) else jedis.set(kSnap(hashId), json)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs) // keep list alive in step with snap
    }

    /** diff:{hashId} → RPUSH {"lsn":"...", "b64":"..."}; LTRIM; PEXPIRE */
    fun appendDiff(hashId: String, lsn: Long, b64: String) {
        val item = mapper.writeValueAsString(mapOf("lsn" to lsn.toString(), "b64" to b64))
        jedis.rpush(kDiff(hashId), item)
        jedis.ltrim(kDiff(hashId), -maxDiffs.toLong(), -1)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs)
    }

    /** range:{hashId} → list of PKs (index = position) */
    fun writeRangeIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRange(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) jedis.rpush(key, *pksInOrder.toTypedArray())
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    /** row:{table}:{pk} → {"row":{...},"lsn":"..."} (hot row cache for ws_gateway) */
    fun writeHotRow(table: String, pk: String, row: Map<String, Any?>, lsn: Long?) {
        val payload = if (lsn != null && lsn > 0) mapOf("row" to row, "lsn" to lsn.toString()) else mapOf("row" to row)
        val json = mapper.writeValueAsString(payload)
        val key = kRow(table, pk)
        if (retentionMs > 0) jedis.psetex(key, retentionMs, json) else jedis.set(key, json)
    }

    private fun kRev(hashId: String) = "${prefix}ridx:$hashId"

    /** ridx:{hashId} → Hash pk -> pos */
    fun writeReverseIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRev(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) {
            val kv = mutableListOf<String>()
            pksInOrder.forEachIndexed { i, pk ->
                kv += pk; kv += i.toString()
            }
            jedis.hset(key, kv.chunked(2).associate { it[0] to it[1] })
        }
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    override fun close() {
        runCatching { jedis.close() }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/PkUtils.kt
Size: 207 B
Code:
package org.cladbe.cdc.engine

/** Extract the PK value from a decoded row using the configured PK column. */
fun extractPkOrNull(row: Map<String, Any?>, pkCol: String): String? =
    row[pkCol]?.toString()

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/QueryControlProcessor.kt
Size: 1.30 KB
Code:
// src/main/java/org/cladbe/cdc/engine/Processors.kt  (QueryControlProcessor part)
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore

class QueryControlProcessor(
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var qmeta: KeyValueStore<String, ByteArray>
    private lateinit var page : KeyValueStore<String, ByteArray>
    private lateinit var out  : KeyValueStore<String, ByteArray>

    override fun init(context: ProcessorContext<Void, Void>) {
        qmeta = context.getStateStore(qmetaName)
        page  = context.getStateStore(pageName)
        out   = context.getStateStore(outboxName)
    }

    override fun process(record: Record<String, ByteArray>) {
        // Expect key = "$table|$hashId"
        val key = record.key() ?: return
        val fb = record.value()

        if (fb == null) {
            page.delete(key)
            out.delete(key)
            qmeta.delete(key)
            return
        }

        qmeta.put(key, fb)
        if (page.get(key) == null) page.put(key, ByteArray(0))
        if (out.get(key)  == null) out.put(key, ByteArray(0))
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/SeedProcessor.kt
Size: 2.39 KB
Code:
// src/main/java/org/cladbe/cdc/engine/SeedProcessor.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore

class SeedProcessor(
    private val rowCacheName: String,
    private val pageName: String,
    // allow tests to inject a fake/mocked KeyDbWriter
    private val keyDbFactory: () -> KeyDbWriter = { KeyDbWriter() }
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var ctx: ProcessorContext<Void, Void>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var page: KeyValueStore<String, ByteArray>
    private val mapper = ObjectMapper()
    private var keydb: KeyDbWriter? = null

    override fun init(context: ProcessorContext<Void, Void>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        page = context.getStateStore(pageName)
        runCatching { keydb = keyDbFactory() }.onFailure { e ->
            println("[streams] KeyDB disabled in SeedProcessor: ${e.message}")
        }
    }

    override fun process(record: Record<String, ByteArray>) {
        val payload = record.value() ?: return
        val m: Map<String, Any?> = try {
            mapper.readValue(payload, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val table = (m["table"] as? String) ?: return
        val hashId = (m["hashId"] as? String) ?: return
        @Suppress("UNCHECKED_CAST")
        val rows = (m["rows"] as? List<Map<String, Any?>>) ?: return

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"

        val pks = ArrayList<String>(rows.size)
        for (r in rows) {
            val pk = extractPkOrNull(r, pkCol) ?: continue
            rowCache.put(pk, mapper.writeValueAsBytes(r))
            keydb?.writeHotRow(table = table, pk = pk, row = r, lsn = 0L)
            pks += pk
        }

        page.put("$table|$hashId", pks.joinToString(",").toByteArray())

        runCatching {
            keydb?.writeRangeIndex(hashId, pks)
            keydb?.writeSnapshot(hashId, rows, lsn = 0L)
        }
    }

    override fun close() {
        runCatching { keydb?.close() }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/TablePkRegistry.kt
Size: 926 B
Code:
package org.cladbe.cdc.engine

/**
 * Immutable registry of primary-key *column names* per base table.
 * If your Kafka topic/table name is "tenant_table", we'll also try the base part after '_' so
 * the same map works for all tenants that share schema.
 */
object TablePkRegistry {
    // Map base table -> PK column name (edit as needed)
    private val pkByBase: Map<String, String> = mapOf(
        "a_notes"            to "id",
        "whatsapp_messages"  to "message_uuid",
        "a_timeline"         to "messageId"
        // add the rest of your tables here…
    )

    /** For names like "tenant_table", return "table"; if no '_', return input. */
    private fun baseName(table: String): String =
        table.substringAfter('_', table)

    /** Return PK column name for this table (or null if unknown). */
    fun pkColumnFor(table: String): String? =
        pkByBase[table] ?: pkByBase[baseName(table)]
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/Topology.kt
Size: 5.05 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.common.serialization.*
import org.apache.kafka.streams.Topology
import org.apache.kafka.streams.state.Stores
import java.util.regex.Pattern

object StreamsTopology {

    data class Topics(
        val cdcPattern: Pattern,
        val queryControlTopic: String,
        val seedTopic: String?,
        val rpcReplyTopic: String,
        val outboxTopic: String,
        val backfillTopic: String
    )

    data class StateStoreNames(
        val qmeta: String   = "qmeta",
        val page: String    = "page",
        val outbox: String  = "outbox",
        val rowCache: String= "rowCache",
        val pending: String = "pending"
    )

    fun build(topics: Topics, stores: StateStoreNames = StateStoreNames()): Topology {
        val debug = System.getenv("CDC_DEBUG") == "1"
        if (debug) println("[Topology] Building with outbox=${topics.outboxTopic} backfill=${topics.backfillTopic} rpcReply=${topics.rpcReplyTopic}")

        val top = Topology()

        val qmetaStore = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.qmeta),
            Serdes.String(), Serdes.ByteArray())
        val pageStore  = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.page),
            Serdes.String(), Serdes.ByteArray())
        val outboxStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.outbox),
            Serdes.String(), Serdes.ByteArray())
        val rowCacheStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.rowCache),
            Serdes.String(), Serdes.ByteArray())
        val pendingStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.pending),
            Serdes.String(), Serdes.String())

        val cdcSource = "src-cdc"
        top.addSource(
            cdcSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.cdcPattern
        )

        val qctlSource = "src-qctl"
        top.addSource(
            qctlSource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.queryControlTopic
        )

        val backfillSource = "src-backfill"
        top.addSource(
            backfillSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.backfillTopic
        )

        val rpcReplySource = "src-rpc-replies"
        top.addSource(
            rpcReplySource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.rpcReplyTopic
        )

        val seedSource: String? = topics.seedTopic?.let {
            "src-seed".also { name ->
                top.addSource(
                    name,
                    StringDeserializer(), ByteArrayDeserializer(),
                    it
                )
            }
        }

        val qctlProc = "proc-qctl"
        top.addProcessor(qctlProc, {
            QueryControlProcessor(
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox
            )
        }, qctlSource)
        top.addStateStore(qmetaStore, qctlProc)
        top.addStateStore(pageStore, qctlProc)
        top.addStateStore(outboxStore, qctlProc)

        val cdcProc = "proc-cdc"
        top.addProcessor(cdcProc, {
            CdcProcessor(
                rowCacheName = stores.rowCache,
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox,
                pendingName = stores.pending
            )
        }, cdcSource, backfillSource)
        top.addStateStore(rowCacheStore, cdcProc)
        top.addStateStore(qmetaStore, cdcProc)
        top.addStateStore(pageStore, cdcProc)
        top.addStateStore(outboxStore, cdcProc)
        top.addStateStore(pendingStore, cdcProc)

        if (seedSource != null) {
            val seedProc = "proc-seed"
            top.addProcessor(seedProc, {
                SeedProcessor(
                    rowCacheName = stores.rowCache,
                    pageName = stores.page
                )
            }, seedSource)
            top.addStateStore(rowCacheStore, seedProc)
            top.addStateStore(pageStore, seedProc)
        }

        val backfillProc = "proc-backfill-replies"
        val backfillSink = "sink-backfill"
        top.addProcessor(backfillProc, {
            BackfillReplyProcessor(
                rowCacheName = stores.rowCache,
                pendingName = stores.pending,
                backfillTopicName = backfillSink
            )
        }, rpcReplySource)
        top.addStateStore(rowCacheStore, backfillProc)
        top.addStateStore(pendingStore, backfillProc)
        top.addSink(
            backfillSink,
            topics.backfillTopic,
            ByteArraySerializer(), ByteArraySerializer(),
            backfillProc
        )

        val outboxSink = "sink-outbox"
        top.addSink(
            outboxSink,
            topics.outboxTopic,
            StringSerializer(), ByteArraySerializer(),
            cdcProc
        )

        if (debug) println("[Topology] Build complete")
        return top
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/tools/SendQctl.kt
Size: 2.62 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.StringSerializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import java.util.Properties
import java.nio.ByteBuffer
import com.google.flatbuffers.FlatBufferBuilder
import SqlSchema.*
import SqlSchema.StreamingSqlDataFilter as Q

object Tools {
    @JvmStatic
    fun main(args: Array<String>) {
        val brokers = System.getenv("KAFKA_BROKERS") ?: "localhost:29092"
        val qctlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control"
        val debug = System.getenv("CDC_DEBUG") == "1"

        if (args.isEmpty()) {
            println("Usage: Tools install-notes-query");
            return
        }
        when (args[0]) {
            "install-notes-query" -> {
                val key = "a_notes|demo"
                val fb = buildNotesQueryFb()
                if (debug) println("[Tools] sending qctl key=$key topic=$qctlTopic brokers=$brokers")
                send(kTopic = qctlTopic, key = key, value = fb, brokers = brokers)
                println("Installed query meta for key=$key to $qctlTopic")
            }
            else -> println("Unknown cmd: ${args[0]}")
        }
    }

    private fun send(kTopic: String, key: String, value: ByteArray, brokers: String) {
        val props = Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
        KafkaProducer<String, ByteArray>(props).use { p ->
            p.send(ProducerRecord(kTopic, key, value)).get()
        }
    }

    private fun buildNotesQueryFb(): ByteArray {
        val b = FlatBufferBuilder(256)

        val order = intArrayOf(
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("__lsn"),
                OrderSort.DESC_DEFAULT, false
            ),
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("id"),
                OrderSort.DESC_DEFAULT, true
            )
        )
        val orderVec = Q.createOrderVector(b, order)
        Q.startStreamingSqlDataFilter(b)
        Q.addLimit(b, 10L) // <-- must be long
        Q.addOrder(b, orderVec)
        val off = Q.endStreamingSqlDataFilter(b)
        b.finish(off)
        return b.sizedByteArray()
    }
}
-------- [ Separator ] ------

File Name: src/main/resources/logback.xml
Size: 274 B
Code:
<configuration>
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder><pattern>%d %-5level %logger{36} - %msg%n</pattern></encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="STDOUT"/>
    </root>
</configuration>

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/DiffOpsTest.kt
Size: 2.01 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.OrderKeySpec
import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.Test

class DiffOpsTest {

    private val acc = RowAccessor<Map<String, Any?>> { row, f -> row[f] }
    private val spec = listOf(
        OrderKeySpec.of("score", Direction.DESC, null),
        OrderKeySpec.of("updated_at", Direction.DESC, null),
        OrderKeySpec.pk("id", Direction.DESC)
    )
    private val tsHints = mapOf("updated_at" to TimestampUnit.MICROS)

    @Test
    fun insertEvictAndReposition() {
        val rows = mutableMapOf<Long, Map<String, Any?>>(
            1L to mapOf("id" to 1L, "score" to 10, "updated_at" to 1_000_000L),
            2L to mapOf("id" to 2L, "score" to 20, "updated_at" to 1_000_100L),
            3L to mapOf("id" to 3L, "score" to 15, "updated_at" to 1_000_050L),
            4L to mapOf("id" to 4L, "score" to 25, "updated_at" to 1_000_200L)
        )
        val lookup = PkLookup<Long, Map<String, Any?>> { rows[it] }
        val cmp = SqlComparators.pkComparator(spec, lookup, acc, tsHints)

        val page = mutableListOf<Long>()
        val K = 3

        // fill
        DiffOps.insertAndDiff(page, 1L, K, cmp)
        DiffOps.insertAndDiff(page, 2L, K, cmp)
        DiffOps.insertAndDiff(page, 3L, K, cmp)
        assertEquals(listOf(2L, 3L, 1L), page)

        // insert 4L (best) -> evicts tail (1L)
        val diffs = DiffOps.insertAndDiff(page, 4L, K, cmp)
        assertEquals(listOf(4L, 2L, 3L), page)
        // Expect ADDED for 4L at pos 0, REMOVED for 1L at previous tail pos 2
        assertTrue(diffs.any { it is Diff.Added && it.pk == 4L && it.pos == 0 })
        assertTrue(diffs.any { it is Diff.Removed && it.pk == 1L && it.pos == 2 })

        // improve 3L → moves before 2L
        rows[3L] = rows[3L]!!.toMutableMap().apply { put("score", 22) }
        val moved = DiffOps.repositionAndDiff(page, 3L, cmp, payloadChanged = true)
        assertEquals(listOf(4L, 3L, 2L), page)
        assertTrue(moved.single() is Diff.Modified)
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/FbPredicateEndToEndTest.kt
Size: 1.78 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.Test
import java.nio.ByteBuffer

class FbPredicateEndToEndTest {

    @Test
    fun `equals and inList and regex all work`() {
        val b = com.google.flatbuffers.FlatBufferBuilder(256)

        // Build 3 basics
        val eqCity   = FbTestBuilders.basicEqualsString(b, "city", "Berlin")
        val inTags   = FbTestBuilders.basicInListString(b, "tag", listOf("hot","new","sale"))
        val rxEmail  = FbTestBuilders.basicRegex(b, "email", ".*@example\\.com$", ci = true)

        val wrapAnd  = FbTestBuilders.wrapperAnd(b, listOf(eqCity, inTags, rxEmail))

        // ORDER BY score DESC, id DESC (pk)
        val ordScore = FbTestBuilders.orderDesc(b, "score", isPk = false)
        val ordPk    = FbTestBuilders.orderDesc(b, "id",    isPk = true)

        val bytes = FbTestBuilders.streamingQueryBytes(
            limit = 10,
            orders = listOf(ordScore, ordPk),
            filtersWrapper = wrapAnd,
            hashId = "q1"
        )

        val q = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(ByteBuffer.wrap(bytes))

        // runtime adapter
        val spec = FbQueryAdapterImpl(tableName = "products", pkField = "id").parse(bytes)

        // rows
        val rowOk = mapOf("id" to 1L, "score" to 99, "city" to "Berlin", "tag" to "new", "email" to "x@EXAMPLE.com")
        val rowBadCity = rowOk + ("city" to "Paris")
        val rowBadTag  = rowOk + ("tag" to "old")
        val rowBadMail = rowOk + ("email" to "foo@bar.com")

        assertTrue(spec.whereMatches(rowOk))
        assertFalse(spec.whereMatches(rowBadCity))
        assertFalse(spec.whereMatches(rowBadTag))
        assertFalse(spec.whereMatches(rowBadMail))
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/FbTestBuilders.kt
Size: 5.90 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*
import com.google.flatbuffers.FlatBufferBuilder

object FbTestBuilders {

    // ----- FilterValue helpers (unchanged) -----
    fun fvString(b: FlatBufferBuilder, s: String): Pair<Int, Int> {
        val sv = StringValue.createStringValue(b, b.createString(s))
        return FilterValue.StringValue.toInt() to sv
    }
    fun fvInt64(b: FlatBufferBuilder, v: Long): Pair<Int, Int> {
        val off = Int64Value.createInt64Value(b, v)
        return FilterValue.Int64Value.toInt() to off
    }
    fun fvNumber(b: FlatBufferBuilder, v: Double): Pair<Int, Int> {
        val off = NumberValue.createNumberValue(b, v)
        return FilterValue.NumberValue.toInt() to off
    }
    fun fvBool(b: FlatBufferBuilder, v: Boolean): Pair<Int, Int> {
        val off = BoolValue.createBoolValue(b, v)
        return FilterValue.BoolValue.toInt() to off
    }
    fun fvStringList(b: FlatBufferBuilder, values: List<String>): Pair<Int, Int> {
        val vec = StringList.createValuesVector(b, values.map { b.createString(it) }.toIntArray())
        val off = StringList.createStringList(b, vec)
        return FilterValue.StringList.toInt() to off
    }

    // ----- Basic filter builders (INT valueType) -----
    fun basicEqualsString(b: FlatBufferBuilder, field: String, value: String): Int {
        val (vt, voff) = fvString(b, value)
        val fname = b.createString(field)
        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fname)
        BasicSqlDataFilter.addValueType(b, vt.toByte()) // <— INT, not Byte
        BasicSqlDataFilter.addValue(b, voff)
        BasicSqlDataFilter.addFilterType(b, BasicSqlDataFilterType.equals) // enum works as is
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    fun basicInListString(b: FlatBufferBuilder, field: String, values: List<String>): Int {
        val (vt, voff) = fvStringList(b, values)
        val fname = b.createString(field)
        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fname)
        BasicSqlDataFilter.addValueType(b, vt.toByte()) // <— INT, not Byte
        BasicSqlDataFilter.addValue(b, voff)
        BasicSqlDataFilter.addFilterType(b, BasicSqlDataFilterType.inList)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    fun basicRegex(b: FlatBufferBuilder, field: String, pattern: String, ci: Boolean = false): Int {
        val (vt, voff) = fvString(b, pattern)
        val fname = b.createString(field)

        val mod = SqlFilterModifier.createSqlFilterModifier(
            b,
            /*distinct*/ false,
            /*case_insensitive*/ ci,
            /*nulls_order*/ NullsSortOrder.default_
        )

        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fname)
        BasicSqlDataFilter.addValueType(b, vt.toByte()) // <— INT, not Byte
        BasicSqlDataFilter.addValue(b, voff)
        BasicSqlDataFilter.addFilterType(b, BasicSqlDataFilterType.regex)
        BasicSqlDataFilter.addModifier(b, mod)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    // ----- Wrapper (AND) -----
    fun wrapperAnd(b: FlatBufferBuilder, basics: List<Int>): Int {
        val types = basics.map { BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte() }.toByteArray()
        val filtersVec = BasicSqlDataFilterWrapper.createFiltersVector(b, basics.toIntArray())
        val typesVec   = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, types)

        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
        BasicSqlDataFilterWrapper.addFilters(b, filtersVec)
        BasicSqlDataFilterWrapper.addFiltersType(b, typesVec)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    // ----- Order spec (fully qualify generated class) -----
    fun orderDesc(b: FlatBufferBuilder, field: String, isPk: Boolean = false): Int {
        val f = b.createString(field)
        return SqlSchema.OrderKeySpec.createOrderKeySpec(b, f, OrderSort.DESC_DEFAULT, isPk)
    }
    fun orderAsc(b: FlatBufferBuilder, field: String, isPk: Boolean = false): Int {
        val f = b.createString(field)
        return SqlSchema.OrderKeySpec.createOrderKeySpec(b, f, OrderSort.ASC_DEFAULT, isPk)
    }

    // ----- Top-level query (cursor valueType = INT) -----
    fun streamingQueryBytes(
        limit: Int,
        orders: List<Int>,
        filtersWrapper: Int? = null,
        cursor: List<Pair<String, Pair<Int, Int>>> = emptyList(),
        hashId: String = "h1"
    ): ByteArray {
        val b = FlatBufferBuilder(256)
        val hashOff = b.createString(hashId)

        val orderVec = if (orders.isNotEmpty())
            StreamingSqlDataFilter.createOrderVector(b, orders.toIntArray()) else 0

        val cursorOffsets = if (cursor.isNotEmpty()) {
            val entries = cursor.map { (field, vpair) ->
                val fieldOff = b.createString(field)
                CursorEntry.startCursorEntry(b)
                CursorEntry.addField(b, fieldOff)
                CursorEntry.addValueType(b, vpair.first.toByte()) // <— INT, not Byte
                CursorEntry.addValue(b, vpair.second)
                CursorEntry.endCursorEntry(b)
            }
            StreamingSqlDataFilter.createCursorVector(b, entries.toIntArray())
        } else 0

        StreamingSqlDataFilter.startStreamingSqlDataFilter(b)
        StreamingSqlDataFilter.addHash(b, hashOff)
        StreamingSqlDataFilter.addLimit(b, limit.toLong())
        if (filtersWrapper != null) StreamingSqlDataFilter.addWrapper(b, filtersWrapper)
        if (orderVec != 0) StreamingSqlDataFilter.addOrder(b, orderVec)
        if (cursorOffsets != 0) StreamingSqlDataFilter.addCursor(b, cursorOffsets)
        val root = StreamingSqlDataFilter.endStreamingSqlDataFilter(b)

        b.finish(root)
        return b.sizedByteArray()
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/LivePaginationTopology.kt
Size: 2.85 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.common.serialization.Serdes
import org.apache.kafka.streams.StreamsBuilder
import org.apache.kafka.streams.Topology
import org.apache.kafka.streams.kstream.Consumed
import org.apache.kafka.streams.kstream.Produced
import org.apache.kafka.streams.processor.api.ProcessorSupplier
import org.apache.kafka.streams.state.Stores

object LivePaginationTopology {

    fun build(
        table: String,
        cdcTopic: String = "server.cdc.$table",
        queryTopic: String = "user.query.$table",
        outboxTopic: String = "server.page.$table.diffs"
    ): Topology {
        val b = StreamsBuilder()

        val rowCacheName = "rowCache.$table"
        val qmetaName    = "qmeta"
        val pageName     = "pagePks"
        val outboxName   = "outbox"

        // ---- FIX: wrap suppliers with keyValueStoreBuilder(...) ----
        val rowCacheStore = Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore(rowCacheName),
            Serdes.String(),            // key: PK as String (adjust if you use bytes)
            Serdes.ByteArray()          // value: row bytes
        )
        val qmetaStore = Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore(qmetaName),
            Serdes.String(),            // key: "$table|$hashId"
            Serdes.ByteArray()          // value: FB query bytes
        )
        val pageStore = Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore(pageName),
            Serdes.String(),            // key: "$table|$hashId"
            Serdes.ByteArray()          // value: compact PK-list bytes
        )
        val outboxStore = Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore(outboxName),
            Serdes.String(),            // key: "$table|$hashId"
            Serdes.ByteArray()          // value: pending diffs bytes
        )

        b.addStateStore(rowCacheStore)
        b.addStateStore(qmetaStore)
        b.addStateStore(pageStore)
        b.addStateStore(outboxStore)

        // ---- Streams ----
        val queries = b.stream(queryTopic, Consumed.with(Serdes.String(), Serdes.ByteArray()))
        val cdc     = b.stream(cdcTopic,   Consumed.with(Serdes.ByteArray(), Serdes.ByteArray()))

        // Control plane (subscribe/unsubscribe)
        queries.process(
            ProcessorSupplier { QueryControlProcessor(table, qmetaName, pageName, outboxName) },
            qmetaName, pageName, outboxName
        )

        // CDC → KStream<String, ByteArray> diffs
        val diffs = cdc.process(
            ProcessorSupplier { CdcProcessor(table, rowCacheName, qmetaName, pageName, outboxName) },
            rowCacheName, qmetaName, pageName, outboxName
        )

        // route to outbox topic
        diffs.to(outboxTopic, Produced.with(Serdes.String(), Serdes.ByteArray()))

        return b.build()
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/PageOpsTest.kt
Size: 2.57 KB
Code:
package org.cladbe.cdc.Evaluator

import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.Test

class PageOpsTest {
    private val acc = RowAccessor<Map<String, Any?>> { row, f -> row[f] }
    private val spec = listOf(
        OrderKeySpec.of("score", Direction.DESC, null),
        OrderKeySpec.of("updated_at", Direction.DESC, null),
        OrderKeySpec.pk("id", Direction.DESC)
    )
    private val tsHints = mapOf("updated_at" to TimestampUnit.MICROS)

    @Test
    fun topKInsertAndEvict() {
        val rows = mutableMapOf<Long, Map<String, Any?>>(
            1L to mapOf("id" to 1L, "score" to 10, "updated_at" to 1_000_000L),
            2L to mapOf("id" to 2L, "score" to 20, "updated_at" to 1_000_100L),
            3L to mapOf("id" to 3L, "score" to 15, "updated_at" to 1_000_050L),
            4L to mapOf("id" to 4L, "score" to 5,  "updated_at" to 1_000_200L)
        )
        val lookup = PkLookup<Long, Map<String, Any?>> { rows[it] }
        val cmp = SqlComparators.pkComparator(spec, lookup, acc, tsHints)

        val page = mutableListOf<Long>()
        val K = 3

        PageOps.insertIfBetter(page, 1L, K, cmp)
        PageOps.insertIfBetter(page, 2L, K, cmp)
        PageOps.insertIfBetter(page, 3L, K, cmp)
        assertEquals(listOf(2L, 3L, 1L), page) // sorted by score DESC, then updated_at DESC, then id DESC

        // Candidate worse than tail → ignored
        val res4 = PageOps.insertIfBetter(page, 4L, K, cmp)
        assertFalse(res4.inserted)
        assertEquals(listOf(2L, 3L, 1L), page)

        // Now improve row 1: score becomes 25 → should move to front
        rows[1L] = rows[1L]!!.toMutableMap().apply { put("score", 25) }
        val moved = PageOps.repositionIfNeeded(page, 1L, cmp)
        assertTrue(moved.moved)
        assertEquals(listOf(1L, 2L, 3L), page)
    }

    @Test
    fun nullsRule() {
        val rows = mutableMapOf<Long, Map<String, Any?>>(
            10L to mapOf("id" to 10L, "score" to null, "updated_at" to 1_0L),
            11L to mapOf("id" to 11L, "score" to 0,    "updated_at" to 1_1L)
        )
        val lookup = PkLookup<Long, Map<String, Any?>> { rows[it] }
        val specNullsLast = listOf(
            OrderKeySpec.of("score", Direction.DESC, Nulls.LAST),
            OrderKeySpec.pk("id", Direction.ASC)
        )
        val cmp = SqlComparators.pkComparator(specNullsLast, lookup, acc, emptyMap())
        val page = mutableListOf<Long>()

        PageOps.insertIfBetter(page, 10L, 5, cmp)
        PageOps.insertIfBetter(page, 11L, 5, cmp)
        assertEquals(listOf(11L, 10L), page) // NULLS LAST under DESC
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/QueryEngineSmokeTest.kt
Size: 2.00 KB
Code:
package org.cladbe.cdc.Evaluator

import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.Test

class QueryEngineSmokeTest {

    @Test
    fun `upsert, promote, delete produce expected diffs and cursor`() {
        val spec = DerivedSpec(
            table = "a_notes",
            k = 2,
            order = listOf(
                OrderKeySpec.of("score", Direction.DESC),
                OrderKeySpec.pk("id", Direction.DESC)
            ),
            cursorTuple = null,
            whereMatches = { row -> (row["archived"] as? Boolean) != true }
        )
        val eng = QueryEngine(
            spec = spec,
            pkField = "id",
            getPk = { r -> r["id"] as Long }
        )

        val r1 = mapOf("id" to 1L, "score" to 5,  "archived" to false)
        val r2 = mapOf("id" to 2L, "score" to 10, "archived" to false)
        val r3 = mapOf("id" to 3L, "score" to 6,  "archived" to false)

        // insert r1 → add
        var changes = eng.upsert(r1)
        assertEquals(listOf(OutboxCodec.WireDiff("added", 1L, pos = 0)), changes)

        // insert r2 → add
        changes = eng.upsert(r2)
        assertEquals(listOf(OutboxCodec.WireDiff("added", 2L, pos = 0)), changes) // higher score goes to 0

        // insert r3 → evict tail (1)
        changes = eng.upsert(r3)
        assertEquals(2, changes.size)
        assertEquals("added", changes[0].type)
        assertEquals("removed", changes[1].type)
        assertEquals(1L, changes[1].pk) // evicted tail

        // archive r3 → removed, and page shrinks (we’ll fill via DB in the real engine)
        val r3arch = r3 + ("archived" to true)
        changes = eng.upsert(r3arch)
        assertEquals(1, changes.size)
        assertEquals("removed", changes[0].type)
        assertEquals(3L, changes[0].pk)

        // cursor = last tuple (only pk=id=2 remains)
        val cur = eng.currentCursor()
        assertEquals(2, cur?.size)
        assertEquals(10, cur?.get("score"))
        assertEquals(2L, eng.page.single())
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/SqlComparatorAndPageOpsTest.kt
Size: 3.00 KB
Code:
package org.cladbe.cdc.Evaluator

import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.Test
import java.time.Instant

class SqlComparatorAndPageOpsTest {

    private val acc = RowAccessor<Map<String, Any?>> { row, f -> row[f] }

    @Test
    fun `row comparator handles ASC DESC nulls and timestamps`() {
        val spec = listOf(
            OrderKeySpec.of("score", Direction.DESC),         // NULLS FIRST by default for DESC
            OrderKeySpec.of("updated_at", Direction.DESC),    // newer first
            OrderKeySpec.pk("id", Direction.DESC)             // PK tiebreaker
        )
        val tsHints = mapOf("updated_at" to TimestampUnit.MILLIS)

        val cmp = SqlComparators.rowComparator(spec, acc, tsHints)

        val a = mapOf("id" to 1L, "score" to 10, "updated_at" to 1724300000000L)
        val b = mapOf("id" to 2L, "score" to 10, "updated_at" to 1724400000000L) // newer → before a
        val c = mapOf("id" to 3L, "score" to null, "updated_at" to 1724500000000L)

        // b should come before a (same score, newer ts)
        assertTrue(cmp.compare(b, a) < 0)

        // null score (DESC + NULLS FIRST) → c should come before a/b
        assertTrue(cmp.compare(c, a) < 0)
        assertTrue(cmp.compare(c, b) < 0)

        // timestamps as Instant also normalize
        val d = mapOf("id" to 4L, "score" to 10, "updated_at" to Instant.ofEpochMilli(1724600000000L))
        assertTrue(cmp.compare(d, b) < 0) // d newer than b
    }

    @Test
    fun `PageOps insert evicts tail when better and reposition moves`() {
        val spec = listOf(
            OrderKeySpec.of("score", Direction.DESC),
            OrderKeySpec.pk("id", Direction.DESC)
        )
        val acc = RowAccessor<Map<String, Any?>> { r, f -> r[f] }
        val rows = mutableMapOf<Long, Map<String, Any?>>(
            1L to mapOf("id" to 1L, "score" to 5),
            2L to mapOf("id" to 2L, "score" to 4),
            3L to mapOf("id" to 3L, "score" to 6)
        )
        val lookup = PkLookup<Long, Map<String, Any?>> { rows[it] }
        val cmp = SqlComparators.pkComparator(spec, lookup, acc)

        val page = mutableListOf<Long>()
        val k = 2

        // insert 1,2
        var diffs = DiffOps.insertAndDiff(page, 1L, k, cmp)
        assertEquals(listOf(Diff.Added(1L, 0)), diffs)
        diffs = DiffOps.insertAndDiff(page, 2L, k, cmp)
        assertEquals(listOf(Diff.Added(2L, 1)), diffs)
        assertEquals(listOf(1L, 2L), page)

        // insert 3 (better than tail 2) → evict 2
        diffs = DiffOps.insertAndDiff(page, 3L, k, cmp)
        assertEquals(2, diffs.size)
        assertEquals(Diff.Added(3L, 0), diffs[0])
        assertEquals(Diff.Removed(2L, 1), diffs[1])
        assertEquals(listOf(3L, 1L), page)

        // score of 1 improves → reposition
        rows[1L] = mapOf("id" to 1L, "score" to 10)
        val move = DiffOps.repositionAndDiff(page, 1L, cmp, payloadChanged = true)
        assertEquals(listOf(Diff.Modified(1L, 0, 1)), move)
        assertEquals(listOf(1L, 3L), page)
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/SqlComparatorsTest.kt
Size: 62 B
Code:
package org.cladbe.cdc.Evaluator

class SqlComparatorsTest {
}
-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/engine/KeyDbWriterTest.kt
Size: 2.61 KB
Code:
// src/test/java/org/cladbe/cdc/engine/KeyDbWriterTest.kt
package org.cladbe.cdc.engine

import io.mockk.*
import org.junit.jupiter.api.AfterEach
import org.junit.jupiter.api.Assertions.assertTrue
import org.junit.jupiter.api.Test
import redis.clients.jedis.JedisPooled
import java.net.URI

class KeyDbWriterTest {

    @AfterEach fun tearDown() = unmockkAll()

    @Test
    fun `writeSnapshot sets snap and expires diff`() {
        mockkConstructor(JedisPooled::class)
        every { anyConstructed<JedisPooled>().psetex(any(), any(), any()) } returns "OK"
        every { anyConstructed<JedisPooled>().pexpire(any(), any()) } returns 1
        every { anyConstructed<JedisPooled>().set(any(), any()) } returns "OK"

        val w = KeyDbWriter(url = "redis://127.0.0.1:6379", prefix = "hcache:", maxDiffs = 10, retentionMs = 5000)
        w.writeSnapshot("H", listOf(mapOf("id" to 1)), 42L)

        verify(exactly = 1) { anyConstructed<JedisPooled>().psetex("hcache:snap:H", 5000, any()) }
        verify { anyConstructed<JedisPooled>().pexpire("hcache:diff:H", 5000) }
        w.close()
    }

    @Test
    fun `appendDiff pushes and trims list with expire`() {
        mockkConstructor(JedisPooled::class)
        every { anyConstructed<JedisPooled>().rpush(any(), any<String>()) } returns 1
        every { anyConstructed<JedisPooled>().ltrim(any(), any(), any()) } returns "OK"
        every { anyConstructed<JedisPooled>().pexpire(any(), any()) } returns 1

        val w = KeyDbWriter(url = "redis://127.0.0.1:6379", prefix = "hcache:", maxDiffs = 3, retentionMs = 1000)
        w.appendDiff("X", 77L, "AAA")

        verify { anyConstructed<JedisPooled>().rpush("hcache:diff:X", any<String>()) }
        verify { anyConstructed<JedisPooled>().ltrim("hcache:diff:X", -3, -1) }
        verify { anyConstructed<JedisPooled>().pexpire("hcache:diff:X", 1000) }
        w.close()
    }

    @Test
    fun `writeRangeIndex resets list and sets expire`() {
        mockkConstructor(JedisPooled::class)
        every { anyConstructed<JedisPooled>().del(any()) } returns 1
        every { anyConstructed<JedisPooled>().rpush(any(), *anyVararg<String>()) } returns 2
        every { anyConstructed<JedisPooled>().pexpire(any(), any()) } returns 1

        val w = KeyDbWriter(url = "redis://127.0.0.1:6379", prefix = "hcache:", retentionMs = 7000)
        w.writeRangeIndex("H", listOf("pk1","pk2"))

        verify { anyConstructed<JedisPooled>().del("hcache:range:H") }
        verify { anyConstructed<JedisPooled>().rpush("hcache:range:H", "pk1", "pk2") }
        verify { anyConstructed<JedisPooled>().pexpire("hcache:range:H", 7000) }
        w.close()
    }
}
-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/Evaluator/engine/SeedProcessorTest.kt
Size: 3.45 KB
Code:
// src/test/java/org/cladbe/cdc/engine/SeedProcessorTest.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.databind.ObjectMapper
import io.mockk.*
import org.apache.kafka.common.serialization.Serdes
import org.apache.kafka.common.serialization.StringSerializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.streams.Topology
import org.apache.kafka.streams.TopologyTestDriver
import org.apache.kafka.streams.state.KeyValueStore
import org.apache.kafka.streams.state.Stores
import org.junit.jupiter.api.AfterEach
import org.junit.jupiter.api.Assertions.assertArrayEquals
import org.junit.jupiter.api.Assertions.assertEquals
import org.junit.jupiter.api.Test
import java.util.Properties

class SeedProcessorTest {

    private val mapper = ObjectMapper()

    @AfterEach fun tearDown() = unmockkAll()

    @Test
    fun `seed warms rowCache, page, and KeyDB`() {
        // --- mock KeyDbWriter
        val keydb = mockk<KeyDbWriter>(relaxed = true)

        // --- build tiny topology: source -> SeedProcessor(+stores)
        val rowCacheName = "rowCache"
        val pageName = "page"
        val seedTopic = "server.page.seed"

        val top = Topology()
        top.addSource("seed-src", org.apache.kafka.common.serialization.StringDeserializer(), org.apache.kafka.common.serialization.ByteArrayDeserializer(), seedTopic)

        val rowCacheStore = Stores.keyValueStoreBuilder(
            Stores.inMemoryKeyValueStore(rowCacheName),
            Serdes.String(), Serdes.ByteArray()
        )
        val pageStore = Stores.keyValueStoreBuilder(
            Stores.inMemoryKeyValueStore(pageName),
            Serdes.String(), Serdes.ByteArray()
        )

        val procName = "seed-proc"
        top.addProcessor(procName, {
            SeedProcessor(rowCacheName, pageName) { keydb }
        }, "seed-src")
        top.addStateStore(rowCacheStore, procName)
        top.addStateStore(pageStore, procName)

        TopologyTestDriver(top, Properties().apply {
            put("application.id", "test-app")
            put("bootstrap.servers", "dummy:9092")
        }).use { driver ->
            val input = driver.createInputTopic(seedTopic, StringSerializer(), ByteArraySerializer())

            // payload like ws_gateway publishes: { companyId, table, hashId, cursor, rows }
            val payload = mapOf(
                "companyId" to "acme",
                "table" to "acme_a_notes",
                "hashId" to "H1",
                "cursor" to mapOf("lsn" to "0"),
                "rows" to listOf(
                    mapOf("id" to 1, "body" to "a"),
                    mapOf("id" to 2, "body" to "b")
                )
            )
            input.pipeInput("ignored", mapper.writeValueAsBytes(payload))

            val rowCache = driver.getKeyValueStore<String, ByteArray>(rowCacheName)
            val page = driver.getKeyValueStore<String, ByteArray>(pageName)

            // assert rowCache warmed
            val r1 = mapper.readTree(rowCache["1"])
            val r2 = mapper.readTree(rowCache["2"])
            assertEquals("a", r1.get("body").asText())
            assertEquals("b", r2.get("body").asText())

            // assert page warmed (CSV "1,2")
            assertArrayEquals("1,2".toByteArray(), page["acme_a_notes|H1"])

            // verify KeyDB interactions
            verify { keydb.writeRangeIndex("H1", listOf("1","2")) }
            verify { keydb.writeSnapshot("H1", match { it.size == 2 }, 0L) }
        }
    }
}
-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/TestFilterProducer.java
Size: 4.45 KB
Code:
package org.cladbe.cdc;

import com.google.flatbuffers.FlatBufferBuilder;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.ByteArraySerializer;
import org.apache.kafka.common.serialization.StringSerializer;
import SqlSchema.*; // <-- use this package, not org.cladbe.flatbuffers.*

import java.nio.ByteBuffer;
import java.util.Properties;

public class TestFilterProducer {

    private static final String FILTERS_TOPIC = "user.filters";
    private static final String BOOTSTRAP = "localhost:9092";

    public static void main(String[] args) {
        // 1) Build a sample filter:  (title CONTAINS "Note") case-insensitive
        byte[] fb = buildContainsFilter(
                /*field*/ "title",
                /*needle*/ "Note",
                /*caseInsensitive*/ true
        );

        // 2) Produce it to user.filters with some hashId
        String hashId = "hash-1";

        Properties p = new Properties();
        p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP);
        p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());

        try (KafkaProducer<String, byte[]> producer = new KafkaProducer<>(p)) {
            String topicName = "server.cdc.a_notes";
            String key       = topicName + "|" + hashId;

            ProducerRecord<String, byte[]> rec = new ProducerRecord<>(FILTERS_TOPIC, key, fb);
            producer.send(rec).get();
            System.out.println("Sent filter for key=" + hashId + " to topic " + FILTERS_TOPIC);
        } catch (Exception e) {
            e.printStackTrace();
        }

        // Optional: sanity-read back the buffer locally to verify it parses
        BasicSqlDataFilterWrapper parsed =
                BasicSqlDataFilterWrapper.getRootAsBasicSqlDataFilterWrapper(ByteBuffer.wrap(fb));
        System.out.println("Wrapper type = " + parsed.filterWrapperType() + ", filters=" + parsed.filtersLength());
    }

    /**
     * Build a BasicSqlDataFilterWrapper with a single BasicSqlDataFilter:
     *   field CONTAINS needle (optionally case-insensitive)
     */
    private static byte[] buildContainsFilter(String field, String needle, boolean caseInsensitive) {
        FlatBufferBuilder b = new FlatBufferBuilder(256);

        // ----- value: union(FilterValue) with StringValue -----
        int needleStr = b.createString(needle);
        int stringValue = StringValue.createStringValue(b, needleStr);

        // ----- field name -----
        int fieldName = b.createString(field);

        // ----- modifier (distinct=false, case_insensitive=..., nulls_order=default) -----
        int mod = SqlFilterModifier.createSqlFilterModifier(
                b,
                false,                          // distinct
                caseInsensitive,                // case_insensitive
                NullsSortOrder.default_         // nulls_order
        );

        // ----- the BasicSqlDataFilter table -----
        BasicSqlDataFilter.startBasicSqlDataFilter(b);
        BasicSqlDataFilter.addFieldName(b, fieldName);
        BasicSqlDataFilter.addFilterType(b, BasicSqlDataFilterType.contains);
        BasicSqlDataFilter.addModifier(b, mod);
        BasicSqlDataFilter.addValueType(b, FilterValue.StringValue);
        BasicSqlDataFilter.addValue(b, stringValue);
        int filterOff = BasicSqlDataFilter.endBasicSqlDataFilter(b);

        // ----- union vector: types + values -----
        // For a vector of unions, FlatBuffers uses a parallel vector of type bytes
        byte[] typesArr = new byte[]{ BasicSqlDataFilterUnion.BasicSqlDataFilter };
        int typesVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, typesArr);

        int[] valsArr = new int[]{ filterOff };
        int valsVec = BasicSqlDataFilterWrapper.createFiltersVector(b, valsArr);

        // ----- wrapper (AND) with one child filter -----
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b);
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and);
        BasicSqlDataFilterWrapper.addFiltersType(b, typesVec);
        BasicSqlDataFilterWrapper.addFilters(b, valsVec);
        int wrapperOff = BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b);

        b.finish(wrapperOff);
        return b.sizedByteArray();
    }
}

-------- [ Separator ] ------

File Name: src/test/java/org/cladbe/cdc/TestFlatbuffers.java
Size: 2.12 KB
Code:
package org.cladbe.cdc;

import com.google.flatbuffers.FlatBufferBuilder;
import SqlSchema.*;

public final class TestFlatbuffers {

    private TestFlatbuffers() {}

    /** Build a BasicSqlDataFilterWrapper root with one BasicSqlDataFilter(contains) */
    public static byte[] buildContainsFilter(String field, String value, boolean ci) {
        FlatBufferBuilder fbb = new FlatBufferBuilder(128);

        // value: union (StringValue)
        int sv = StringValue.createStringValue(fbb, fbb.createString(value));

        // optional modifier
        int mod = 0;
        if (ci) {
            SqlFilterModifier.startSqlFilterModifier(fbb);
            SqlFilterModifier.addCaseInsensitive(fbb, true);
            mod = SqlFilterModifier.endSqlFilterModifier(fbb);
        }

        // the basic filter
        int fieldName = fbb.createString(field);
        BasicSqlDataFilter.startBasicSqlDataFilter(fbb);
        BasicSqlDataFilter.addFieldName(fbb, fieldName);
        BasicSqlDataFilter.addFilterType(fbb, BasicSqlDataFilterType.contains);
        BasicSqlDataFilter.addValueType(fbb, FilterValue.StringValue); // union discriminator
        BasicSqlDataFilter.addValue(fbb, sv);                           // union payload
        if (ci) BasicSqlDataFilter.addModifier(fbb, mod);
        int basic = BasicSqlDataFilter.endBasicSqlDataFilter(fbb);

        // wrapper (vector<union>)
        int filtersVec = BasicSqlDataFilterWrapper.createFiltersVector(
                fbb, new int[]{ basic });
        int typesVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(
                fbb, new byte[]{ BasicSqlDataFilterUnion.BasicSqlDataFilter });

        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(fbb);
        BasicSqlDataFilterWrapper.addFilterWrapperType(fbb, SQLFilterWrapperType.and);
        BasicSqlDataFilterWrapper.addFiltersType(fbb, typesVec);
        BasicSqlDataFilterWrapper.addFilters(fbb, filtersVec);
        int wrapper = BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(fbb);

        // ✅ your app expects wrapper as the root object
        fbb.finish(wrapper);
        return fbb.sizedByteArray();
    }
}

-------- [ Separator ] ------
