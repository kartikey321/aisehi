Project Overview
===============

Project Statistics:
Total Files: 64
Total Size: 1.26 MB

File Types:
  .kt: 27 files
  .bin: 12 files
  .lock: 7 files
  .properties: 6 files
  no extension: 3 files
  .kts: 2 files
  .fbs: 2 files
  .probe: 1 files
  .md: 1 files
  .jar: 1 files
  .bat: 1 files
  .xml: 1 files

Detected Technologies:

Folder Structure (Tree)
=====================
Legend: ✓ = Included in output, ✗ = Excluded from output

├── .gitignore (514 B) ✓
├── .gradle/
│   ├── 8.14/
│   │   ├── checksums/
│   │   │   ├── checksums.lock (17 B) ✓
│   │   │   ├── md5-checksums.bin (30.76 KB) ✓
│   │   │   └── sha1-checksums.bin (80.81 KB) ✓
│   │   ├── executionHistory/
│   │   │   ├── executionHistory.bin (19.21 KB) ✓
│   │   │   └── executionHistory.lock (17 B) ✓
│   │   ├── fileChanges/
│   │   │   └── last-build.bin (1 B) ✓
│   │   ├── fileHashes/
│   │   │   ├── fileHashes.bin (18.26 KB) ✓
│   │   │   └── fileHashes.lock (17 B) ✓
│   │   └── gc.properties (0 B) ✓
│   ├── 8.7/
│   │   ├── checksums/
│   │   │   ├── checksums.lock (17 B) ✓
│   │   │   ├── md5-checksums.bin (42.62 KB) ✓
│   │   │   └── sha1-checksums.bin (137.28 KB) ✓
│   │   ├── dependencies-accessors/
│   │   │   └── gc.properties (0 B) ✓
│   │   ├── executionHistory/
│   │   │   ├── executionHistory.bin (663.80 KB) ✓
│   │   │   └── executionHistory.lock (17 B) ✓
│   │   ├── expanded/
│   │   ├── fileChanges/
│   │   │   └── last-build.bin (1 B) ✓
│   │   ├── fileHashes/
│   │   │   ├── fileHashes.bin (84.63 KB) ✓
│   │   │   ├── fileHashes.lock (17 B) ✓
│   │   │   └── resourceHashesCache.bin (23.21 KB) ✓
│   │   ├── gc.properties (0 B) ✓
│   │   └── vcsMetadata/
│   ├── buildOutputCleanup/
│   │   ├── buildOutputCleanup.lock (17 B) ✓
│   │   ├── cache.properties (50 B) ✓
│   │   └── outputFiles.bin (19.80 KB) ✓
│   ├── file-system.probe (8 B) ✓
│   └── vcs-1/
│       └── gc.properties (0 B) ✓
├── .kotlin/
│   └── sessions/
├── ARCHITECTURE.md (6.55 KB) ✓
├── Dockerfile (906 B) ✓
├── build.gradle.kts (4.58 KB) ✓
├── gradle/
│   └── wrapper/
│       ├── gradle-wrapper.jar (42.43 KB) ✓
│       └── gradle-wrapper.properties (250 B) ✓
├── gradlew (8.49 KB) ✓
├── gradlew.bat (2.85 KB) ✓
├── settings.gradle.kts (42 B) ✓
└── src/
    ├── main/
    │   ├── flatbuffers/
    │   │   ├── sql_rpc.fbs (2.73 KB) ✓
    │   │   └── sql_schema.fbs (6.28 KB) ✓
    │   ├── kotlin/
    │   │   └── org/
    │   │       └── cladbe/
    │   │           ├── cdc/
    │   │           │   ├── Evaluator/
    │   │           │   │   ├── DerivedSpec.kt (1.00 KB) ✓
    │   │           │   │   ├── DiffOps.kt (3.12 KB) ✓
    │   │           │   │   ├── FbMappings.kt (3.00 KB) ✓
    │   │           │   │   ├── FbPredicate.kt (7.43 KB) ✓
    │   │           │   │   ├── FbQueryAdapter.kt (213 B) ✓
    │   │           │   │   ├── FbQueryAdapterImpl.kt (1.04 KB) ✓
    │   │           │   │   ├── FilterFieldExtractor.kt (950 B) ✓
    │   │           │   │   ├── OrderSpec.kt (1.18 KB) ✓
    │   │           │   │   ├── PageOps.kt (3.07 KB) ✓
    │   │           │   │   ├── QueryEngine.kt (9.54 KB) ✓
    │   │           │   │   └── SqlComparators.kt (6.07 KB) ✓
    │   │           │   └── engine/
    │   │           │       ├── App.kt (1.66 KB) ✓
    │   │           │       ├── BackfillClient.kt (4.34 KB) ✓
    │   │           │       ├── BackfillReplyProcessor.kt (2.28 KB) ✓
    │   │           │       ├── CdcProcessor.kt (14.39 KB) ✓
    │   │           │       ├── FbRequests.kt (13.31 KB) ✓
    │   │           │       ├── FbResponses.kt (636 B) ✓
    │   │           │       ├── KeyDbWriter.kt (3.35 KB) ✓
    │   │           │       ├── PkUtils.kt (207 B) ✓
    │   │           │       ├── QueryControlProcessor.kt (1.30 KB) ✓
    │   │           │       ├── SeedProcessor.kt (2.39 KB) ✓
    │   │           │       ├── TablePkRegistry.kt (926 B) ✓
    │   │           │       └── Topology.kt (5.05 KB) ✓
    │   │           └── tools/
    │   │               └── SendQctl.kt (2.62 KB) ✓
    │   └── resources/
    │       └── logback.xml (274 B) ✓
    └── test/
        └── kotlin/
            └── org/
                └── cladbe/
                    └── cdc/
                        └── Evaluator/
                            ├── DiffOpsTest.kt (2.47 KB) ✓
                            ├── PageOpsTest.kt (2.47 KB) ✓
                            └── QueryEngineSmokeTest.kt (3.28 KB) ✓

==============

File Name: .gitignore
Size: 514 B
Code:
target/
!.mvn/wrapper/maven-wrapper.jar
!**/src/main/**/target/
!**/src/test/**/target/

### IntelliJ IDEA ###
.idea/modules.xml
.idea/jarRepositories.xml
.idea/compiler.xml
.idea/libraries/
*.iws
*.iml
*.ipr

### Eclipse ###
.apt_generated
.classpath
.factorypath
.project
.settings
.springBeans
.sts4-cache

### NetBeans ###
/nbproject/private/
/nbbuild/
/dist/
/nbdist/
/.nb-gradle/
build/
!**/src/main/**/build/
!**/src/test/**/build/

### VS Code ###
.vscode/

### Mac OS ###
.DS_Store
.idea/
.mvm
.idea/
.mvn
-------- [ Separator ] ------

File Name: .gradle/8.14/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/8.7/dependencies-accessors/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/8.7/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: .gradle/buildOutputCleanup/cache.properties
Size: 50 B
Code:
#Wed Sep 03 12:07:56 CEST 2025
gradle.version=8.7

-------- [ Separator ] ------

File Name: .gradle/vcs-1/gc.properties
Size: 0 B
Code:

-------- [ Separator ] ------

File Name: ARCHITECTURE.md
Size: 6.55 KB
Code:
# Kafka Streams CDC Engine — Mental Model

This document summarizes the core topology, control plane, data flow, and operational notes for the Kafka Streams CDC engine used in this project.

## A. Topology & State Stores

- **Build Entry**: `StreamsTopology.buildForTable` wires everything.
- **Sources**:
  - CDC topic (raw JSON rows)
  - Query control topic (FB `StreamingSqlDataFilter`)
  - Optional seed topic (JSON snapshot from gateway)
  - SQL-RPC replies topic (FB `ResponseEnvelope`)
  - Internal backfill topic (rows fetched from RPC)
- **Processors**:
  - `QueryControlProcessor` — registers/unregisters queries
  - `CdcProcessor` — main engine: upserts rows, diffs, backfill triggers, flush
  - `SeedProcessor` — warms caches & page from snapshot
  - `BackfillReplyProcessor` — handles DB fetch replies, warms row cache, forwards to backfill topic
- **Sink**: outbox diffs to `WS_OUTBOX_TOPIC`
- **State stores (RocksDB)**: `qmeta`, `page`, `outbox`, `rowCache`, `pending`

## B. Registering a Live Query (Control Plane)

- **Input**: `server.query.control` with key `"$table|$hashId"` and value FlatBuffer `StreamingSqlDataFilter` (order, limit K, wrapper filters, optional cursor).
- **`QueryControlProcessor`**:
  - Stores FB bytes in `qmeta[$table|$hashId]`
  - Ensures `page` and `outbox` are initialized
  - Later, `CdcProcessor` will lazily build a `QueryEngine` per `hashId`.

## C. Ingesting Rows & Producing Diffs

- `CdcProcessor.process` fires on each CDC row (and also on backfill rows):
  1. Parse JSON to `Map<String, Any?>`; extract `__lsn` if present (used for headers & cache metadata).
  2. Resolve PK column via `TablePkRegistry` (e.g., `"a_notes" → "id"`). Store raw row bytes in `rowCache[pk]` (for comparator/engine lookups).
  3. For each registered query (qmeta entries for this table):
     - Build/reuse a `QueryEngine<String>` bound to that query:
       - `FbQueryAdapterImpl.parse` converts FB → `DerivedSpec`
       - `toOrderSpec` builds order keys (+ auto-append PK if missing)
       - `whereMatches` uses `FbPredicate` to evaluate SQL-like filters (equals, <, inList, contains, regex, isNull, etc.)
       - `filterFields` lists fields referenced in WHERE (to detect if an UPDATE could change match-ness)
       - `SqlComparators.pkComparator` gives a total order (ASC/DESC + explicit NULLS FIRST/LAST, temporal normalization) using rowCache lookups.
       - `QueryEngine.upsert(row)` decides:
         - not matching? → remove if currently in page (emit removed)
         - new & better than tail? → insert and evict tail (emit added + removed)
         - updated in page?
           - if order fields changed → reposition (emit modified with from→to)
           - else payload-only → modified in-place
       - It sets two intent flags: `needsBackfill` and `probeTail`.
- **`CdcProcessor` updates**:
  - `page[$table|$hashId]` with compact CSV PK list
  - `outbox[$table|$hashId]` with a compact string batch of diffs
  - Tracks “dirty” queries so snapshots/range indexes get refreshed
  - If backfill/probe is needed → send SQL-RPC request (see D).
- **Adaptive flush (`CdcProcessor`)**:
  - On a short schedule (starts ~200ms, adapts): flushes pending outbox batches to `WS_OUTBOX_TOPIC` (key=`hashId`, value=batch bytes, header `lsn` if known).
  - Also writes to KeyDB via `KeyDbWriter`:
    - `appendDiff(hashId, lsn, base64(batch))`
    - if the query was marked dirty: `writeRangeIndex(hashId, pkOrder)` and `writeSnapshot(hashId, rows, lsn)`
  - State resets for next batch.

## D. Backfill Round-Trip (Keyset Pagination)

- When `needsBackfill` or `probeTail` is set:
  - `CdcProcessor.triggerBackfill`:
    - `engine.currentCursor()` returns the last tuple by the query’s ORDER BY (e.g., `{score: 10, id: 2}`).
    - Build a SQL-RPC `GetDataReq` using FlatBuffers builders (`FbRequests.buildGetDataFromQueryFb`):
      - Compose wrapper: `AND(userWrapper, strictlyAfter(cursorTuple))`
      - `limit = 1`, `strictlyAfter = true`
    - Generate a correlation id `corr`, remember `pending[corr] = "$table|$hashId"`, and produce to `sql.rpc.requests`.
    - Your external `postgres_rpc` worker responds on `sql.rpc.responses` with a `ResponseEnvelope` → `RowsJson`.
  - `BackfillReplyProcessor`:
    - Match `corr` → get `"$table|$hashId"`
    - Parse the first row JSON (if present)
    - Warm `rowCache` and forward the raw row JSON bytes to the internal backfill topic (`BACKFILL_ROWS_TOPIC`) where `CdcProcessor` also listens
    - Delete `pending[corr]`
    - The forwarded row is then treated as if it arrived from CDC, which lets the engine insert it and emit diffs.

## E. Seeding (Optional Happy Path)

- `SeedProcessor` consumes `server.page.seed` JSON from your gateway, e.g.:

  ```json
  {"table":"acme_a_notes","hashId":"H1","rows":[...]}
  ```

- Warms `rowCache` for each row, sets `page["acme_a_notes|H1"]` to ordered PKs, and writes a KeyDB snapshot + range index so the gateway can read a full page immediately.

## F. Ordering, Diffs, and Correctness

- **Ordering (`SqlComparators`)** implements SQL-like sorting over mixed types:
  - Numeric coercion via `BigDecimal`
  - Strings, booleans, class-safe `Comparable`
  - Null semantics: explicit FIRST/LAST or defaults (ASC → LAST, DESC → FIRST)
  - Timestamps normalized (Instant, millis, micros, nanos) with unit hints
- **PageOps + DiffOps** implement the minimal Top-K page edits:
  - Stable `insertIfBetter` (with tail eviction)
  - `repositionIfNeeded` on order change
  - Emits Added, Removed, Modified (and encodes to your wire `OutboxCodec.WireDiff`)
- **`QueryEngine`** ties it together per query: filtering, ordering, page mutations, backfill intent, cursor.

---

## What Topics Do I Need?

- **Minimum for a basic run**:
  - Input CDC: `cdc.acme_a_notes` (or your actual topic)
  - Control plane: `server.query.control` (FlatBuffer queries)
  - Output diffs: `server.cdc.filtered`
- **If you seed pages**: `server.page.seed`
- **If you test backfill**: `sql.rpc.requests`, `sql.rpc.responses`, and `server.backfill.rows` (internal)

For dev, let auto-creation happen. For prod, pre-create with your desired RF/retention and compaction where appropriate (state changelog topics are created by Streams automatically).

---

## Common Pitfalls (Quick Checklist)

- Inside Docker, always use `kafka:9092` (container listener), not `localhost:29092`.
- Ensure unique `STREAMS_APP_ID` per table/app instance.
- Mount `/tmp/kafka-streams` so RocksDB state survives container restarts.
- If you disable KeyDB, remove/blank `KEYDB_URL`.
- If you don’t run the SQL-RPC worker yet, backfill just won’t return anything—normal for dev.


-------- [ Separator ] ------

File Name: Dockerfile
Size: 906 B
Code:
# ---- build stage ----
FROM maven:3.9.6-eclipse-temurin-17 AS build
WORKDIR /app

# flatc install (keep your working snippet)
RUN apt-get update && apt-get install -y wget unzip && rm -rf /var/lib/apt/lists/*
RUN wget  https://github.com/google/flatbuffers/releases/download/v25.2.10/Linux.flatc.binary.g++-13.zip \
 && unzip  Linux.flatc.binary.g++-13.zip -d /usr/local/bin \
 && chmod +x /usr/local/bin/flatc && rm Linux.flatc.binary.g++-13.zip

# Cache layer: only pom.xml → resolves deps once and caches them
COPY pom.xml .
RUN mvn -B  -DskipTests dependency:resolve dependency:resolve-plugins

# Now bring sources and compile
COPY src ./src
RUN mvn -B  -DskipTests -T 1C package

# ---- runtime stage ----
FROM eclipse-temurin:17-jre AS runtime
WORKDIR /opt/app
RUN useradd -ms /bin/bash appuser
COPY --from=build /app/target/*.jar app.jar
USER appuser
ENTRYPOINT ["java","-jar","/opt/app/app.jar"]
-------- [ Separator ] ------

File Name: build.gradle.kts
Size: 4.58 KB
Code:
plugins {
    kotlin("jvm") version "2.2.0"
    application
    id("com.github.johnrengelman.shadow") version "8.1.1" // like Maven Shade
}

repositories {
    mavenCentral()
    maven { url = uri("https://packages.confluent.io/maven/") } // matches your <repositories>
}

val flatbuffersVersion = "25.2.10"
val kafkaStreamsVersion = "4.0.0"
val confluentVersion = "7.7.1"
val slf4jVersion = "2.0.13"
val logbackVersion = "1.5.13"
val avroVersion = "1.11.4"
val jacksonVersion = "2.17.1"
val commonsLang3Version = "3.18.0"
val jedisVersion = "5.1.3"
val kotlinVersionPin = "2.2.0"
val junitJupiterVersion = "5.10.2"

dependencies {
    // Kafka Streams
    implementation("org.apache.kafka:kafka-streams:$kafkaStreamsVersion")

    // Confluent Avro SerDes (you had them in pom)
    implementation("io.confluent:kafka-streams-avro-serde:$confluentVersion")
    implementation("io.confluent:kafka-avro-serializer:$confluentVersion")

    // Logging — same stack as pom (logback + slf4j)
    implementation("org.slf4j:slf4j-api:$slf4jVersion")
    implementation("ch.qos.logback:logback-classic:$logbackVersion")
    implementation("ch.qos.logback:logback-core:$logbackVersion")

    // Data formats / utils
    implementation("org.apache.avro:avro:$avroVersion")
    implementation("com.fasterxml.jackson.core:jackson-databind:$jacksonVersion")
    implementation("com.fasterxml.jackson.core:jackson-core:$jacksonVersion")
    implementation("com.fasterxml.jackson.core:jackson-annotations:$jacksonVersion")
    implementation("org.apache.commons:commons-lang3:$commonsLang3Version")

    // FlatBuffers runtime
    implementation("com.google.flatbuffers:flatbuffers-java:$flatbuffersVersion")

    // Redis / KeyDB
    implementation("redis.clients:jedis:$jedisVersion")

    // Kotlin stdlib
    implementation("org.jetbrains.kotlin:kotlin-stdlib-jdk8:$kotlinVersionPin")

    // Tests (match pom)
    testImplementation("org.apache.kafka:kafka-streams-test-utils:$kafkaStreamsVersion")
    testImplementation("org.jetbrains.kotlin:kotlin-test:$kotlinVersionPin")
    testImplementation("org.jetbrains.kotlin:kotlin-test-junit5:$kotlinVersionPin")
    testImplementation("io.mockk:mockk:1.13.10")
    testImplementation("io.mockk:mockk-agent-jvm:1.13.10")
    testImplementation("org.junit.jupiter:junit-jupiter-api:$junitJupiterVersion")
    testRuntimeOnly("org.junit.jupiter:junit-jupiter-engine:$junitJupiterVersion")
}

kotlin {
    jvmToolchain(17) // <source>/<target> 17 as in pom
}

application {
    mainClass.set("org.cladbe.cdc.engine.AppKt") // matches your pom <mainClass>
}

tasks.test {
    useJUnitPlatform()
}

/**
 * -------- FlatBuffers codegen (parity with exec-maven-plugin) --------
 * Generates Java sources from:
 *   src/main/flatbuffers/sql_schema.fbs
 *   src/main/flatbuffers/sql_rpc.fbs
 * into build/generated-src/flatbuffers/java
 */
val flatcOutputDir = layout.buildDirectory.dir("generated-src/flatbuffers/java")

val flatcGenerate by tasks.registering(Exec::class) {
    group = "codegen"
    description = "Run flatc to generate Java classes from .fbs schemas"
    // Ensure output dir exists
    doFirst { flatcOutputDir.get().asFile.mkdirs() }
    commandLine(
        "flatc",
        "--java",
        "-o", flatcOutputDir.get().asFile.absolutePath,
        "-I", "$projectDir/src/main/flatbuffers",
        "$projectDir/src/main/flatbuffers/sql_schema.fbs",
        "$projectDir/src/main/flatbuffers/sql_rpc.fbs"
    )
    // Rerun when schemas change
    inputs.files(
        file("$projectDir/src/main/flatbuffers/sql_schema.fbs"),
        file("$projectDir/src/main/flatbuffers/sql_rpc.fbs")
    )
    outputs.dir(flatcOutputDir)
}

// Include generated sources in main compilation (parity with build-helper-maven-plugin)
sourceSets {
    named("main") {
        java.srcDir(flatcOutputDir)
        java.srcDir("src/main/java")
        kotlin.srcDir("src/main/kotlin")
    }
    named("test") {
        java.srcDir("src/test/java")
        kotlin.srcDir("src/test/kotlin")
    }
}

// Make Kotlin & Java compilation depend on flatc codegen
tasks.withType<org.jetbrains.kotlin.gradle.tasks.KotlinCompile> {
    dependsOn(flatcGenerate)
}
tasks.withType<JavaCompile> {
    dependsOn(flatcGenerate)
}

/**
 * -------- Packaging (Shadow = Maven Shade) --------
 * Produces an uber-jar at build/libs/kafka-streams-project-all.jar
 * with Main-Class set, same as your Maven Shade config.
 */
tasks.named<com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar>("shadowJar") {
    archiveBaseName.set("kafka-streams-project")
    archiveClassifier.set("all")
    manifest {
        attributes["Main-Class"] = "org.cladbe.cdc.engine.AppKt"
    }
}
-------- [ Separator ] ------

File Name: gradle/wrapper/gradle-wrapper.properties
Size: 250 B
Code:
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.7-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists

-------- [ Separator ] ------

File Name: gradlew
Size: 8.49 KB
Code:
#!/bin/sh

#
# Copyright © 2015-2021 the original authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

##############################################################################
#
#   Gradle start up script for POSIX generated by Gradle.
#
#   Important for running:
#
#   (1) You need a POSIX-compliant shell to run this script. If your /bin/sh is
#       noncompliant, but you have some other compliant shell such as ksh or
#       bash, then to run this script, type that shell name before the whole
#       command line, like:
#
#           ksh Gradle
#
#       Busybox and similar reduced shells will NOT work, because this script
#       requires all of these POSIX shell features:
#         * functions;
#         * expansions «$var», «${var}», «${var:-default}», «${var+SET}»,
#           «${var#prefix}», «${var%suffix}», and «$( cmd )»;
#         * compound commands having a testable exit status, especially «case»;
#         * various built-in commands including «command», «set», and «ulimit».
#
#   Important for patching:
#
#   (2) This script targets any POSIX shell, so it avoids extensions provided
#       by Bash, Ksh, etc; in particular arrays are avoided.
#
#       The "traditional" practice of packing multiple parameters into a
#       space-separated string is a well documented source of bugs and security
#       problems, so this is (mostly) avoided, by progressively accumulating
#       options in "$@", and eventually passing that to Java.
#
#       Where the inherited environment variables (DEFAULT_JVM_OPTS, JAVA_OPTS,
#       and GRADLE_OPTS) rely on word-splitting, this is performed explicitly;
#       see the in-line comments for details.
#
#       There are tweaks for specific operating systems such as AIX, CygWin,
#       Darwin, MinGW, and NonStop.
#
#   (3) This script is generated from the Groovy template
#       https://github.com/gradle/gradle/blob/HEAD/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt
#       within the Gradle project.
#
#       You can find Gradle at https://github.com/gradle/gradle/.
#
##############################################################################

# Attempt to set APP_HOME

# Resolve links: $0 may be a link
app_path=$0

# Need this for daisy-chained symlinks.
while
    APP_HOME=${app_path%"${app_path##*/}"}  # leaves a trailing /; empty if no leading path
    [ -h "$app_path" ]
do
    ls=$( ls -ld "$app_path" )
    link=${ls#*' -> '}
    case $link in             #(
      /*)   app_path=$link ;; #(
      *)    app_path=$APP_HOME$link ;;
    esac
done

# This is normally unused
# shellcheck disable=SC2034
APP_BASE_NAME=${0##*/}
# Discard cd standard output in case $CDPATH is set (https://github.com/gradle/gradle/issues/25036)
APP_HOME=$( cd "${APP_HOME:-./}" > /dev/null && pwd -P ) || exit

# Use the maximum available, or set MAX_FD != -1 to use that value.
MAX_FD=maximum

warn () {
    echo "$*"
} >&2

die () {
    echo
    echo "$*"
    echo
    exit 1
} >&2

# OS specific support (must be 'true' or 'false').
cygwin=false
msys=false
darwin=false
nonstop=false
case "$( uname )" in                #(
  CYGWIN* )         cygwin=true  ;; #(
  Darwin* )         darwin=true  ;; #(
  MSYS* | MINGW* )  msys=true    ;; #(
  NONSTOP* )        nonstop=true ;;
esac

CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar


# Determine the Java command to use to start the JVM.
if [ -n "$JAVA_HOME" ] ; then
    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
        # IBM's JDK on AIX uses strange locations for the executables
        JAVACMD=$JAVA_HOME/jre/sh/java
    else
        JAVACMD=$JAVA_HOME/bin/java
    fi
    if [ ! -x "$JAVACMD" ] ; then
        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
else
    JAVACMD=java
    if ! command -v java >/dev/null 2>&1
    then
        die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
fi

# Increase the maximum file descriptors if we can.
if ! "$cygwin" && ! "$darwin" && ! "$nonstop" ; then
    case $MAX_FD in #(
      max*)
        # In POSIX sh, ulimit -H is undefined. That's why the result is checked to see if it worked.
        # shellcheck disable=SC2039,SC3045
        MAX_FD=$( ulimit -H -n ) ||
            warn "Could not query maximum file descriptor limit"
    esac
    case $MAX_FD in  #(
      '' | soft) :;; #(
      *)
        # In POSIX sh, ulimit -n is undefined. That's why the result is checked to see if it worked.
        # shellcheck disable=SC2039,SC3045
        ulimit -n "$MAX_FD" ||
            warn "Could not set maximum file descriptor limit to $MAX_FD"
    esac
fi

# Collect all arguments for the java command, stacking in reverse order:
#   * args from the command line
#   * the main class name
#   * -classpath
#   * -D...appname settings
#   * --module-path (only if needed)
#   * DEFAULT_JVM_OPTS, JAVA_OPTS, and GRADLE_OPTS environment variables.

# For Cygwin or MSYS, switch paths to Windows format before running java
if "$cygwin" || "$msys" ; then
    APP_HOME=$( cygpath --path --mixed "$APP_HOME" )
    CLASSPATH=$( cygpath --path --mixed "$CLASSPATH" )

    JAVACMD=$( cygpath --unix "$JAVACMD" )

    # Now convert the arguments - kludge to limit ourselves to /bin/sh
    for arg do
        if
            case $arg in                                #(
              -*)   false ;;                            # don't mess with options #(
              /?*)  t=${arg#/} t=/${t%%/*}              # looks like a POSIX filepath
                    [ -e "$t" ] ;;                      #(
              *)    false ;;
            esac
        then
            arg=$( cygpath --path --ignore --mixed "$arg" )
        fi
        # Roll the args list around exactly as many times as the number of
        # args, so each arg winds up back in the position where it started, but
        # possibly modified.
        #
        # NB: a `for` loop captures its iteration list before it begins, so
        # changing the positional parameters here affects neither the number of
        # iterations, nor the values presented in `arg`.
        shift                   # remove old arg
        set -- "$@" "$arg"      # push replacement arg
    done
fi


# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
DEFAULT_JVM_OPTS='"-Xmx64m" "-Xms64m"'

# Collect all arguments for the java command:
#   * DEFAULT_JVM_OPTS, JAVA_OPTS, JAVA_OPTS, and optsEnvironmentVar are not allowed to contain shell fragments,
#     and any embedded shellness will be escaped.
#   * For example: A user cannot expect ${Hostname} to be expanded, as it is an environment variable and will be
#     treated as '${Hostname}' itself on the command line.

set -- \
        "-Dorg.gradle.appname=$APP_BASE_NAME" \
        -classpath "$CLASSPATH" \
        org.gradle.wrapper.GradleWrapperMain \
        "$@"

# Stop when "xargs" is not available.
if ! command -v xargs >/dev/null 2>&1
then
    die "xargs is not available"
fi

# Use "xargs" to parse quoted args.
#
# With -n1 it outputs one arg per line, with the quotes and backslashes removed.
#
# In Bash we could simply go:
#
#   readarray ARGS < <( xargs -n1 <<<"$var" ) &&
#   set -- "${ARGS[@]}" "$@"
#
# but POSIX shell has neither arrays nor command substitution, so instead we
# post-process each arg (as a line of input to sed) to backslash-escape any
# character that might be a shell metacharacter, then use eval to reverse
# that process (while maintaining the separation between arguments), and wrap
# the whole thing up as a single "set" statement.
#
# This will of course break if any of these variables contains a newline or
# an unmatched quote.
#

eval "set -- $(
        printf '%s\n' "$DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS" |
        xargs -n1 |
        sed ' s~[^-[:alnum:]+,./:=@_]~\\&~g; ' |
        tr '\n' ' '
    )" '"$@"'

exec "$JAVACMD" "$@"

-------- [ Separator ] ------

File Name: gradlew.bat
Size: 2.85 KB
Code:
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%"=="" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%"=="" set DIRNAME=.
@rem This is normally unused
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if %ERRORLEVEL% equ 0 goto execute

echo. 1>&2
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH. 1>&2
echo. 1>&2
echo Please set the JAVA_HOME variable in your environment to match the 1>&2
echo location of your Java installation. 1>&2

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo. 1>&2
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME% 1>&2
echo. 1>&2
echo Please set the JAVA_HOME variable in your environment to match the 1>&2
echo location of your Java installation. 1>&2

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar


@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if %ERRORLEVEL% equ 0 goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
set EXIT_CODE=%ERRORLEVEL%
if %EXIT_CODE% equ 0 set EXIT_CODE=1
if not ""=="%GRADLE_EXIT_CONSOLE%" exit %EXIT_CODE%
exit /b %EXIT_CODE%

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega

-------- [ Separator ] ------

File Name: settings.gradle.kts
Size: 42 B
Code:
rootProject.name = "kafka-streams-project"
-------- [ Separator ] ------

File Name: src/main/flatbuffers/sql_rpc.fbs
Size: 2.73 KB
Code:
include "sql_schema.fbs";

namespace SqlRpc;

// ---------- RPC Method ----------
enum RpcMethod : ubyte {
  GET_DATA         = 0,
  GET_SINGLE       = 1,
  ADD_SINGLE       = 2,
  UPDATE_SINGLE    = 3,
  DELETE_ROW       = 4,
  CREATE_TABLE     = 5,
  TABLE_EXISTS     = 6,
  RUN_AGGREGATION  = 7
}

// ---------- Requests ----------
table GetDataReq {
  company_id: string;
  table_name: string;
  wrapper: SqlSchema.BasicSqlDataFilterWrapper; // optional
  limit: uint32 = 0;                             // 0 → no limit
  offset: uint32 = 0;                            // legacy; prefer cursor
  order: [SqlSchema.OrderKeySpec];               // optional
  cursor: [SqlSchema.CursorEntry];               // keyset cursor
  strict_after: bool = true;                     // default true
}

table GetSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table AddSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  row_json: string; // JSON row payload
}

table UpdateSingleReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
  updates_json: string; // JSON partial
}

table DeleteRowReq {
  company_id: string;
  table_name: string;
  primary_key_column: string;
  primary_id: string;
}

table CreateTableReq {
  company_id: string;
  definition: SqlSchema.TableDefinition;
}

table TableExistsReq {
  company_id: string;
  table_name: string;
}

table RunAggregationReq {
  company_id: string;
  table_name: string;
  count_enabled: bool = false;
  sum_fields: [string];
  average_fields: [string];
  minimum_fields: [string];
  maximum_fields: [string];
  wrapper: SqlSchema.BasicSqlDataFilterWrapper;
}

// ---------- Responses ----------
table RowsJson { rows: [string]; }           // unchanged
table RowJson  { row: string; }
table BoolRes  { value: bool; }
table AggRes   { agg: SqlSchema.DataHelperAggregation; }

// NEW: rows + cursor (e.g. WAL LSN) in one message
table RowsWithCursor {
  rows: [string];
  cursor: [SqlSchema.CursorEntry];
}

enum ErrorCode : ubyte { NONE = 0, BAD_REQUEST = 1, INTERNAL = 2 }

// Single union definition including the new type
union RpcResponse {
  RowsJson,
  RowJson,
  BoolRes,
  AggRes,
  RowsWithCursor
}

table ResponseEnvelope {
  correlation_id: string;
  ok: bool = true;
  error_code: ErrorCode = NONE;
  error_message: string;
  data: RpcResponse;
}

// ---------- Envelope ----------
union RpcPayload {
  GetDataReq,
  GetSingleReq,
  AddSingleReq,
  UpdateSingleReq,
  DeleteRowReq,
  CreateTableReq,
  TableExistsReq,
  RunAggregationReq
}

table RequestEnvelope {
  correlation_id: string;
  reply_topic: string;
  method: RpcMethod;
  payload: RpcPayload;
}

root_type RequestEnvelope;
-------- [ Separator ] ------

File Name: src/main/flatbuffers/sql_schema.fbs
Size: 6.28 KB
Code:
namespace SqlSchema;

// ---------------- Enums with explicit numbers ----------------

// Stable: text/numeric types
enum SQLDataType : ubyte {
  text             = 0,
  varchar          = 1,
  char_            = 2,
  varcharArray     = 3,
  textArray        = 4,
  charArray        = 5,
  integer          = 6,
  bigInt           = 7,
  smallInt         = 8,
  decimal          = 9,
  numeric          = 10,
  real             = 11,
  doublePrecision  = 12,
  serial           = 13,
  bigSerial        = 14,
  smallSerial      = 15,
  money            = 16,
  date             = 17,
  time             = 18,
  timestamp        = 19,
  timestamptz      = 20,
  interval         = 21,
  timetz           = 22,
  boolean_         = 23,
  bytea            = 24,
  json             = 25,
  jsonb            = 26,
  jsonArray        = 27,
  jsonbArray       = 28,
  uuid             = 29,
  xml              = 30,
  array            = 31,
  custom           = 32
}

enum ColumnConstraint : ubyte {
  primaryKey        = 0,
  unique            = 1,
  notNull           = 2,
  check             = 3,
  default_          = 4,
  indexed           = 5,
  exclusion         = 6,
  generated         = 7,
  identity          = 8,
  references        = 9,
  noInherit         = 10,
  nullsNotDistinct  = 11
}

enum SQLFilterWrapperType : ubyte {
  or   = 0,
  and  = 1
}

enum NullsSortOrder : ubyte {
  first   = 0,
  last    = 1,
  default_= 2
}

enum TimeUnit : ubyte {
  SECONDS = 0,
  MILLIS  = 1,
  MICROS  = 2,
  NANOS   = 3
}

enum BasicSqlDataFilterType : ubyte {
  equals              = 0,
  notEquals           = 1,
  lessThan            = 2,
  lessThanOrEquals    = 3,
  greaterThan         = 4,
  greaterThanOrEquals = 5,

  isNull              = 6,
  isNotNull           = 7,

  regex               = 8,
  notRegex            = 9,
  startsWith          = 10,
  endsWith            = 11,
  contains            = 12,
  notContains         = 13,

  arrayContains       = 14,
  arrayContainedBy    = 15,
  arrayOverlaps       = 16,
  arrayEquals         = 17,
  arrayNotEquals      = 18,
  arrayEmpty          = 19,
  arrayNotEmpty       = 20,
  arrayLength         = 21,

  jsonContains        = 22,
  jsonContainedBy     = 23,
  jsonHasKey          = 24,
  jsonHasAnyKey       = 25,
  jsonHasAllKeys      = 26,
  jsonGetField        = 27,
  jsonGetFieldAsText  = 28,

  between             = 29,
  notBetween          = 30,
  rangeContains       = 31,
  rangeContainedBy    = 32,

  inList              = 33,
  notInList           = 34
}

enum OrderSort : ubyte {
  ASC_DEFAULT      = 0,
  ASC_NULLS_FIRST  = 1,
  ASC_NULLS_LAST   = 2,
  DESC_DEFAULT     = 3,
  DESC_NULLS_FIRST = 4,
  DESC_NULLS_LAST  = 5
}

// ---------------- Helper tables (unchanged) ----------------
table KeyValuePair { key: string; value: string; }
table CustomOptions { options: [KeyValuePair]; }
table TableOptions  { options: [KeyValuePair]; }

table DataHelperAggregation {
  count: uint32;
  sum_values: [KeyValuePair];
  avg_values: [KeyValuePair];
  minimum_values: [KeyValuePair];
  maximum_values: [KeyValuePair];
}

table DataSort { field: string; ascending: bool; }

// ---------------- Value tables ----------------
table StringValue   { value: string; }
table NumberValue   { value: double; }
table Int64Value    { value: long; }
table BoolValue     { value: bool; }
table NullValue     { }

table TimestampValue {
  epoch: long;
  unit:  TimeUnit = MICROS;
}

table StringList  { values:[string] (required); }
table Int64List   { values:[long]   (required); }
table Float64List { values:[double] (required); }
table BoolList    { values:[bool]   (required); }

// ---------------- Unions ----------------
union FilterValue {
  StringValue,
  NumberValue,
  BoolValue,
  NullValue,
  Int64Value,
  TimestampValue,
  StringList,
  Int64List,
  Float64List,
  BoolList
}

table RangeValue {
  low:  FilterValue;
  high: FilterValue;
  include_low:  bool = true;
  include_high: bool = true;
}

table CursorEntry { field:string (required); value:FilterValue; }

// ---------------- Filters ----------------
table SqlFilterModifier {
  distinct: bool;
  case_insensitive: bool;
  nulls_order: NullsSortOrder;
}

table BasicSqlDataFilter {
  field_name: string;
  value: FilterValue;
  filter_type: BasicSqlDataFilterType;
  modifier: SqlFilterModifier;
}

union BasicSqlDataFilterUnion { BasicSqlDataFilterWrapper, BasicSqlDataFilter }

table BasicSqlDataFilterWrapper {
  filter_wrapper_type: SQLFilterWrapperType;
  filters: [BasicSqlDataFilterUnion];
}

// ---------------- Query ----------------
table OrderKeySpec {
  field:string (required);
  sort:OrderSort = DESC_DEFAULT;
  is_pk:bool = false;
}

table StreamingSqlDataFilter {
  hash: string;
  wrapper: BasicSqlDataFilterWrapper;
  limit: uint32 = 50;
  order: [OrderKeySpec];
  cursor: [CursorEntry];
  schema_version: ushort = 1;
}

// ---------------- Table metadata ----------------
table TableColumn {
  name: string;
  data_type: SQLDataType;
  is_nullable: bool;
  constraints: [ColumnConstraint];
  custom_options: CustomOptions;
}

table TableDefinition {
  name: string;
  columns: [TableColumn];
  comment: string;
  table_options: TableOptions;
}

root_type StreamingSqlDataFilter;


// ---------------- Query spec for non-streaming RPC ----------------
// Same modeling as StreamingSqlDataFilter but:
// - no 'hash' (RPC doesn't need it)
// - adds 'strict_after' to mirror your keyset semantics
// - no schema_version (rpc payload versioning usually handled at the envelope)
table SqlQuerySpec {
  // WHERE / boolean-expression tree
  wrapper: BasicSqlDataFilterWrapper;

  // LIMIT
  limit: uint32 = 50;

  // ORDER BY (multi key). Use is_pk=true on keys that are part of the PK if needed.
  order: [OrderKeySpec];

  // KEYSET pagination: the last-seen tuple of ordered columns
  cursor: [CursorEntry];

  // Keyset semantics: when true, results must be strictly AFTER cursor
  // (equivalent to your "strictAfter" flag in manager)
  strict_after: bool = true;
}

// ---------------- Aggregation spec for RPC ----------------
table AggregationSpec {
  // SELECT COUNT(*)
  count_enabled: bool = false;

  // Aggregation over fields (server-side schema validates numeric types)
  sum_fields:      [string];
  average_fields:  [string];
  minimum_fields:  [string];
  maximum_fields:  [string];

  // Optional WHERE (reuse your wrapper)
  wrapper: BasicSqlDataFilterWrapper;
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/DerivedSpec.kt
Size: 1.00 KB
Code:
package org.cladbe.cdc.Evaluator

/**
 * Fully-derived spec the QueryEngine needs at runtime.
 *
 * - `order` MUST end with the PK key (isPk=true), giving a total order.
 * - `whereMatches` implements the user predicate with exact SQL semantics.
 * - `filterFields` includes every field referenced by WHERE; the engine uses it
 *   to detect when an UPDATE changes “match-ness” and whether to probe backfill.
 * - `k` is the page capacity.
 */
data class DerivedSpec(
    val table: String,
    val k: Int,
    val order: List<OrderKeySpec>,
    val whereMatches: (Map<String, Any?>) -> Boolean,
    val filterFields: Set<String>
) {
    init {
        require(order.isNotEmpty()) { "order must not be empty" }
        require(order.last().isPk) { "last order key must be the PK (isPk=true)" }
        require(order.count { it.isPk } == 1) { "exactly one order key must be marked isPk=true" }
    }

    /** Convenience: the PK field name from the trailing order key. */
    val pkField: String get() = order.last().field
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/DiffOps.kt
Size: 3.12 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

/** Minimal change model for your outbox. */
sealed interface Diff<PK> {
    val pk: PK
    data class Added<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    data class Removed<PK>(override val pk: PK, val pos: Int) : Diff<PK>
    /**
     * Modified: row stayed in the page but content and/or order position changed.
     * - If `from != null && from != pos`, the row moved (promotion/demotion).
     * - If `from == pos`, treat as in-place payload change.
     */
    data class Modified<PK>(override val pk: PK, val pos: Int, val from: Int? = null) : Diff<PK>
}

object DiffOps {

    /**
     * Insert a PK into a sorted Top-K page and emit diffs:
     * - ADDED(pos) when inserted.
     * - REMOVED(tailPos) if the tail was evicted.
     * - [] if nothing changed.
     */
    @JvmStatic
    fun <PK> insertAndDiff(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): List<Diff<PK>> {
        val diffs = mutableListOf<Diff<PK>>()
        val prevSize = page.size
        val res = PageOps.insertIfBetter(page, pk, k, cmp)

        if (res.inserted) {
            diffs += Diff.Added(pk, res.pos)
        }
        if (res.evicted != null) {
            // evicted was the previous tail
            val evictedPos = (prevSize - 1).coerceAtLeast(0)
            diffs += Diff.Removed(res.evicted, evictedPos)
        }
        return diffs
    }

    /**
     * Reposition a PK already in the page when its order fields changed.
     * Emits MODIFIED(newPos, from=oldPos) if moved, or MODIFIED(pos, from=pos) if payload-only change.
     * If the PK is not present, returns [] (call insertAndDiff instead).
     */
    @JvmStatic
    fun <PK> repositionAndDiff(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>,
        payloadChanged: Boolean = true
    ): List<Diff<PK>> {
        val res = PageOps.repositionIfNeeded(page, pk, cmp)
        return if (res.from >= 0) {
            val from = res.from
            val to = res.to
            // Always emit Modified if caller says payload changed, or if moved
            if (payloadChanged || res.moved) listOf(Diff.Modified(pk, to, from)) else emptyList()
        } else {
            emptyList()
        }
    }

    /**
     * Remove a PK if present and emit REMOVED(pos).
     */
    @JvmStatic
    fun <PK> removeAndDiff(
        page: MutableList<PK>,
        pk: PK
    ): List<Diff<PK>> {
        val idx = page.indexOf(pk)
        if (idx >= 0) {
            page.removeAt(idx)
            return listOf(Diff.Removed(pk, idx))
        }
        return emptyList()
    }
}
object OutboxCodec {
    data class WireDiff<PK>(val type: String, val pk: PK, val pos: Int? = null, val from: Int? = null)

    @JvmStatic
    fun <PK> toWire(diffs: List<Diff<PK>>): List<WireDiff<PK>> =
        diffs.map {
            when (it) {
                is Diff.Added   -> WireDiff("added",   it.pk, pos = it.pos)
                is Diff.Removed -> WireDiff("removed", it.pk, pos = it.pos)
                is Diff.Modified-> WireDiff("modified",it.pk, pos = it.pos, from = it.from)
            }
        }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbMappings.kt
Size: 3.00 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import SqlSchema.OrderKeySpec as FbOrderKeySpec
import SqlSchema.OrderSort as FbOrderSort
import SqlSchema.*

object FbMappings {

    @JvmStatic
    fun mapOrderSort(code: Int): Pair<Direction, Nulls> = when (code) {
        FbOrderSort.ASC_DEFAULT.toInt()      -> Direction.ASC  to Nulls.LAST
        FbOrderSort.ASC_NULLS_FIRST.toInt()  -> Direction.ASC  to Nulls.FIRST
        FbOrderSort.ASC_NULLS_LAST.toInt()   -> Direction.ASC  to Nulls.LAST
        FbOrderSort.DESC_DEFAULT.toInt()     -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_FIRST.toInt() -> Direction.DESC to Nulls.FIRST
        FbOrderSort.DESC_NULLS_LAST.toInt()  -> Direction.DESC to Nulls.LAST
        else                                 -> Direction.DESC to Nulls.FIRST
    }

    @JvmStatic
    fun toOrderSpec(q: StreamingSqlDataFilter, pkField: String, pkDir: Direction = Direction.DESC): List<OrderKeySpec> {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val list = buildList {
            for (i in 0 until q.orderLength()) {
                val ok: FbOrderKeySpec = q.order(i)!!
                val (dir, nulls) = mapOrderSort(ok.sort().toInt())
                add(if (ok.isPk()) OrderKeySpec.pk(ok.field(), dir, nulls) else OrderKeySpec.of(ok.field(), dir, nulls))
            }
        }

        val hasPk = list.any { it.isPk }
        val res =
            if (hasPk && list.last().isPk) list
            else if (hasPk) {
                val pk = list.first { it.isPk }
                list.filterNot { it.isPk } + pk
            } else list + OrderKeySpec.pk(pkField, pkDir)

        if (debug) println("[FbMappings] orderSpec=$res")
        return res
    }

    @JvmStatic
    fun toCursorTuple(q: StreamingSqlDataFilter): Map<String, Any?>? {
        if (q.cursorLength() == 0) return null
        val m = LinkedHashMap<String, Any?>(q.cursorLength())
        for (i in 0 until q.cursorLength()) {
            val ce = q.cursor(i)!!
            m[ce.field()] = readValue(ce.valueType().toInt(), ce.value(null))
        }
        return m
    }

    @JvmStatic
    fun readValue(t: Int, table: com.google.flatbuffers.Table?): Any? = when (t) {
        FilterValue.StringValue.toInt()    -> (table as StringValue).value()
        FilterValue.NumberValue.toInt()    -> (table as NumberValue).value()
        FilterValue.Int64Value.toInt()     -> (table as Int64Value).value()
        FilterValue.BoolValue.toInt()      -> (table as BoolValue).value()
        FilterValue.NullValue.toInt()      -> null
        FilterValue.TimestampValue.toInt() -> {
            val tv = table as TimestampValue
            val base = tv.epoch()
            when (tv.unit().toInt()) {
                TimeUnit.SECONDS.toInt() -> base * 1_000_000L
                TimeUnit.MILLIS.toInt()  -> base * 1_000L
                TimeUnit.MICROS.toInt()  -> base
                TimeUnit.NANOS.toInt()   -> base / 1_000L
                else -> base
            }
        }
        else -> null
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbPredicate.kt
Size: 7.43 KB
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FbPredicate {

    @JvmStatic
    fun evalWrapper(w: BasicSqlDataFilterWrapper?, row: Map<String, Any?>): Boolean {
        if (w == null) return true
        val isAnd = w.filterWrapperType() == SQLFilterWrapperType.and
        var acc = if (isAnd) true else false
        for (i in 0 until w.filtersLength()) {
            val t = w.filtersType(i).toInt() // ← normalize to Int
            val ok = when (t) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() ->
                    evalBasic(w.filters(null, i) as BasicSqlDataFilter, row)
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() ->
                    evalWrapper(w.filters(null, i) as BasicSqlDataFilterWrapper, row)
                else -> false
            }
            acc = if (isAnd) (acc && ok) else (acc || ok)
            if (isAnd && !acc) return false
            if (!isAnd && acc) return true
        }
        return acc
    }

    @JvmStatic
    fun evalBasic(f: BasicSqlDataFilter, row: Map<String, Any?>): Boolean {
        val field = f.fieldName()
        val got = row[field]
        val ci = f.modifier()?.caseInsensitive() == true

        return when (f.filterType().toInt()) { // ← normalize to Int
            BasicSqlDataFilterType.equals.toInt()              -> eq(got, f, ci)
            BasicSqlDataFilterType.notEquals.toInt()           -> !eq(got, f, ci)

            BasicSqlDataFilterType.lessThan.toInt()            -> cmp(got, f) { a, b -> a <  b }
            BasicSqlDataFilterType.lessThanOrEquals.toInt()    -> cmp(got, f) { a, b -> a <= b }
            BasicSqlDataFilterType.greaterThan.toInt()         -> cmp(got, f) { a, b -> a >  b }
            BasicSqlDataFilterType.greaterThanOrEquals.toInt() -> cmp(got, f) { a, b -> a >= b }

            BasicSqlDataFilterType.inList.toInt()              -> inList(got, f, ci)
            BasicSqlDataFilterType.notInList.toInt()           -> !inList(got, f, ci)

            BasicSqlDataFilterType.isNull.toInt()              -> got == null
            BasicSqlDataFilterType.isNotNull.toInt()           -> got != null

            BasicSqlDataFilterType.startsWith.toInt()          -> str(got)?.let { a -> startsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.endsWith.toInt()            -> str(got)?.let { a -> endsWith(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.contains.toInt()            -> str(got)?.let { a -> contains(a, strVal(f), ci) } == true
            BasicSqlDataFilterType.notContains.toInt()         -> str(got)?.let { a -> !contains(a, strVal(f), ci) } == true

            BasicSqlDataFilterType.regex.toInt()               -> regexMatch(str(got) ?: "", strVal(f), ci)
            BasicSqlDataFilterType.notRegex.toInt()            -> !regexMatch(str(got) ?: "", strVal(f), ci)

            else -> false
        }
    }

    private fun strVal(f: BasicSqlDataFilter): String =
        (f.value(StringValue()) as? StringValue)?.value() ?: ""

    private fun eq(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean = when (f.valueType().toInt()) { // ← normalize
        FilterValue.StringValue.toInt()    -> str(got)?.let { a -> eqStr(a, (f.value(StringValue()) as StringValue).value(), ci) } ?: false
        FilterValue.NumberValue.toInt()    -> num(got)?.let  { a -> a == (f.value(NumberValue()) as NumberValue).value() } ?: false
        FilterValue.Int64Value.toInt()     -> long(got)?.let { a -> a == (f.value(Int64Value()) as Int64Value).value() } ?: false
        FilterValue.BoolValue.toInt()      -> (got as? Boolean) == (f.value(BoolValue()) as BoolValue).value()
        FilterValue.TimestampValue.toInt() -> micros(got)?.let { a -> a == tvMicros(f.value(TimestampValue()) as TimestampValue) } ?: false
        FilterValue.NullValue.toInt()      -> got == null
        else -> false
    }

    private inline fun cmp(got: Any?, f: BasicSqlDataFilter, crossinline op: (Double, Double) -> Boolean): Boolean {
        val g = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> micros(got)?.toDouble()
            FilterValue.Int64Value.toInt()     -> long(got)?.toDouble()
            else                               -> num(got)
        } ?: return false

        val e = when (f.valueType().toInt()) { // ← normalize
            FilterValue.TimestampValue.toInt() -> tvMicros(f.value(TimestampValue()) as TimestampValue).toDouble()
            FilterValue.Int64Value.toInt()     -> (f.value(Int64Value()) as Int64Value).value().toDouble()
            FilterValue.NumberValue.toInt()    -> (f.value(NumberValue()) as NumberValue).value()
            else -> return false
        }
        return op(g, e)
    }

    private fun inList(got: Any?, f: BasicSqlDataFilter, ci: Boolean): Boolean {
        if (got == null) return false
        return when (f.valueType().toInt()) { // ← normalize
            FilterValue.StringList.toInt() -> {
                val L = f.value(StringList()) as StringList
                val probe = str(got) ?: return false
                (0 until L.valuesLength()).any { i -> eqStr(probe, L.values(i) ?: return@any false, ci) }
            }
            FilterValue.Int64List.toInt() -> {
                val L = f.value(Int64List()) as Int64List
                val g = long(got) ?: return false
                (0 until L.valuesLength()).any { i -> g == L.values(i) }
            }
            FilterValue.Float64List.toInt() -> {
                val L = f.value(Float64List()) as Float64List
                val g = num(got) ?: return false
                (0 until L.valuesLength()).any { i -> java.lang.Double.compare(g, L.values(i)) == 0 }
            }
            FilterValue.BoolList.toInt() -> {
                val L = f.value(BoolList()) as BoolList
                val g = got as? Boolean ?: return false
                (0 until L.valuesLength()).any { i -> g == (L.values(i) == true) }
            }
            else -> false
        }
    }

    // small utils
    private fun str(v: Any?): String? = when (v) { null -> null; is CharSequence -> v.toString(); else -> v.toString() }
    private fun eqStr(a: String, b: String, ci: Boolean) = if (ci) a.equals(b, true) else a == b
    private fun startsWith(a: String, b: String, ci: Boolean) = if (ci) a.lowercase().startsWith(b.lowercase()) else a.startsWith(b)
    private fun endsWith(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().endsWith(b.lowercase()) else a.endsWith(b)
    private fun contains(a: String, b: String, ci: Boolean)   = if (ci) a.lowercase().contains(b.lowercase()) else a.contains(b)
    private fun regexMatch(text: String, pattern: String, ci: Boolean) =
        if (ci) Regex(pattern, setOf(RegexOption.IGNORE_CASE)).containsMatchIn(text) else Regex(pattern).containsMatchIn(text)

    private fun num(v: Any?): Double? = when (v) { is Number -> v.toDouble(); is String -> v.toDoubleOrNull(); else -> null }
    private fun long(v: Any?): Long?  = when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }
    private fun micros(v: Any?): Long?= when (v) { is Number -> v.toLong();   is String -> v.toLongOrNull();  else -> null }

    private fun tvMicros(tv: TimestampValue): Long = when (tv.unit().toInt()) {
        TimeUnit.SECONDS.toInt() -> tv.epoch() * 1_000_000L
        TimeUnit.MILLIS.toInt()  -> tv.epoch() * 1_000L
        TimeUnit.MICROS.toInt()  -> tv.epoch()
        TimeUnit.NANOS.toInt()   -> tv.epoch() / 1_000L
        else -> tv.epoch()
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbQueryAdapter.kt
Size: 213 B
Code:
// Evaluator/FbQueryAdapter.kt
package org.cladbe.cdc.Evaluator

/** Adapts a FlatBuffer StreamingSqlDataFilter into a runtime DerivedSpec. */
interface FbQueryAdapter {
    fun parse(fb: ByteArray): DerivedSpec
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FbQueryAdapterImpl.kt
Size: 1.04 KB
Code:
// Evaluator/FbQueryAdapterImpl.kt
package org.cladbe.cdc.Evaluator

import SqlSchema.StreamingSqlDataFilter
import java.nio.ByteBuffer

class FbQueryAdapterImpl(
    private val tableName: String,
    private val pkField: String,
    private val pkDir: Direction = Direction.DESC
) : FbQueryAdapter {

    override fun parse(fb: ByteArray): DerivedSpec {
        val q = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(ByteBuffer.wrap(fb))
        val k = q.limit().toInt()
        val order = FbMappings.toOrderSpec(q, pkField, pkDir)
        val where: (Map<String, Any?>) -> Boolean = { row -> FbPredicate.evalWrapper(q.wrapper(), row) }

        // TODO: if you add a real extractor, plug it here.
        // For now, a safe fallback is to scan leaf filters and collect field names.
        val filterFields = FilterFieldExtractor.fromWrapper(q.wrapper())

        return DerivedSpec(
            table = tableName,
            k = k,
            order = order,
            whereMatches = where,
            filterFields = filterFields
        )
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/FilterFieldExtractor.kt
Size: 950 B
Code:
package org.cladbe.cdc.Evaluator

import SqlSchema.*

object FilterFieldExtractor {
    @JvmStatic
    fun fromWrapper(w: BasicSqlDataFilterWrapper?): Set<String> {
        if (w == null) return emptySet()
        val out = LinkedHashSet<String>()
        walk(w, out)
        return out
    }

    private fun walk(w: BasicSqlDataFilterWrapper, out: MutableSet<String>) {
        for (i in 0 until w.filtersLength()) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilter.toInt() -> {
                    val f = w.filters(BasicSqlDataFilter(), i) as BasicSqlDataFilter
                    out += f.fieldName()
                }
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val child = w.filters(BasicSqlDataFilterWrapper(), i) as BasicSqlDataFilterWrapper
                    walk(child, out)
                }
            }
        }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/OrderSpec.kt
Size: 1.18 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.*

enum class TimestampUnit { SECONDS, MILLIS, MICROS, NANOS }
enum class Direction { ASC, DESC }
enum class Nulls { FIRST, LAST }

/**
 * One ORDER BY item.
 * - `isPk` must be true for exactly one trailing key (the primary-key tiebreaker).
 * - `nulls` is explicit (no null); default follows SQL: ASC→LAST, DESC→FIRST.
 */
data class OrderKeySpec(
    val field: String,
    val dir: Direction,
    val nulls: Nulls = nullsDefaultFor(dir),
    val isPk: Boolean = false
) {
    companion object {
        @JvmStatic fun of(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, false)

        @JvmStatic fun pk(field: String, dir: Direction, nulls: Nulls = nullsDefaultFor(dir)) =
            OrderKeySpec(field, dir, nulls, true)

        @JvmStatic fun nullsDefaultFor(d: Direction): Nulls =
            if (d == Direction.ASC) Nulls.LAST else Nulls.FIRST
    }
}

/** Row field accessor (SAM for Java interop). */
fun interface RowAccessor<R> { fun get(row: R, field: String): Any? }

/** Lookup a row by PK (SAM for Java interop). */
fun interface PkLookup<PK, R> { fun get(pk: PK): R? }
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/PageOps.kt
Size: 3.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.util.Comparator

object PageOps {

    data class InsertResult<PK>(
        val inserted: Boolean,
        val pos: Int,            // where we inserted; -1 if not inserted
        val evicted: PK? = null  // tail PK evicted when window was full
    )

    data class RepositionResult<PK>(
        val moved: Boolean,
        val from: Int,
        val to: Int
    )

    /**
     * Lower-bound binary search: first index where [key] could be inserted
     * without violating sorted order (stable insert).
     */
    @JvmStatic
    fun <T> insertionIndex(list: List<T>, key: T, cmp: Comparator<T>): Int {
        var lo = 0
        var hi = list.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val c = cmp.compare(list[mid], key)
            if (c < 0) lo = mid + 1 else hi = mid
        }
        return lo
    }

    /**
     * Insert a PK into a sorted Top-K page if it belongs.
     * - If page.size < K → insert at correct position.
     * - If full and new PK is “better” than current tail → evict tail, insert.
     * - Else → ignore.
     */
    @JvmStatic
    fun <PK> insertIfBetter(
        page: MutableList<PK>,
        pk: PK,
        k: Int,
        cmp: Comparator<PK>
    ): InsertResult<PK> {
        // If already present, do nothing (call repositionIfNeeded after you’ve merged rowCache)
        val existing = page.indexOf(pk)
        if (existing >= 0) return InsertResult(inserted = false, pos = existing, evicted = null)

        if (page.size < k) {
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = null)
        }

        // Page full: compare against tail
        val tail = page.last()
        if (cmp.compare(pk, tail) < 0) {
            // New item outranks the tail → evict tail and insert
            page.removeAt(page.size - 1)
            val pos = insertionIndex(page, pk, cmp)
            page.add(pos, pk)
            return InsertResult(inserted = true, pos = pos, evicted = tail)
        }
        return InsertResult(inserted = false, pos = -1, evicted = null)
    }

    /**
     * If a PK is already in the page and its order fields changed,
     * remove it and reinsert at its correct position.
     */
    @JvmStatic
    fun <PK> repositionIfNeeded(
        page: MutableList<PK>,
        pk: PK,
        cmp: Comparator<PK>
    ): RepositionResult<PK> {
        val from = page.indexOf(pk)
        if (from < 0) return RepositionResult(moved = false, from = -1, to = -1)

        // Remove first so the insertion index is computed against the remaining items
        page.removeAt(from)
        val to = insertionIndex(page, pk, cmp)
        page.add(to, pk)
        return RepositionResult(moved = (to != from), from = from, to = to)
    }

    /** Remove PK if present. Returns the removed index or -1. */
    @JvmStatic
    fun <PK> removeIfPresent(page: MutableList<PK>, pk: PK): Int {
        val idx = page.indexOf(pk)
        if (idx >= 0) page.removeAt(idx)
        return idx
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/QueryEngine.kt
Size: 9.54 KB
Code:
package org.cladbe.cdc.engine

import kotlin.math.max
import org.slf4j.LoggerFactory
import org.cladbe.cdc.Evaluator.DerivedSpec
import org.cladbe.cdc.Evaluator.Direction
import org.cladbe.cdc.Evaluator.Nulls
import org.cladbe.cdc.Evaluator.OutboxCodec

sealed class WireDiff<PK> {
    data class Added<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    data class Removed<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    data class Modified<PK>(val pk: PK, val pos: Int) : WireDiff<PK>()
    data class Promoted<PK>(val pk: PK, val from: Int, val to: Int) : WireDiff<PK>()
}

data class UpsertResult<PK>(
    val changes: List<OutboxCodec.WireDiff<PK>>,
    val needsBackfill: Boolean,
    val probeTail: Boolean
)

class QueryEngine<PK>(
    private val spec: DerivedSpec,
    private val pkField: String,
    private val getPk: (Map<String, Any?>) -> PK,
    private val rowProvider: (PK) -> Map<String, Any?>? = { null }
) {
    private val log = LoggerFactory.getLogger(QueryEngine::class.java)

    val page: MutableList<PK> = mutableListOf()
    fun pageCapacity(): Int = spec.k

    fun upsert(row: Map<String, Any?>): UpsertResult<PK> {
        val pk = getPk(row)
        val prev = rowProvider(pk)
        val inPageBefore = page.contains(pk)

        val matchesAfter = spec.whereMatches(row)
        val changed = diffFields(prev, row)

        val orderFields = spec.order.map { it.field }.toSet()
        val orderChanged = changed.any { it in orderFields }
        val filterChanged = changed.any { it in spec.filterFields }

        // ——— CASE A: row does not match WHERE anymore ———
        if (!matchesAfter) {
            if (!inPageBefore) {
                log.debug("[upsert] pk={} no-op (didn't match and wasn't in page)", pk)
                return UpsertResult(emptyList(), false, false)
            }
            val idx = page.indexOf(pk)
            if (idx >= 0) {
                page.removeAt(idx)
                log.debug("[upsert] pk={} removed at idx={} (no longer matches)", pk, idx)
                return UpsertResult(
                    changes = listOf(OutboxCodec.WireDiff("removed", pk, pos = idx)),
                    needsBackfill = true,
                    probeTail = false
                )
            }
            return UpsertResult(emptyList(), false, false)
        }

        // ——— CASE B: brand-new PK (not in page before) ———
        if (!inPageBefore) {
            // Space available → insert where it belongs
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                log.debug("[upsert] pk={} inserted at pos={} (page had space)", pk, pos)
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            }

            // Page full → compare vs tail; if tail row is missing, DO NOT silently drop.
            val tailIndex = page.size - 1
            val tailPk = page.last()
            val tailRow = rowProvider(tailPk)

            if (tailRow == null) {
                // Permanent fix: treat unknown tail as lowest rank, evict it.
                page.removeAt(tailIndex)
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                log.debug("[upsert] pk={} inserted at pos={}, evicted tail pk={} (tail row was missing)", pk, pos, tailPk)
                return UpsertResult(
                    changes = listOf(
                        OutboxCodec.WireDiff("removed", tailPk, pos = tailIndex),
                        OutboxCodec.WireDiff("added", pk, pos = pos)
                    ),
                    needsBackfill = false,
                    probeTail = false
                )
            }

            return if (compareRows(row, tailRow) < 0) {
                // Better than tail → evict tail and insert
                page.removeAt(tailIndex)
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                log.debug("[upsert] pk={} inserted at pos={}, evicted tail pk={}", pk, pos, tailPk)
                UpsertResult(
                    listOf(
                        OutboxCodec.WireDiff("removed", tailPk, pos = tailIndex),
                        OutboxCodec.WireDiff("added", pk, pos = pos)
                    ),
                    false, false
                )
            } else {
                log.debug("[upsert] pk={} ignored (worse than/equal to current tail)", pk)
                UpsertResult(emptyList(), false, false)
            }
        }

        // ——— CASE C: we *thought* it wasn't in page, but oldIdx < 0 (page mutated) ———
        val oldIdx = page.indexOf(pk)
        if (oldIdx < 0) {
            // Reapply the same logic as CASE B to be deterministic.
            if (page.size < spec.k) {
                val pos = findInsertPosition(row)
                page.add(pos, pk)
                log.debug("[upsert] pk={} inserted at pos={} (late-detected)", pk, pos)
                return UpsertResult(listOf(OutboxCodec.WireDiff("added", pk, pos = pos)), false, false)
            } else {
                val tailIndex = page.size - 1
                val tailPk = page.last()
                val tailRow = rowProvider(tailPk)
                return if (tailRow == null || compareRows(row, tailRow) < 0) {
                    page.removeAt(tailIndex)
                    val pos = findInsertPosition(row)
                    page.add(pos, pk)
                    log.debug("[upsert] pk={} inserted at pos={}, evicted tail pk={} (late-detected)", pk, pos, tailPk)
                    UpsertResult(
                        listOf(
                            OutboxCodec.WireDiff("removed", tailPk, pos = tailIndex),
                            OutboxCodec.WireDiff("added", pk, pos = pos)
                        ),
                        false, false
                    )
                } else {
                    log.debug("[upsert] pk={} ignored (late-detected, worse than tail)", pk)
                    UpsertResult(emptyList(), false, false)
                }
            }
        }

        // ——— CASE D: existing PK ———
        if (orderChanged) {
            page.removeAt(oldIdx)
            val newPos = findInsertPosition(row)
            page.add(newPos, pk)
            log.debug("[upsert] pk={} moved from {} to {} (order fields changed)", pk, oldIdx, newPos)
            val diff = OutboxCodec.WireDiff("modified", pk, pos = newPos, from = oldIdx)
            val probe = (filterChanged || orderChanged) && page.size >= spec.k
            return UpsertResult(listOf(diff), needsBackfill = false, probeTail = probe)
        }

        val diffs =
            if (changed.isNotEmpty()) {
                log.debug("[upsert] pk={} modified in-place at {}", pk, oldIdx)
                listOf(OutboxCodec.WireDiff("modified", pk, pos = oldIdx, from = oldIdx))
            } else emptyList()

        val probe = (filterChanged || orderChanged) && page.size >= spec.k
        return UpsertResult(diffs, needsBackfill = false, probeTail = probe)
    }

    fun currentCursor(): Map<String, Any?>? {
        if (page.isEmpty()) return null
        val tailPk = page.last()
        val tail = rowProvider(tailPk) ?: return null
        val m = LinkedHashMap<String, Any?>(spec.order.size)
        for (ok in spec.order) m[ok.field] = tail[ok.field]
        return m
    }

    // ---- helpers ----
    private fun findInsertPosition(row: Map<String, Any?>): Int {
        var lo = 0
        var hi = page.size
        while (lo < hi) {
            val mid = (lo + hi) ushr 1
            val midRow = rowProvider(page[mid])
            // Permanent fix: unknown midRow should *not* push new row to the end.
            // Treat null midRow as lowest-quality row so new item sorts *before* it.
            val cmp = if (midRow == null) -1 else compareRows(row, midRow)
            if (cmp < 0) hi = mid else lo = mid + 1
        }
        return lo
    }

    private fun compareRows(a: Map<String, Any?>, b: Map<String, Any?>): Int {
        for ((i, ok) in spec.order.withIndex()) {
            val av = a[ok.field]
            val bv = b[ok.field]
            val asc = ok.dir == Direction.ASC
            val nullsFirst = ok.nulls == Nulls.FIRST
            val c = compareField(av, bv, asc, nullsFirst)
            if (c != 0) return c
            if (i == spec.order.lastIndex) return 0
        }
        return 0
    }

    private fun compareField(a: Any?, b: Any?, asc: Boolean, nullsFirst: Boolean): Int {
        if (a == null && b == null) return 0
        if (a == null) return if (nullsFirst) -1 else 1
        if (b == null) return if (nullsFirst)  1 else -1
        val base = when {
            a is Number && b is Number -> a.toDouble().compareTo(b.toDouble())
            a is String && b is String -> a.compareTo(b)
            a is Boolean && b is Boolean -> a.compareTo(b)
            a::class == b::class && a is Comparable<*> -> (a as Comparable<Any>).compareTo(b)
            else -> a.toString().compareTo(b.toString())
        }
        return if (asc) base else -base
    }

    private fun diffFields(prev: Map<String, Any?>?, next: Map<String, Any?>): Set<String> {
        if (prev == null) return next.keys
        val keys = HashSet<String>(next.keys).apply { addAll(prev.keys) }
        val changed = mutableSetOf<String>()
        for (k in keys) if (!eq(prev[k], next[k])) changed += k
        return changed
    }
    private fun eq(a: Any?, b: Any?): Boolean = when {
        a === b -> true
        a == null || b == null -> false
        a is Number && b is Number -> a.toDouble() == b.toDouble()
        else -> a == b
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/Evaluator/SqlComparators.kt
Size: 6.07 KB
Code:
package org.cladbe.cdc.Evaluator

import java.math.BigDecimal
import java.time.*
import java.util.Comparator

object SqlComparators {

    /** Comparator for rows, SQL-like semantics with NULLS rule and PK tiebreaker. */
    @JvmStatic
    fun <R> rowComparator(
        spec: List<OrderKeySpec>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<R> {
        validateSpec(spec)
        val s = applyNullDefaults(spec)
        val hints = tsHints

        return Comparator { a, b ->
            for (k in s) {
                var va = acc.get(a, k.field)
                var vb = acc.get(b, k.field)

                // normalize temporals to epoch-nanos (Long)
                val hint = hints[k.field]
                va = normalizeTemporal(va, hint)
                vb = normalizeTemporal(vb, hint)

                val c = compareField(va, vb, k)
                if (c != 0) return@Comparator c
            }
            0 // equal across all fields (including PK)
        }
    }

    /** Comparator for PKs that fetches rows from a cache on demand. */
    @JvmStatic
    fun <PK, R> pkComparator(
        spec: List<OrderKeySpec>,
        lookup: PkLookup<PK, R>,
        acc: RowAccessor<R>,
        tsHints: Map<String, TimestampUnit> = emptyMap()
    ): Comparator<PK> {
        val rowCmp = rowComparator(spec, acc, tsHints)
        return Comparator { pk1, pk2 ->
            if (pk1 == pk2) return@Comparator 0
            val r1 = lookup.get(pk1)
            val r2 = lookup.get(pk2)
            if (r1 == null && r2 == null) return@Comparator 0
            if (r1 == null) return@Comparator 1
            if (r2 == null) return@Comparator -1
            val c = rowCmp.compare(r1, r2)
            if (c != 0) c else safeCompare(pk1, pk2)
        }
    }

    // ---------- internals ----------

    private fun validateSpec(spec: List<OrderKeySpec>) {
        require(spec.isNotEmpty()) { "orderSpec must contain at least one field (include PK as last)" }
        require(spec.last().isPk) { "orderSpec must end with the PK field as final tiebreaker" }
    }

    private fun applyNullDefaults(inSpec: List<OrderKeySpec>): List<OrderKeySpec> =
        inSpec.map { k ->
            val n = k.nulls ?: OrderKeySpec.nullsDefaultFor(k.dir)
            k.copy(nulls = n)
        }

    private fun compareField(a: Any?, b: Any?, k: OrderKeySpec): Int {
        // 1) NULL rules
        if (a == null || b == null) {
            if (a == null && b == null) return 0
            val firstIsNull = a == null
            return if (k.nulls == Nulls.FIRST)
                if (firstIsNull) -1 else 1
            else
                if (firstIsNull) 1 else -1
        }

        // 2) Non-null comparison
        val base = compareNonNull(a, b)
        if (base == 0) return 0

        // 3) Direction
        return if (k.dir == Direction.ASC) base else -base
    }

    @Suppress("UNCHECKED_CAST")
    private fun compareNonNull(a: Any, b: Any): Int {
        // Numbers → BigDecimal
        if (a is Number && b is Number) {
            val da = toBigDecimal(a)
            val db = toBigDecimal(b)
            return da.compareTo(db)
        }

        // CharSequence → String (safe fallback for mixed types later)
        if (a is CharSequence && b is CharSequence) {
            return a.toString().compareTo(b.toString())
        }

        // Boolean: false < true
        if (a is Boolean && b is Boolean) {
            return java.lang.Boolean.compare(a, b)
        }

        // Same class & Comparable
        if (a::class.java == b::class.java && a is Comparable<*>) {
            return (a as Comparable<Any>).compareTo(b)
        }

        // Fallback: String compare to guarantee total order
        return a.toString().compareTo(b.toString())
    }

    private fun toBigDecimal(n: Number): BigDecimal =
        when (n) {
            is Long, is Int, is Short, is Byte -> BigDecimal.valueOf(n.toLong())
            is BigDecimal -> n
            is Float, is Double -> BigDecimal.valueOf(n.toDouble())
            else -> BigDecimal(n.toString())
        }

    private fun normalizeTemporal(v: Any?, hint: TimestampUnit?): Any? {
        if (v == null) return null

        // java.time first
        when (v) {
            is Instant -> return toEpochNanos(v)
            is OffsetDateTime -> return toEpochNanos(v.toInstant())
            is ZonedDateTime -> return toEpochNanos(v.toInstant())
            is LocalDateTime -> return toEpochNanos(v.toInstant(ZoneOffset.UTC))      // assume UTC
            is LocalDate -> return toEpochNanos(v.atStartOfDay(ZoneOffset.UTC).toInstant())
        }

        // legacy dates
        if (v is java.util.Date) return toEpochNanos(v.toInstant())
        if (v is java.sql.Timestamp) return toEpochNanos(v.toInstant())

        // numeric epochs
        if (v is Number) {
            val raw = v.toLong()
            val unit = hint ?: guessUnit(raw)
            return when (unit) {
                TimestampUnit.SECONDS -> raw * 1_000_000_000L
                TimestampUnit.MILLIS  -> raw * 1_000_000L
                TimestampUnit.MICROS  -> raw * 1_000L
                TimestampUnit.NANOS   -> raw
            }
        }

        // Not temporal → return as-is
        return v
    }

    private fun toEpochNanos(i: Instant): Long =
        i.epochSecond * 1_000_000_000L + i.nano

    private fun guessUnit(v: Long): TimestampUnit {
        val av = kotlin.math.abs(v)
        return when {
            av < 100_000_000_000L -> TimestampUnit.SECONDS      // < 1e11
            av < 100_000_000_000_000L -> TimestampUnit.MILLIS   // < 1e14
            av < 100_000_000_000_000_000L -> TimestampUnit.MICROS // < 1e17
            else -> TimestampUnit.NANOS
        }
    }

    private fun <T> safeCompare(a: T?, b: T?): Int {
        if (a == null && b == null) return 0
        if (a == null) return 1
        if (b == null) return -1
        return if (a::class.java == b::class.java && a is Comparable<*>) {
            @Suppress("UNCHECKED_CAST")
            (a as Comparable<T>).compareTo(b)
        } else {
            a.toString().compareTo(b.toString())
        }
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/App.kt
Size: 1.66 KB
Code:
// src/main/java/org/cladbe/cdc/engine/App.kt
package org.cladbe.cdc.engine

import org.apache.kafka.streams.KafkaStreams
import org.apache.kafka.streams.StreamsConfig
import java.util.Properties
import java.util.regex.Pattern

fun main() {
    val brokers  = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"
    val appId    = System.getenv("STREAMS_APP_ID") ?: "cladbe-kstreams-shared"

    val topics = StreamsTopology.Topics(
        cdcPattern        = Pattern.compile(System.getenv("CDC_TOPIC_PATTERN") ?: "^server\\.cdc\\..*$"),
        queryControlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control",
        seedTopic         = System.getenv("PAGE_SEED_TOPIC") ?: "server.page.seed",
        rpcReplyTopic     = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
        outboxTopic       = System.getenv("WS_OUTBOX_TOPIC") ?: "server.page.diffs",
        backfillTopic     = System.getenv("BACKFILL_ROWS_TOPIC") ?: "server.backfill.rows"
    )

    val topology = StreamsTopology.build(topics)

    val props = Properties().apply {
        put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
        put(StreamsConfig.APPLICATION_ID_CONFIG, appId)
        put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.AT_LEAST_ONCE)
        put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
        put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
            org.apache.kafka.common.serialization.Serdes.ByteArray()::class.java)
    }

    val streams = KafkaStreams(topology, props)
    Runtime.getRuntime().addShutdownHook(Thread { streams.close() })
    streams.start()
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/BackfillClient.kt
Size: 4.34 KB
Code:
// src/main/java/org/cladbe/cdc/engine/BackfillClient.kt
package org.cladbe.cdc.engine

import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.ByteArrayDeserializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.kafka.common.serialization.StringSerializer
import java.time.Duration
import java.util.*
import java.util.concurrent.ThreadLocalRandom

/**
 * Minimal Kafka RPC client for the postgres_rpc worker.
 *
 * NOTE:
 * - We send a RequestEnvelope(GetData) that the TS worker understands.
 * - The worker now supports keyset: orderKeys + cursor + strictAfter on its side.
 * - Response payload "RowsJson.rows[]" are JSON strings (one per row).
 */
class BackfillClient(
    private val requestTopic: String = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests",
    private val responseTopic: String = System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses",
    bootstrapServers: String = System.getenv("KAFKA_BROKERS") ?: "localhost:9092",
    private val groupId: String = System.getenv("SQL_RPC_GROUP_ID") ?: "cladbe-kstreams-backfill"
) : AutoCloseable {

    private val producer = KafkaProducer<String, ByteArray>(
        Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
    )

    private val consumer = KafkaConsumer<String, ByteArray>(
        Properties().apply {
            put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
            put(ConsumerConfig.GROUP_ID_CONFIG, "$groupId-${ThreadLocalRandom.current().nextInt(1_000_000)}")
            put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
            put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer::class.java)
            put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest")
            put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true")
        }
    ).also { it.subscribe(listOf(responseTopic)) }

    override fun close() {
        try { consumer.close() } catch (_: Exception) {}
        try { producer.close() } catch (_: Exception) {}
    }

    // ---------- FB builders (RequestEnvelope<GetData>) ----------
    // We reuse your FlatBuffers generated Java from sql_rpc.fbs & sql_schema.fbs
    // Package name below assumes flatc generated into SqlRpc / SqlSchema namespaces.

    fun requestOne(
        companyId: String,
        table: String,
        order: List<OrderKey>,               // simple POJO below
        filtersFbBytes: ByteArray?,          // optional FB BasicSqlDataFilterWrapper (or null)
        cursorTuple: Map<String, Any?>?,     // last-tuple of order keys
        strictAfter: Boolean = true,
        timeoutMs: Long = 1_000
    ): String? /* row JSON */ {
        val corr = UUID.randomUUID().toString()

        val reqBytes = FbRequests.buildGetDataEnvelope(
            correlationId = corr,
            replyTopic = responseTopic,
            companyId = companyId,
            tableName = table,
            order = order,
            wrapperFbBytes = filtersFbBytes,
            limit = 1,
            offset = null,
            cursorTuple = cursorTuple,
            strictAfter = strictAfter
        )

        // send
        producer.send(ProducerRecord(requestTopic, corr, reqBytes))

        // wait single reply for corr id
        val deadline = System.currentTimeMillis() + timeoutMs
        while (System.currentTimeMillis() < deadline) {
            val polled = consumer.poll(Duration.ofMillis(50))
            for (rec in polled) {
                if (rec.key() != corr) continue
                val rowJson = FbResponses.parseRowsJsonFirst(rec.value()) // null if none
                return rowJson
            }
        }
        return null
    }
}

/** Minimal order POJO for the FB builder */
data class OrderKey(val field: String, val sortOrdinal: Int)

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/BackfillReplyProcessor.kt
Size: 2.28 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore
import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper

class BackfillReplyProcessor(
    private val rowCacheName: String,
    private val pendingName: String,
    /** Name of the child sink node wired to the internal backfill topic. */
    private val backfillTopicName: String
) : Processor<String, ByteArray, ByteArray, ByteArray> {

    private lateinit var ctx: ProcessorContext<ByteArray, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var pending : KeyValueStore<String, String>
    private val mapper = ObjectMapper()
    private val debug = System.getenv("CDC_DEBUG") == "1"

    override fun init(context: ProcessorContext<ByteArray, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        pending  = context.getStateStore(pendingName)
    }

    override fun process(record: Record<String, ByteArray>) {
        val corr = record.key() ?: return
        val fb = record.value() ?: return

        val hashKey = pending.get(corr) ?: return // "$table|$hashId"
        val table = hashKey.substringBefore('|')
        val hashId = hashKey.substringAfter('|')

        val rowJson: String = FbResponses.parseRowsJsonFirst(fb) ?: return

        val rowMap: Map<String, Any?> = try {
            mapper.readValue(rowJson, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(rowMap, pkCol) ?: return
        val rowBytes = rowJson.toByteArray()

        if (debug) println("[BackfillReply] corr=$corr table=$table hashId=$hashId pkCol=$pkCol pk=$pk forwarding to backfill topic")

        // Warm row cache
        rowCache.put(pk, rowBytes)

        // Forward to internal backfill topic
        ctx.forward(
            Record(pk.toByteArray(), rowBytes, record.timestamp()),
            backfillTopicName
        )

        pending.delete(corr)
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/CdcProcessor.kt
Size: 14.39 KB
Code:
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.header.internals.RecordHeaders
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.apache.kafka.streams.processor.Cancellable
import org.apache.kafka.streams.processor.PunctuationType
import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore
import org.cladbe.cdc.Evaluator.FbQueryAdapterImpl
import org.cladbe.cdc.Evaluator.OutboxCodec
import java.time.Duration
import java.util.*

class CdcProcessor(
    private val rowCacheName: String,
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String,
    private val pendingName: String? = null
) : Processor<ByteArray, ByteArray, String, ByteArray> {

    private lateinit var ctx: ProcessorContext<String, ByteArray>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var qmeta   : KeyValueStore<String, ByteArray>
    private lateinit var page    : KeyValueStore<String, ByteArray>
    private lateinit var outbox  : KeyValueStore<String, ByteArray>
    private var pending          : KeyValueStore<String, String>? = null

    private val engines = mutableMapOf<String, QueryEngine<String>>() // key: "$table|$hashId"
    private val mapper = ObjectMapper()
    private val debug = System.getenv("CDC_DEBUG") == "1"

    // adaptive flush
    private var flushTask: Cancellable? = null
    private var flushIntervalMs = 200L
    private val minFlushMs = 50L
    private val maxFlushMs = 5_000L
    private val batchChangeThreshold = 64
    private val hardMaxLatencyMs = 750L

    private var pendingChangeCount = 0
    private var lastFlushMs = 0L
    private var lastActivityMs = 0L
    private var lastSeenLsn: Long = 0L
    private val dirtyHashes = mutableSetOf<String>()
    private var keydb: KeyDbWriter? = null

    // backfill
    private lateinit var rpcProducer: KafkaProducer<ByteArray, ByteArray>
    private val requestTopic = System.getenv("SQL_RPC_REQUEST_TOPIC") ?: "sql.rpc.requests"
    private val replyTopic   = System.getenv("SQL_RPC_REPLY_TOPIC")
        ?: System.getenv("SQL_RPC_RESPONSE_TOPIC") ?: "sql.rpc.responses"
    private val rpcBrokers   = System.getenv("KAFKA_BROKERS") ?: "localhost:9092"

    override fun init(context: ProcessorContext<String, ByteArray>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        qmeta    = context.getStateStore(qmetaName)
        page     = context.getStateStore(pageName)
        outbox   = context.getStateStore(outboxName)
        if (pendingName != null) pending = context.getStateStore(pendingName)

        val now = System.currentTimeMillis()
        lastFlushMs = now
        lastActivityMs = now

        scheduleFlush(flushIntervalMs)
        rpcProducer = newProducer()
        runCatching { keydb = KeyDbWriter() }.onFailure { e ->
            println("[streams] KeyDB disabled: ${e.message}")
        }
        if (debug) {
            println("[CdcProcessor] init: requestTopic=$requestTopic replyTopic=$replyTopic brokers=$rpcBrokers")
            println("[CdcProcessor] init: flushIntervalMs=$flushIntervalMs min=$minFlushMs max=$maxFlushMs")
        }
    }

    override fun process(record: Record<ByteArray, ByteArray>) {
        // --- derive table from topic name ---
        val topic = ctx.recordMetadata().orElse(null)?.topic() ?: return
        // expect topics like "server.cdc.tenant_table"
        val table = topic.substringAfter("server.cdc.", topic.substringAfterLast('.'))

        val row: Map<String, Any?> = decodeRow(record.value()) ?: return
        extractLsn(record.value())?.let { lsn -> if (lsn > lastSeenLsn) lastSeenLsn = lsn }

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"
        val pk = extractPkOrNull(row, pkCol) ?: return

        rowCache.put(pk, record.value())
        keydb?.writeHotRow(table = table, pk = pk, row = row, lsn = lastSeenLsn)

        if (debug) {
            println("[CdcProcessor] process: topic=$topic table=$table pkCol=$pkCol pk=$pk lsn=$lastSeenLsn")
        }

        var producedChanges = 0

        // Iterate only queries for this table: keys starting with "$table|"
        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val entry = iter.next()
                val key = entry.key
                if (!key.startsWith("$table|")) continue
                val hashKey = key // "$table|$hashId"
                val hashId = hashKey.substringAfter('|')

                val engine = engines.getOrPut(hashKey) {
                    val spec = FbQueryAdapterImpl(tableName = table, pkField = pkCol).parse(entry.value)
                    if (debug) println("[CdcProcessor] engine:create hashId=$hashId specK=${spec.k} order=${spec.order} filterFields=${spec.filterFields}")
                    QueryEngine(
                        spec = spec,
                        pkField = pkCol,
                        getPk = { r -> extractPkOrNull(r, pkCol)!! },
                        rowProvider = { p -> rowFromCache(p) }
                    )
                }

                val result = engine.upsert(row)

                if (result.changes.isNotEmpty()) {
                    producedChanges += result.changes.size
                    page.put(hashKey, encodePkList(engine.page))
                    val prev = outbox.get(hashKey)
                    val appended = appendOutbox(prev, result.changes)
                    outbox.put(hashKey, appended)

                    if (debug) {
                        val added = result.changes.count { it.type == "added" }
                        val removed = result.changes.count { it.type == "removed" }
                        val modified = result.changes.count { it.type == "modified" }
                        println("[CdcProcessor] outbox:+ hashId=$hashId Δ=${result.changes.size} (add=$added rm=$removed mod=$modified) pageSize=${engine.page.size} bytes=${appended.size}")
                    }

                    dirtyHashes += hashKey
                }

                when {
                    result.needsBackfill -> {
                        if (debug) println("[CdcProcessor] needsBackfill: hashId=$hashId")
                        triggerBackfill(hashKey, engine, table)
                    }
                    result.probeTail     -> {
                        if (debug) println("[CdcProcessor] probeTail: hashId=$hashId")
                        triggerBackfill(hashKey, engine, table)
                    }
                }
            }
        }

        if (producedChanges > 0) {
            lastActivityMs = System.currentTimeMillis()
            pendingChangeCount += producedChanges
            val age = lastActivityMs - lastFlushMs
            val hitBatch = pendingChangeCount >= batchChangeThreshold
            val hitAge   = age >= hardMaxLatencyMs
            if (debug) {
                println("[CdcProcessor] batch: produced=$producedChanges pending=$pendingChangeCount ageMs=$age hitBatch=$hitBatch hitAge=$hitAge flushIntervalMs=$flushIntervalMs")
            }
            if (hitBatch || hitAge) {
                flushAll()
                if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            } else if (flushIntervalMs > 2 * minFlushMs) {
                rescheduleFlush(flushIntervalMs / 2)
            }
        }
    }

    private fun onFlushTick() {
        if (pendingChangeCount > 0) {
            if (debug) println("[CdcProcessor] flushTick: pending=$pendingChangeCount → flushAll() now")
            flushAll()
            if (flushIntervalMs > minFlushMs) rescheduleFlush(minFlushMs)
            return
        }
        val newInterval = (flushIntervalMs * 2).coerceAtMost(maxFlushMs)
        if (newInterval != flushIntervalMs) rescheduleFlush(newInterval)
        if (debug) println("[CdcProcessor] flushTick: idle; nextInMs=$flushIntervalMs")
    }

    private fun flushAll() {
        var flushedAny = false
        var batches = 0
        var totalBytes = 0

        qmeta.all().use { iter ->
            while (iter.hasNext()) {
                val e = iter.next()
                val hashKey = e.key              // "$table|$hashId"
                val hashId  = hashKey.substringAfter('|')
                val ob = outbox.get(hashKey) ?: continue
                if (ob.isEmpty()) continue

                val headers = RecordHeaders().apply {
                    if (lastSeenLsn > 0L) add("lsn", longToBe(lastSeenLsn))
                }
                ctx.forward(Record(hashId, ob, System.currentTimeMillis(), headers))
                if (debug) println("[CdcProcessor] flush: hashId=$hashId bytes=${ob.size} lsn=$lastSeenLsn")

                runCatching {
                    val b64 = java.util.Base64.getEncoder().encodeToString(ob)
                    keydb?.appendDiff(hashId, lastSeenLsn, b64)
                }

                outbox.put(hashKey, ByteArray(0))
                flushedAny = true
                batches += 1
                totalBytes += ob.size

                if (hashKey in dirtyHashes) {
                    val engine = engines[hashKey]
                    if (engine != null) {
                        runCatching {
                            keydb?.writeRangeIndex(hashId, engine.page.toList())
                            keydb?.writeReverseIndex(hashId, engine.page.toList())
                        }
                        val rows = buildSnapshotRows(hashKey)
                        runCatching { keydb?.writeSnapshot(hashId, rows, lastSeenLsn) }
                        if (debug) println("[CdcProcessor] snapshot: hashId=$hashId rows=${engine.page.size}")
                    }
                    dirtyHashes.remove(hashKey)
                }
            }
        }
        if (flushedAny) {
            pendingChangeCount = 0
            lastFlushMs = System.currentTimeMillis()
            if (debug) println("[CdcProcessor] flush: complete batches=$batches bytes=$totalBytes nextFlushMs=$flushIntervalMs")
        } else if (debug) {
            println("[CdcProcessor] flush: nothing-to-do")
        }
    }

    override fun close() {
        flushTask?.cancel()
        runCatching { rpcProducer.close() }
        if (debug) println("[CdcProcessor] close()")
    }

    private fun triggerBackfill(hashKey: String, engine: QueryEngine<String>, table: String) {
        val tailCursor = engine.currentCursor() ?: run {
            if (debug) println("[CdcProcessor] backfill: skip (no cursor) hashKey=$hashKey")
            return
        }
        val qfb = qmeta.get(hashKey) ?: run {
            if (debug) println("[CdcProcessor] backfill: skip (no qmeta) hashKey=$hashKey")
            return
        }

        val corr = UUID.randomUUID().toString()
        pending?.put(corr, hashKey)

        val reqBytes = FbRequests.buildGetDataFromQueryFb(
            correlationId = corr,
            replyTopic = replyTopic,
            queryFb = qfb,
            cursorTuple = tailCursor,
            limit = 1,
            strictlyAfter = true,
            fullTableName = table
        )
        rpcProducer.send(ProducerRecord(requestTopic, corr.toByteArray(), reqBytes))
        if (debug) println("[CdcProcessor] backfill: sent corr=$corr hashKey=$hashKey cursor=$tailCursor -> requestTopic=$requestTopic")
    }

    // ---- timers & helpers ----
    private fun scheduleFlush(intervalMs: Long) {
        flushTask?.cancel()
        flushIntervalMs = intervalMs
        flushTask = ctx.schedule(Duration.ofMillis(intervalMs), PunctuationType.WALL_CLOCK_TIME) { onFlushTick() }
        if (debug) println("[CdcProcessor] scheduleFlush: every ${intervalMs}ms")
    }
    private fun rescheduleFlush(newIntervalMs: Long) {
        if (newIntervalMs == flushIntervalMs) return
        scheduleFlush(newIntervalMs)
        if (debug) println("[CdcProcessor] rescheduleFlush: → ${newIntervalMs}ms")
    }

    private fun rowFromCache(pk: String): Map<String, Any?>? {
        val bytes = rowCache.get(pk) ?: return null
        return decodeRow(bytes)
    }
    @Suppress("UNCHECKED_CAST")
    private fun decodeRow(bytes: ByteArray): Map<String, Any?>? =
        try { mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {}) } catch (_: Exception) { null }

    private fun encodePkList(list: List<String>): ByteArray = list.joinToString(",").toByteArray()
    private fun appendOutbox(prev: ByteArray?, changes: List<OutboxCodec.WireDiff<String>>): ByteArray {
        val add = changes.joinToString("|") { "${it.type}:${it.pk}@${it.pos ?: -1},${it.from ?: -1}" }
        val s = (prev?.toString(Charsets.UTF_8)?.takeIf { it.isNotEmpty() }?.plus("||") ?: "") + add
        return s.toByteArray()
    }
    private fun buildSnapshotRows(hashKey: String): List<Map<String, Any?>> {
        val engine = engines[hashKey] ?: return emptyList()
        val rows = ArrayList<Map<String, Any?>>(engine.page.size)
        for (pk in engine.page) rowFromCache(pk)?.let { rows += it }
        return rows
    }
    private fun newProducer(): KafkaProducer<ByteArray, ByteArray> {
        val props = java.util.Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, rpcBrokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
            put(ProducerConfig.LINGER_MS_CONFIG, "5")
        }
        return KafkaProducer(props)
    }
    private fun extractLsn(bytes: ByteArray): Long? = try {
        val m: Map<String, Any?> = mapper.readValue(bytes, object : TypeReference<Map<String, Any?>>() {})
        when (val v = m["__lsn"]) {
            is Number -> v.toLong()
            is String -> v.toLongOrNull()
            is Map<*, *> -> (v["long"] as? Number)?.toLong() ?: (v["long"] as? String)?.toLongOrNull()
            else -> null
        }
    } catch (_: Exception) { null }

    private fun longToBe(v: Long): ByteArray {
        val b = ByteArray(8)
        b[0] = (v ushr 56).toByte(); b[1] = (v ushr 48).toByte()
        b[2] = (v ushr 40).toByte(); b[3] = (v ushr 32).toByte()
        b[4] = (v ushr 24).toByte(); b[5] = (v ushr 16).toByte()
        b[6] = (v ushr 8).toByte();  b[7] = (v).toByte()
        return b
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/FbRequests.kt
Size: 13.31 KB
Code:
// src/main/java/org/cladbe/cdc/engine/FbRequests.kt
package org.cladbe.cdc.engine

import com.google.flatbuffers.FlatBufferBuilder
import SqlRpc.*
import SqlSchema.*
import java.nio.ByteBuffer


object FbRequests {

    fun buildGetDataEnvelope(
        correlationId: String,
        replyTopic: String,
        companyId: String,
        tableName: String,
        order: List<OrderKey>,
        wrapperFbBytes: ByteArray?,
        limit: Int,
        offset: Int?,
        cursorTuple: Map<String, Any?>?,
        strictAfter: Boolean
    ): ByteArray {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val b = FlatBufferBuilder(1024)

        val orderOffsets = order.map { ok ->
            val fieldOff = b.createString(ok.field)
            SqlSchema.OrderKeySpec.createOrderKeySpec(
                b,
                fieldOff,
                ok.sortOrdinal,
                false
            )
        }.toIntArray()
        val orderVec = GetDataReq.createOrderVector(b, orderOffsets)

        // INCLUDE the wrapper if provided
        val wrapperOff = wrapperFbBytes?.let {
            val bb = ByteBuffer.wrap(it)
            val w = BasicSqlDataFilterWrapper.getRootAsBasicSqlDataFilterWrapper(bb)
            copyWrapper(b, w)
        } ?: 0

        val cursorOffsets = cursorTuple?.entries?.map { (field, value) ->
            val fieldOff = b.createString(field)
            val (valType, valOff) = encodeFilterValue(b, value)
            CursorEntry.createCursorEntry(b, fieldOff, valType, valOff)
        }?.toIntArray()

        val cursorVec = if (cursorOffsets != null && cursorOffsets.isNotEmpty())
            GetDataReq.createCursorVector(b, cursorOffsets)
        else 0

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        val companyOff = b.createString(companyId)
        val tableOff = b.createString(tableName)

        val reqOff = GetDataReq.createGetDataReq(
            b,
            companyOff,
            tableOff,
            wrapperOff,
            limit.toLong(),
            (offset ?: 0).toLong(),
            orderVec,
            cursorVec,
            strictAfter
        )

        if (debug) println("[FbRequests] buildGetDataEnvelope corr=$correlationId company=$companyId table=$tableName limit=$limit strictAfter=$strictAfter hasWrapper=${wrapperOff!=0} hasCursor=${cursorVec!=0}")

        val envOff = RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(envOff)
        return b.sizedByteArray()
    }

    fun buildGetDataFromQueryFb(
        correlationId: String,
        replyTopic: String,
        queryFb: ByteArray,
        cursorTuple: Map<String, Any?>,
        limit: Int,
        strictlyAfter: Boolean,
        fullTableName: String
    ): ByteArray {
        val debug = System.getenv("CDC_DEBUG") == "1"
        val bb = ByteBuffer.wrap(queryFb)
        val src = StreamingSqlDataFilter.getRootAsStreamingSqlDataFilter(bb)

        val companyId = fullTableName.substringBefore('_', "")
        val baseTable = fullTableName.substringAfter('_', fullTableName)

        val b = FlatBufferBuilder(1024)

        val orderOffsets = IntArray(src.orderLength())
        for (i in 0 until src.orderLength()) {
            val ok = src.order(i)!!
            val fieldOff = b.createString(ok.field())
            val okOff = SqlSchema.OrderKeySpec.createOrderKeySpec(
                b, fieldOff, ok.sort(), ok.isPk()
            )
            orderOffsets[i] = okOff
        }
        val orderVec = SqlRpc.GetDataReq.createOrderVector(b, orderOffsets)

        val userWrapper = src.wrapper()
        val afterOff = buildStrictlyAfterWrapper(b, cursorTuple, src)
        val wrapperOff = if (userWrapper == null) {
            afterOff
        } else {
            val children = intArrayOf(copyWrapper(b, userWrapper), afterOff)
            val unionTypes = byteArrayOf(
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte(),
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
            )
            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(
                b,
                BasicSqlDataFilterWrapper.createFiltersVector(b, children)
            )
            BasicSqlDataFilterWrapper.addFiltersType(
                b,
                BasicSqlDataFilterWrapper.createFiltersTypeVector(b, unionTypes)
            )
            BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
        }

        val cidOff  = b.createString(companyId)
        val tblOff  = b.createString(baseTable)
        val reqOff = SqlRpc.GetDataReq.createGetDataReq(
            b,
            cidOff,
            tblOff,
            wrapperOff,
            limit.toLong(),
            0L,
            orderVec,
            0,
            strictlyAfter
        )

        val corrOff = b.createString(correlationId)
        val replyOff = b.createString(replyTopic)
        if (debug) println("[FbRequests] buildGetDataFromQueryFb corr=$correlationId table=$fullTableName limit=$limit strictlyAfter=$strictlyAfter")
        val env = SqlRpc.RequestEnvelope.createRequestEnvelope(
            b,
            corrOff,
            replyOff,
            RpcMethod.GET_DATA,
            RpcPayload.GetDataReq,
            reqOff
        )
        b.finish(env)
        return b.sizedByteArray()
    }

    // ---------------- helpers moved here from CdcProcessor ----------------

    private fun buildStrictlyAfterWrapper(
        b: FlatBufferBuilder,
        cursorTuple: Map<String, Any?>,
        src: StreamingSqlDataFilter
    ): Int {
        val disjChildren = mutableListOf<Int>()
        val disjTypes = mutableListOf<Byte>()

        val n = src.orderLength()
        for (k in 0 until n) {
            val conj = mutableListOf<Int>()
            val conjTypes = mutableListOf<Byte>()

            for (i in 0 until k) {
                val ok = src.order(i)!!
                val fName = ok.field()
                val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])
                conj += buildBinaryFilter(b, fName, t, vOff, BasicSqlDataFilterType.equals)
                conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
            }

            val ok = src.order(k)!!
            val fName = ok.field()
            val (t, vOff) = encodeFilterValue(b, cursorTuple[fName])

            val type = when (ok.sort()) {
                OrderSort.ASC_DEFAULT, OrderSort.ASC_NULLS_FIRST, OrderSort.ASC_NULLS_LAST ->
                    BasicSqlDataFilterType.greaterThan
                else -> BasicSqlDataFilterType.lessThan
            }
            conj += buildBinaryFilter(b, fName, t, vOff, type)
            conjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()

            val conjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, conj.toIntArray())
            val conjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, conjTypes.toByteArray())

            BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
            BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.and)
            BasicSqlDataFilterWrapper.addFilters(b, conjVec)
            BasicSqlDataFilterWrapper.addFiltersType(b, conjTypeVec)
            val conjOff = BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)

            disjChildren += conjOff
            disjTypes += BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
        }

        val disjVec = BasicSqlDataFilterWrapper.createFiltersVector(b, disjChildren.toIntArray())
        val disjTypeVec = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, disjTypes.toByteArray())
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, SQLFilterWrapperType.or)
        BasicSqlDataFilterWrapper.addFilters(b, disjVec)
        BasicSqlDataFilterWrapper.addFiltersType(b, disjTypeVec)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyWrapper(b: FlatBufferBuilder, w: BasicSqlDataFilterWrapper?): Int {
        if (w == null) return 0
        val count = w.filtersLength()
        val children = IntArray(count)
        val types = ByteArray(count)

        for (i in 0 until count) {
            when (w.filtersType(i).toInt()) {
                BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toInt() -> {
                    val childW = BasicSqlDataFilterWrapper()
                    w.filters(childW, i)
                    children[i] = copyWrapper(b, childW)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilterWrapper.toByte()
                }
                else -> {
                    val childL = BasicSqlDataFilter()
                    w.filters(childL, i)
                    children[i] = copyLeaf(b, childL)
                    types[i] = BasicSqlDataFilterUnion.BasicSqlDataFilter.toByte()
                }
            }
        }

        val fv = BasicSqlDataFilterWrapper.createFiltersVector(b, children)
        val tv = BasicSqlDataFilterWrapper.createFiltersTypeVector(b, types)
        BasicSqlDataFilterWrapper.startBasicSqlDataFilterWrapper(b)
        BasicSqlDataFilterWrapper.addFilterWrapperType(b, w.filterWrapperType())
        BasicSqlDataFilterWrapper.addFilters(b, fv)
        BasicSqlDataFilterWrapper.addFiltersType(b, tv)
        return BasicSqlDataFilterWrapper.endBasicSqlDataFilterWrapper(b)
    }

    private fun copyLeaf(b: FlatBufferBuilder, lf: BasicSqlDataFilter): Int {
        val fieldOff = b.createString(lf.fieldName())

        val (vt: Byte, vOff: Int) = when (lf.valueType().toInt()) {
            FilterValue.StringValue.toInt() -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
            FilterValue.NumberValue.toInt() -> {
                val tmp = NumberValue(); lf.value(tmp)
                FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, tmp.value())
            }
            FilterValue.Int64Value.toInt() -> {
                val tmp = Int64Value(); lf.value(tmp)
                FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, tmp.value())
            }
            FilterValue.BoolValue.toInt() -> {
                val tmp = BoolValue(); lf.value(tmp)
                FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, tmp.value())
            }
            FilterValue.NullValue.toInt() -> {
                NullValue.startNullValue(b)
                val nv = NullValue.endNullValue(b)
                FilterValue.NullValue.toByte() to nv
            }
            else -> {
                val tmp = StringValue(); lf.value(tmp)
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(tmp.value()))
            }
        }

        val mod = lf.modifier()
        val modOff = if (mod == null) 0 else run {
            SqlFilterModifier.startSqlFilterModifier(b)
            SqlFilterModifier.addDistinct(b, mod.distinct())
            SqlFilterModifier.addCaseInsensitive(b, mod.caseInsensitive())
            SqlFilterModifier.addNullsOrder(b, mod.nullsOrder())
            SqlFilterModifier.endSqlFilterModifier(b)
        }

        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, lf.filterType())
        if (modOff != 0) BasicSqlDataFilter.addModifier(b, modOff)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun buildBinaryFilter(
        b: FlatBufferBuilder,
        field: String,
        vt: Byte,
        vOff: Int,
        type: Int
    ): Int {
        val fieldOff = b.createString(field)
        BasicSqlDataFilter.startBasicSqlDataFilter(b)
        BasicSqlDataFilter.addFieldName(b, fieldOff)
        BasicSqlDataFilter.addValueType(b, vt)
        BasicSqlDataFilter.addValue(b, vOff)
        BasicSqlDataFilter.addFilterType(b, type)
        return BasicSqlDataFilter.endBasicSqlDataFilter(b)
    }

    private fun encodeFilterValue(b: FlatBufferBuilder, v: Any?): Pair<Byte, Int> {
        if (v == null) {
            NullValue.startNullValue(b)
            val off = NullValue.endNullValue(b)
            return FilterValue.NullValue.toByte() to off
        }
        return when (v) {
            is String  -> FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(v))
            is Boolean -> FilterValue.BoolValue.toByte() to BoolValue.createBoolValue(b, v)
            is Int     -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v.toLong())
            is Long    -> FilterValue.Int64Value.toByte() to Int64Value.createInt64Value(b, v)
            is Number  -> FilterValue.NumberValue.toByte() to NumberValue.createNumberValue(b, v.toDouble())
            else       -> {
                val s = v.toString()
                FilterValue.StringValue.toByte() to StringValue.createStringValue(b, b.createString(s))
            }
        }
    }


}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/FbResponses.kt
Size: 636 B
Code:
// src/main/java/org/cladbe/cdc/engine/FbResponses.kt
package org.cladbe.cdc.engine

import SqlRpc.*
import java.nio.ByteBuffer

object FbResponses {
    /** Returns first row JSON (string) from RowsJson, or null if none/other response */
    fun parseRowsJsonFirst(bytes: ByteArray): String? {
        val bb = ByteBuffer.wrap(bytes)
        val env = ResponseEnvelope.getRootAsResponseEnvelope(bb)
        if (!env.ok()) return null
        if (env.dataType() != RpcResponse.RowsJson) return null
        val rows = RowsJson()
        env.data(rows)
        if (rows.rowsLength() <= 0) return null
        return rows.rows(0)
    }
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/KeyDbWriter.kt
Size: 3.35 KB
Code:
// src/main/java/org/cladbe/cdc/engine/KeyDbWriter.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.databind.ObjectMapper
import redis.clients.jedis.JedisPooled
import java.net.URI

class KeyDbWriter(
    url: String = System.getenv("KEYDB_URL") ?: "redis://127.0.0.1:6379",
    private val prefix: String = (System.getenv("KEYDB_PREFIX") ?: "hcache:").replace("\\s".toRegex(), ""),
    private val maxDiffs: Int = (System.getenv("HOTCACHE_MAX_DIFFS") ?: "5000").toInt(),
    private val retentionMs: Long = (System.getenv("HOTCACHE_RETENTION_MS") ?: "${10 * 60_000}").toLong()
) : AutoCloseable {

    private val jedis = JedisPooled(URI(url))
    private val mapper = ObjectMapper()

    private fun kSnap(hashId: String)  = "${prefix}snap:$hashId"
    private fun kDiff(hashId: String)  = "${prefix}diff:$hashId"
    private fun kRange(hashId: String) = "${prefix}range:$hashId"
    private fun kRow(table: String, pk: String) = "${prefix}row:$table:$pk"

    /** snap:{hashId} → {"rows":[...], "cursor":{"lsn": "..."}, "ts": epochMillis} */
    fun writeSnapshot(hashId: String, rows: List<Map<String, Any?>>, lsn: Long, ts: Long = System.currentTimeMillis()) {
        val payload = mapOf("rows" to rows, "cursor" to mapOf("lsn" to lsn.toString()), "ts" to ts)
        val json = mapper.writeValueAsString(payload)
        if (retentionMs > 0) jedis.psetex(kSnap(hashId), retentionMs, json) else jedis.set(kSnap(hashId), json)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs) // keep list alive in step with snap
    }

    /** diff:{hashId} → RPUSH {"lsn":"...", "b64":"..."}; LTRIM; PEXPIRE */
    fun appendDiff(hashId: String, lsn: Long, b64: String) {
        val item = mapper.writeValueAsString(mapOf("lsn" to lsn.toString(), "b64" to b64))
        jedis.rpush(kDiff(hashId), item)
        jedis.ltrim(kDiff(hashId), -maxDiffs.toLong(), -1)
        if (retentionMs > 0) jedis.pexpire(kDiff(hashId), retentionMs)
    }

    /** range:{hashId} → list of PKs (index = position) */
    fun writeRangeIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRange(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) jedis.rpush(key, *pksInOrder.toTypedArray())
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    /** row:{table}:{pk} → {"row":{...},"lsn":"..."} (hot row cache for ws_gateway) */
    fun writeHotRow(table: String, pk: String, row: Map<String, Any?>, lsn: Long?) {
        val payload = if (lsn != null && lsn > 0) mapOf("row" to row, "lsn" to lsn.toString()) else mapOf("row" to row)
        val json = mapper.writeValueAsString(payload)
        val key = kRow(table, pk)
        if (retentionMs > 0) jedis.psetex(key, retentionMs, json) else jedis.set(key, json)
    }

    private fun kRev(hashId: String) = "${prefix}ridx:$hashId"

    /** ridx:{hashId} → Hash pk -> pos */
    fun writeReverseIndex(hashId: String, pksInOrder: List<String>) {
        val key = kRev(hashId)
        jedis.del(key)
        if (pksInOrder.isNotEmpty()) {
            val kv = mutableListOf<String>()
            pksInOrder.forEachIndexed { i, pk ->
                kv += pk; kv += i.toString()
            }
            jedis.hset(key, kv.chunked(2).associate { it[0] to it[1] })
        }
        if (retentionMs > 0) jedis.pexpire(key, retentionMs)
    }

    override fun close() {
        runCatching { jedis.close() }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/PkUtils.kt
Size: 207 B
Code:
package org.cladbe.cdc.engine

/** Extract the PK value from a decoded row using the configured PK column. */
fun extractPkOrNull(row: Map<String, Any?>, pkCol: String): String? =
    row[pkCol]?.toString()

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/QueryControlProcessor.kt
Size: 1.30 KB
Code:
// src/main/java/org/cladbe/cdc/engine/Processors.kt  (QueryControlProcessor part)
package org.cladbe.cdc.engine

import org.apache.kafka.streams.processor.api.*
import org.apache.kafka.streams.state.KeyValueStore

class QueryControlProcessor(
    private val qmetaName: String,
    private val pageName: String,
    private val outboxName: String
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var qmeta: KeyValueStore<String, ByteArray>
    private lateinit var page : KeyValueStore<String, ByteArray>
    private lateinit var out  : KeyValueStore<String, ByteArray>

    override fun init(context: ProcessorContext<Void, Void>) {
        qmeta = context.getStateStore(qmetaName)
        page  = context.getStateStore(pageName)
        out   = context.getStateStore(outboxName)
    }

    override fun process(record: Record<String, ByteArray>) {
        // Expect key = "$table|$hashId"
        val key = record.key() ?: return
        val fb = record.value()

        if (fb == null) {
            page.delete(key)
            out.delete(key)
            qmeta.delete(key)
            return
        }

        qmeta.put(key, fb)
        if (page.get(key) == null) page.put(key, ByteArray(0))
        if (out.get(key)  == null) out.put(key, ByteArray(0))
    }

    override fun close() { /* no-op */ }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/SeedProcessor.kt
Size: 2.39 KB
Code:
// src/main/java/org/cladbe/cdc/engine/SeedProcessor.kt
package org.cladbe.cdc.engine

import com.fasterxml.jackson.core.type.TypeReference
import com.fasterxml.jackson.databind.ObjectMapper
import org.apache.kafka.streams.processor.api.Processor
import org.apache.kafka.streams.processor.api.ProcessorContext
import org.apache.kafka.streams.processor.api.Record
import org.apache.kafka.streams.state.KeyValueStore

class SeedProcessor(
    private val rowCacheName: String,
    private val pageName: String,
    // allow tests to inject a fake/mocked KeyDbWriter
    private val keyDbFactory: () -> KeyDbWriter = { KeyDbWriter() }
) : Processor<String, ByteArray, Void, Void> {

    private lateinit var ctx: ProcessorContext<Void, Void>
    private lateinit var rowCache: KeyValueStore<String, ByteArray>
    private lateinit var page: KeyValueStore<String, ByteArray>
    private val mapper = ObjectMapper()
    private var keydb: KeyDbWriter? = null

    override fun init(context: ProcessorContext<Void, Void>) {
        ctx = context
        rowCache = context.getStateStore(rowCacheName)
        page = context.getStateStore(pageName)
        runCatching { keydb = keyDbFactory() }.onFailure { e ->
            println("[streams] KeyDB disabled in SeedProcessor: ${e.message}")
        }
    }

    override fun process(record: Record<String, ByteArray>) {
        val payload = record.value() ?: return
        val m: Map<String, Any?> = try {
            mapper.readValue(payload, object : TypeReference<Map<String, Any?>>() {})
        } catch (_: Exception) { return }

        val table = (m["table"] as? String) ?: return
        val hashId = (m["hashId"] as? String) ?: return
        @Suppress("UNCHECKED_CAST")
        val rows = (m["rows"] as? List<Map<String, Any?>>) ?: return

        val pkCol = TablePkRegistry.pkColumnFor(table) ?: "id"

        val pks = ArrayList<String>(rows.size)
        for (r in rows) {
            val pk = extractPkOrNull(r, pkCol) ?: continue
            rowCache.put(pk, mapper.writeValueAsBytes(r))
            keydb?.writeHotRow(table = table, pk = pk, row = r, lsn = 0L)
            pks += pk
        }

        page.put("$table|$hashId", pks.joinToString(",").toByteArray())

        runCatching {
            keydb?.writeRangeIndex(hashId, pks)
            keydb?.writeSnapshot(hashId, rows, lsn = 0L)
        }
    }

    override fun close() {
        runCatching { keydb?.close() }
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/TablePkRegistry.kt
Size: 926 B
Code:
package org.cladbe.cdc.engine

/**
 * Immutable registry of primary-key *column names* per base table.
 * If your Kafka topic/table name is "tenant_table", we'll also try the base part after '_' so
 * the same map works for all tenants that share schema.
 */
object TablePkRegistry {
    // Map base table -> PK column name (edit as needed)
    private val pkByBase: Map<String, String> = mapOf(
        "a_notes"            to "id",
        "whatsapp_messages"  to "message_uuid",
        "a_timeline"         to "messageId"
        // add the rest of your tables here…
    )

    /** For names like "tenant_table", return "table"; if no '_', return input. */
    private fun baseName(table: String): String =
        table.substringAfter('_', table)

    /** Return PK column name for this table (or null if unknown). */
    fun pkColumnFor(table: String): String? =
        pkByBase[table] ?: pkByBase[baseName(table)]
}

-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/cdc/engine/Topology.kt
Size: 5.05 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.common.serialization.*
import org.apache.kafka.streams.Topology
import org.apache.kafka.streams.state.Stores
import java.util.regex.Pattern

object StreamsTopology {

    data class Topics(
        val cdcPattern: Pattern,
        val queryControlTopic: String,
        val seedTopic: String?,
        val rpcReplyTopic: String,
        val outboxTopic: String,
        val backfillTopic: String
    )

    data class StateStoreNames(
        val qmeta: String   = "qmeta",
        val page: String    = "page",
        val outbox: String  = "outbox",
        val rowCache: String= "rowCache",
        val pending: String = "pending"
    )

    fun build(topics: Topics, stores: StateStoreNames = StateStoreNames()): Topology {
        val debug = System.getenv("CDC_DEBUG") == "1"
        if (debug) println("[Topology] Building with outbox=${topics.outboxTopic} backfill=${topics.backfillTopic} rpcReply=${topics.rpcReplyTopic}")

        val top = Topology()

        val qmetaStore = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.qmeta),
            Serdes.String(), Serdes.ByteArray())
        val pageStore  = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.page),
            Serdes.String(), Serdes.ByteArray())
        val outboxStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.outbox),
            Serdes.String(), Serdes.ByteArray())
        val rowCacheStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.rowCache),
            Serdes.String(), Serdes.ByteArray())
        val pendingStore= Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(stores.pending),
            Serdes.String(), Serdes.String())

        val cdcSource = "src-cdc"
        top.addSource(
            cdcSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.cdcPattern
        )

        val qctlSource = "src-qctl"
        top.addSource(
            qctlSource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.queryControlTopic
        )

        val backfillSource = "src-backfill"
        top.addSource(
            backfillSource,
            ByteArrayDeserializer(), ByteArrayDeserializer(),
            topics.backfillTopic
        )

        val rpcReplySource = "src-rpc-replies"
        top.addSource(
            rpcReplySource,
            StringDeserializer(), ByteArrayDeserializer(),
            topics.rpcReplyTopic
        )

        val seedSource: String? = topics.seedTopic?.let {
            "src-seed".also { name ->
                top.addSource(
                    name,
                    StringDeserializer(), ByteArrayDeserializer(),
                    it
                )
            }
        }

        val qctlProc = "proc-qctl"
        top.addProcessor(qctlProc, {
            QueryControlProcessor(
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox
            )
        }, qctlSource)
        top.addStateStore(qmetaStore, qctlProc)
        top.addStateStore(pageStore, qctlProc)
        top.addStateStore(outboxStore, qctlProc)

        val cdcProc = "proc-cdc"
        top.addProcessor(cdcProc, {
            CdcProcessor(
                rowCacheName = stores.rowCache,
                qmetaName = stores.qmeta,
                pageName = stores.page,
                outboxName = stores.outbox,
                pendingName = stores.pending
            )
        }, cdcSource, backfillSource)
        top.addStateStore(rowCacheStore, cdcProc)
        top.addStateStore(qmetaStore, cdcProc)
        top.addStateStore(pageStore, cdcProc)
        top.addStateStore(outboxStore, cdcProc)
        top.addStateStore(pendingStore, cdcProc)

        if (seedSource != null) {
            val seedProc = "proc-seed"
            top.addProcessor(seedProc, {
                SeedProcessor(
                    rowCacheName = stores.rowCache,
                    pageName = stores.page
                )
            }, seedSource)
            top.addStateStore(rowCacheStore, seedProc)
            top.addStateStore(pageStore, seedProc)
        }

        val backfillProc = "proc-backfill-replies"
        val backfillSink = "sink-backfill"
        top.addProcessor(backfillProc, {
            BackfillReplyProcessor(
                rowCacheName = stores.rowCache,
                pendingName = stores.pending,
                backfillTopicName = backfillSink
            )
        }, rpcReplySource)
        top.addStateStore(rowCacheStore, backfillProc)
        top.addStateStore(pendingStore, backfillProc)
        top.addSink(
            backfillSink,
            topics.backfillTopic,
            ByteArraySerializer(), ByteArraySerializer(),
            backfillProc
        )

        val outboxSink = "sink-outbox"
        top.addSink(
            outboxSink,
            topics.outboxTopic,
            StringSerializer(), ByteArraySerializer(),
            cdcProc
        )

        if (debug) println("[Topology] Build complete")
        return top
    }
}
-------- [ Separator ] ------

File Name: src/main/kotlin/org/cladbe/tools/SendQctl.kt
Size: 2.62 KB
Code:
package org.cladbe.cdc.engine

import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.StringSerializer
import org.apache.kafka.common.serialization.ByteArraySerializer
import java.util.Properties
import java.nio.ByteBuffer
import com.google.flatbuffers.FlatBufferBuilder
import SqlSchema.*
import SqlSchema.StreamingSqlDataFilter as Q

object Tools {
    @JvmStatic
    fun main(args: Array<String>) {
        val brokers = System.getenv("KAFKA_BROKERS") ?: "localhost:29092"
        val qctlTopic = System.getenv("QUERY_CONTROL_TOPIC") ?: "server.query.control"
        val debug = System.getenv("CDC_DEBUG") == "1"

        if (args.isEmpty()) {
            println("Usage: Tools install-notes-query");
            return
        }
        when (args[0]) {
            "install-notes-query" -> {
                val key = "a_notes|demo"
                val fb = buildNotesQueryFb()
                if (debug) println("[Tools] sending qctl key=$key topic=$qctlTopic brokers=$brokers")
                send(kTopic = qctlTopic, key = key, value = fb, brokers = brokers)
                println("Installed query meta for key=$key to $qctlTopic")
            }
            else -> println("Unknown cmd: ${args[0]}")
        }
    }

    private fun send(kTopic: String, key: String, value: ByteArray, brokers: String) {
        val props = Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java.name)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer::class.java.name)
            put(ProducerConfig.ACKS_CONFIG, "1")
        }
        KafkaProducer<String, ByteArray>(props).use { p ->
            p.send(ProducerRecord(kTopic, key, value)).get()
        }
    }

    private fun buildNotesQueryFb(): ByteArray {
        val b = FlatBufferBuilder(256)

        val order = intArrayOf(
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("__lsn"),
                OrderSort.DESC_DEFAULT, false
            ),
            OrderKeySpec.createOrderKeySpec(
                b, b.createString("id"),
                OrderSort.DESC_DEFAULT, true
            )
        )
        val orderVec = Q.createOrderVector(b, order)
        Q.startStreamingSqlDataFilter(b)
        Q.addLimit(b, 10L) // <-- must be long
        Q.addOrder(b, orderVec)
        val off = Q.endStreamingSqlDataFilter(b)
        b.finish(off)
        return b.sizedByteArray()
    }
}
-------- [ Separator ] ------

File Name: src/main/resources/logback.xml
Size: 274 B
Code:
<configuration>
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder><pattern>%d %-5level %logger{36} - %msg%n</pattern></encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="STDOUT"/>
    </root>
</configuration>

-------- [ Separator ] ------

File Name: src/test/kotlin/org/cladbe/cdc/Evaluator/DiffOpsTest.kt
Size: 2.47 KB
Code:
package org.cladbe.cdc.Evaluator

import kotlin.test.Test
import kotlin.test.assertEquals
import kotlin.test.assertTrue
import java.util.Comparator

class DiffOpsTest {

    private val intCmp = Comparator<Int> { a, b -> a.compareTo(b) } // ASC: smaller is “better”

    @Test
    fun insertAndDiff_basicInsert_thenEvictTailWhenFull() {
        val page = mutableListOf(2, 5) // sorted ASC
        val k = 3

        // Insert 3 -> should go between 2 and 5
        val diffs1 = DiffOps.insertAndDiff(page, 3, k, intCmp)
        assertEquals(listOf(Diff.Added(3, 1)), diffs1)
        assertEquals(listOf(2, 3, 5), page)

        // Now full (k=3). Insert 4 (better than current tail 5) → evict 5, insert 4
        val diffs2 = DiffOps.insertAndDiff(page, 4, k, intCmp)
        // Previous size was 3 → evicted tail pos = prevSize-1 = 2
        assertEquals(
            listOf(Diff.Added(4, 2), Diff.Removed(5, 2)),
            diffs2
        )
        assertEquals(listOf(2, 3, 4), page)

        // Insert 10 (worse than tail 4) → no change
        val diffs3 = DiffOps.insertAndDiff(page, 10, k, intCmp)
        assertTrue(diffs3.isEmpty())
        assertEquals(listOf(2, 3, 4), page)
    }

    @Test
    fun repositionAndDiff_moveAndPayloadOnly() {
        val page = mutableListOf(2, 4, 6, 8) // ASC
        // Reposition for 6 with no change in order → payload-only change:
        val payloadOnly = DiffOps.repositionAndDiff(page, 6, intCmp, payloadChanged = true)
        assertEquals(listOf(Diff.Modified(6, pos = 2, from = 2)), payloadOnly)
        assertEquals(listOf(2, 4, 6, 8), page)

        // Simulate order change that moves 6 to before 4 by using a custom comparator for this call
        val weirdCmp = Comparator<Int> { a, b ->
            val aKey = if (a == 6) 3 else a
            val bKey = if (b == 6) 3 else b
            aKey.compareTo(bKey)
        }
        val moved = DiffOps.repositionAndDiff(page, 6, weirdCmp, payloadChanged = true)
        // from index 2 to new index 1
        assertEquals(listOf(Diff.Modified(6, pos = 1, from = 2)), moved)
        assertEquals(listOf(2, 6, 4, 8), page)
    }

    @Test
    fun removeAndDiff_presentAndAbsent() {
        val page = mutableListOf(1, 2, 3)
        val diffs1 = DiffOps.removeAndDiff(page, 2)
        assertEquals(listOf(Diff.Removed(2, 1)), diffs1)
        assertEquals(listOf(1, 3), page)

        val diffs2 = DiffOps.removeAndDiff(page, 42)
        assertTrue(diffs2.isEmpty())
        assertEquals(listOf(1, 3), page)
    }
}
-------- [ Separator ] ------

File Name: src/test/kotlin/org/cladbe/cdc/Evaluator/PageOpsTest.kt
Size: 2.47 KB
Code:
package org.cladbe.cdc.Evaluator

import kotlin.test.Test
import kotlin.test.assertEquals
import kotlin.test.assertFalse
import kotlin.test.assertTrue
import java.util.Comparator

class PageOpsTest {

    private val intCmp = Comparator<Int> { a, b -> a.compareTo(b) } // ASC

    @Test
    fun insertionIndex_binarySearch() {
        val page = listOf(2, 4, 6, 8)
        // first idx where key can go (stable insert)
        assertEquals(0, PageOps.insertionIndex(page, 1, intCmp))
        assertEquals(1, PageOps.insertionIndex(page, 3, intCmp))
        assertEquals(4, PageOps.insertionIndex(page, 10, intCmp))
    }

    @Test
    fun insertIfBetter_growThenBounded() {
        val page = mutableListOf<Int>()
        val k = 3

        val r1 = PageOps.insertIfBetter(page, 5, k, intCmp)
        assertTrue(r1.inserted)
        assertEquals(0, r1.pos)
        assertEquals(listOf(5), page)

        val r2 = PageOps.insertIfBetter(page, 2, k, intCmp)
        assertTrue(r2.inserted)
        assertEquals(0, r2.pos)
        assertEquals(listOf(2, 5), page)

        val r3 = PageOps.insertIfBetter(page, 4, k, intCmp)
        assertTrue(r3.inserted)
        assertEquals(1, r3.pos)
        assertEquals(listOf(2, 4, 5), page)

        // full; 6 is worse than tail(5) → ignore
        val r4 = PageOps.insertIfBetter(page, 6, k, intCmp)
        assertFalse(r4.inserted)
        assertEquals(-1, r4.pos)
        assertEquals(listOf(2, 4, 5), page)

        // better than tail → evict tail
        val r5 = PageOps.insertIfBetter(page, 3, k, intCmp)
        assertTrue(r5.inserted)
        assertEquals(1, r5.pos)
        assertEquals(5, r5.evicted) // old tail
        assertEquals(listOf(2, 3, 4), page)
    }

    @Test
    fun repositionIfNeeded_moveOrStay() {
        val page = mutableListOf(2, 4, 6, 8)
        // No movement under same comparator
        val stay = PageOps.repositionIfNeeded(page, 6, intCmp)
        assertFalse(stay.moved)
        assertEquals(2, stay.from)
        assertEquals(2, stay.to)
        assertEquals(listOf(2, 4, 6, 8), page)

        // Custom comparator: “pretend” 6 should sort before 4
        val cmp = Comparator<Int> { a, b ->
            val aa = if (a == 6) 3 else a
            val bb = if (b == 6) 3 else b
            aa.compareTo(bb)
        }
        val moved = PageOps.repositionIfNeeded(page, 6, cmp)
        assertTrue(moved.moved)
        assertEquals(2, moved.from)
        assertEquals(1, moved.to)
        assertEquals(listOf(2, 6, 4, 8), page)
    }
}
-------- [ Separator ] ------

File Name: src/test/kotlin/org/cladbe/cdc/Evaluator/QueryEngineSmokeTest.kt
Size: 3.28 KB
Code:
// src/test/kotlin/org/cladbe/cdc/Evaluator/QueryEngineContractTest.kt
package org.cladbe.cdc.Evaluator

import org.cladbe.cdc.engine.QueryEngine
import org.junit.jupiter.api.Assertions.assertEquals
import org.junit.jupiter.api.Test

class QueryEngineContractTest {

    private fun specK3(): DerivedSpec =
        DerivedSpec(
            table = "a_notes",
            k = 3,
            order = listOf(
                OrderKeySpec.of("__lsn", Direction.DESC),
                OrderKeySpec.pk("id", Direction.DESC) // PK tiebreaker last
            ),
            whereMatches = { true },
            filterFields = emptySet()
        )

    /** Minimal row store the engine can read from (simulates CdcProcessor rowCache). */
    private class Store : (String) -> Map<String, Any?>? {
        private val m = linkedMapOf<String, Map<String, Any?>>()
        fun put(row: Map<String, Any?>) { m[row["id"].toString()] = row }
        override fun invoke(pk: String): Map<String, Any?>? = m[pk]
    }

    private fun upsertRow(
        engine: QueryEngine<String>,
        store: Store,
        row: Map<String, Any?>
    ) = run {
        store.put(row)                    // 1) warm cache like CdcProcessor
        engine.upsert(row)                // 2) upsert (now the rowProvider can see it)
    }

    @Test
    fun topK_insert_promote_demote_remove_basicFlow() {
        val store = Store()
        val spec  = specK3()

        val engine = QueryEngine(
            spec = spec,
            pkField = "id",
            getPk = { it["id"].toString() },
            rowProvider = store
        )

        // ---- Insert first row
        val r1 = mapOf("id" to "1", "__lsn" to 1000L)
        val res1 = upsertRow(engine, store, r1)
        assertEquals(1, res1.changes.size, "first insert should emit 1 change (added)")

        // ---- Insert second row (higher lsn → should be before r1)
        val r2 = mapOf("id" to "2", "__lsn" to 2000L)
        val res2 = upsertRow(engine, store, r2)
        assertEquals(2, engine.page.size)               // page grows to 2
        assertEquals(1, res2.changes.size, "second insert should emit 1 change (added)")

        // ---- Promote r1 by bumping its lsn above r2
        val r1Promoted = r1 + ("__lsn" to 3000L)
        val res3 = upsertRow(engine, store, r1Promoted)
        assertEquals(1, res3.changes.size, "promotion should emit 1 modified")
        // (Optional) you can assert the “from/to” positions if your WireDiff captures them

        // ---- Demote r1 by dropping lsn
        val r1Demoted = r1 + ("__lsn" to 1L)
        val res4 = upsertRow(engine, store, r1Demoted)
        assertEquals(1, res4.changes.size, "demotion should emit 1 modified")

        // ---- Insert third row to fill K=3
        val r3 = mapOf("id" to "3", "__lsn" to 1500L)
        val res5 = upsertRow(engine, store, r3)
        assertEquals(3, engine.page.size)
        assertEquals(1, res5.changes.size, "third insert should emit 1 change")

        // ---- Insert fourth row (should evict tail if it outranks it)
        val r4 = mapOf("id" to "4", "__lsn" to 5000L)  // outranks everyone
        val res6 = upsertRow(engine, store, r4)
        // Expect 2 changes: removed(evicted tail) + added(new)
        assertEquals(2, res6.changes.size, "eviction path should emit 2 changes (removed + added)")
    }
}
-------- [ Separator ] ------
